[{"title":"(ISCA2019)GraphSSD:图语义感知的SSD","date":"2024-05-07T02:43:39.000Z","path":"2024/05/07/graphssd/","text":"GraphSSD: Graph Semantics Aware SSD 作者：Kiran Kumar Matam, Gunjae Koo, Haipeng Zha, Hung-Wei Tseng and Murali Annavaram @ University of Southern California 现有的Out-of-core图处理系统试图通过分块的方式减缓的I&#x2F;O访存瓶颈，然而这种方法产生了很多冗余的数据访问开销。本文提出的GraphSSD是一个在SSD上进行图存储、图访问和图分析的一体化方案。其主要创新点有： GraphSSD在用新颖的vertex-to-page框架替换掉传统的物理页映射机制，将冗余数据访问降到了最小 最小化了不必要的页面替换开销来支持图更新 提供了有效的编程接口，支持应用开发人员像访问内存内的数据一样访问图。 Introduction在图分析领域中，由于许多图的大小已经远远超过了现有的单机主存的大小，需要来考虑以存储为中心的图处理方案。在这类问题中，访问图数据的I&#x2F;O时间相比于CPU计算和内存的访问时间成为了新的系统性能瓶颈。 存储层面上来看，SSD的价格下降明显(大约100$每TB)。并且伴随着NVMe接口的普及，SSD在带宽上有了明显的提升。除此之外，SSD内部还提供一些计算功能来管理闪存。这种SSD的出现使得构建感知图语义的存储系统成为了可能。 所谓的“感知图语义的存储系统”，具体来说就是将图结构直接看成一种本地数据，而实际上它在存储一端。相比于以往都是将图看成成页的数据，这种方式可以再编程友好的前提下融合更多底层的访存优化。 背景和挑战现代可编程的SSD平台 SSD内部通过多个闪存信道(Flash Channels)来提供高数据带宽。物理页面是一条闪存命令能够读写的最小单元。 SSD的写操作比较特殊：闪存页面在被写入操作之前要被完全擦除，但因为擦除闪存单元往往需要更高的能耗，也会污染邻近的页面，所以擦除过程以块单位上进行。故擦除过程很明显的比读写操作要慢，这叫做写放大现象。SSD都不会在每次写入操作时都执行擦除-写入的过程，而是会将更新的页内容写到空的物理页。每当页数据被更新的时候，FTL会将闪存页的逻辑块(LBA)地址映射到新的物理页地址(PPA)上，并老的页设定为失效。SSD控制器将这个映射机制放置在FTL中(闪存翻译层)的映射表里。 此外，SSD控制器内部对于失效的页建立了一个垃圾回收机制，来创造更多空的物理页面。这个机制擦除整个block，将该block中所有有效的物理页面写入到不同的block中的另一个空的物理页中。 图更新虽然许多图处理系统都假设图是静态的，但是在实际应用场景中，节点或边信息有可能会被更新。基于此，GraphSSD支持基于Delta Graph机制的图更新。 Delta Graph机制将图更新作为一系列的快照。一开始，所有的节点使用一个连续的向量来保存他的邻居信息。当图更新产生的时候，多个更新会合并形成一个snapshot。在常规的运行间隙，一个snapshot会被写入到存储中去，而新的snapshot会建立，以容纳新的更新。graphSSD在常规的间隙也会合并snapshot，来为每个节点创建单一的连续邻接矩阵。 当使用基于CSR的图表示来合并更新时，有可能需要遍历整个图以归并更新，将其写到新的位置。而出于一致性考虑，只有在全部的归并操作完成之后，才能从新位置读取节点的邻接矩阵。因此，支持归并会对节点的遍历过程带来很大的性能损失。 架构设计 组成部分： SSD端：图翻译层(GTL)，图命令解码器 HOST端：图缓存和图更新日志管理 图命令解码器 GraphSSD向上层提供一系列图访问的API。每一个API函数会进而激活SSD控制器来完成访存工作。这些函数会通过host端向SSD端发送的NVMe命令来实现。 所有的NVMe命令首先被GraphSSD图命令解码器解码，图命令解码器将GraphSSD和常规的NVMe命令执行不同的处理路线。因此GraphSSD和其他传统的存储可以共存。 一般来说，所有GraphSSD函数处理的第一步都是在闪存中访问节点及邻边。为了与原始的物理页面访问机制(FTL)进行区别，GraphSSD建立了GTL层(graph translation layer)。 GTL层和图存储格式GTL(graph translation layer)将传统的LBA-to-PPN机制更改了，其直接将节点id映射到保存其邻接节点信息的物理NAND页序号。 GTL结构GTL中的每一项都将节点id映射到物理页数，并且还保存有一系列的状态信息。(dirty &#x2F; extension &#x2F; valid)。因为很多点其邻接边非常少，可以将多个节点的信息全部定位到同一个物理页中。这样能够减少GTL的项数。GTL上只保存给定物理页中最小的节点作表项。在GTL中保存的节点信息是按照节点序号的大小从小到大进行编号的。如果是有向图，GraphSSD建立两个GTL，分别表示入边信息和出边信息。 物理页面存储 在一个物理页中， 最后一部分是本页中包含的节点个数； 第二部分中保存有很多二元组&lt;节点，偏移量&gt;，二元组记录节点的id和本节点在全页的邻接向量中的开始偏移量的比特数，最后一个二元组保存有整个页的有效数据的结束位置。 一页的开始部分保存有节点邻接信息。这样保存的好处：能够减少GTT的大小，使得dram中可以保持一个非常大的GTT；使得访问一个节点id的全部信息不需要额外nand页的访问； GTT的不同情况： 节点Vi的所有信息都能保存在页中，平凡不讨论； 节点Vi的信息太大，一个页保存不下。多页保存同一个Vi。建立物理页映射，多个项和物理页对应一个节点。在满的一页上面写一个节点的信息； 节点Vi的信息恰好在拐角处，不允许这样的情况发生，采取规避措施 用GTL进行访存的例子GTL使用二分查找的方式对节点进行查找。对于节点Vi，我们要找到下标使得j&lt;&#x3D;i&lt;&#x3D;k。当定位到Vj时，在上面的二元组进行二分查找，是否能够找到Vi。如果找得到则获取其邻接矩阵；如果Vi的邻结点很多那么将找到很多的页，来构建其邻接矩阵。 如何在上图中找到V3的邻接矩阵？在GTT的节点id上进行二分查找，因为V3&#x3D;V3 &lt; V6,所以V3应该在P2中，之后P2会被取到，P2中的最后一个位置中写着3，也就是本页中会有三个节点。之后GTL会搜索后半部分来查找V3的偏移位置，也就是1。通过其下一项V4的位置3，我们可以知道V3的邻结点有两个，即为V1,V2。 支持图更新的设计GTT支持的操作AddEdge(在两个点之间加一条边)和AddVertex(添加一个节点及其邻接边和权值) AddEdge(Vid1,Vid2)：将Vid2添加在Vid1的邻接矩阵中，并且也要添加值到value中。因为Vid1有可能存在一个物理页或两个中，所以有两种情况，如下。 当Vid1在单一页中时：Vid被添加到Vid1邻结点的最后。在Vid1之后的其他节点都向更高的页偏移进行移动。这些节点的指针也在本页的最后被更新。特殊情况：有可能发生溢出，溢出时重新添加一页，并且将这部分节点信息大概分成两页每页里面的空白空间相近。这是为了在之后的更新中尽可能减少溢出。 对于一个加入的页，我们应该将本GTT中的所有内容都向后移动一些，在最差的情况下，有可能移动所有的GTT项。为了防止这种现象发生，我们采用了GTT项中的扩展位，设置一个指针指向两页的位置。如下图。 当Vid1在多页时，添加在最后一页的最后，和上述操作基本相似。 AddEdge的例子： 一致性考虑将更新写在DRAM中就会有断电失去状态的风险。重要的是在重启之后要保持和重启前相同的一致性。因此每个更新都需要在重启后自动进行。我们在开始更新过程之前先创建redo-log，保存了正在更新的GTL项、deltaPointer和deltaUpdate数据。更新过程会先写新的页，之后清空GTL dirty位，将GTLentry的指针指向新的一页，最终将原来的老的物理页面设置为无效。 如果在写新的页或者在写完页，没有更新dirty位的时候断电，重启时，redo-log会从选择另一个页进行写入开始整个更新过程；之前写入的页会被垃圾回收机制回收。 如果在dirty位更新之后断电，并且GTLentry没有改动，GTL会把dirty为设置回来，重新开始更新过程； 如果在GTLentry更新之后改动，但老的页面并没有设置为无效，redo-log会简单的把老的页面设置为无效，把deltaPointer也设置为无效。 Garbage Collection机制：如果该页正在被用于保存图数据，在移动这一页的时候，GC会通知GraphSSD的运行时更新GTT。正在移动的页面已经包含有其包含的最小节点信息的内容。将该GTT项的节点信息改为这个节点信息并且更新page地址为移动到的位置。 保证更新效率前面提起的图更新过程会导致非常多没有必要的物理页写入。每次写入都会引起read-modify-write的过程，导致非常明显的写放大现象。为了减少写放大，直到更新数量足够多或者到达一定的时间要求，所有的更新都被记录在DRAM端的log文件中。主机端的GraphSSD log manager为了处理日志功能而创建。 HOST端的logger 更新需要先检查被更新的节点或者边信息还在不在图里。因此主机端的logger向SSD发送一个请求，确认节点或者边的存在性。Logger也同时启动另一进程来检查边或点信息在日志文件中是否存在，这是因为要被更新的节点或边信息可能仍然存在在DRAM的log文件中，但是这个更新还没有被映射到SSD中。如果不存在则返回FALSE。注意到这只需要一个简单的读操作，不会触发物理页面的更新。 Delta graph 当DRAMlog文件满了或者计时器到达了设定时间时，HOST端的logger会初始化一个更新队列。GraphSSD采用Delta graph来进一步减少写放大现象。我们在graphSSD中用两个数组来实现delta graph，包括deltapointer和deltaUpdates。一个邻接表的所有更新会被放在deltaUpdate vector里。对于一个节点，新添加的更新会被链接在之前的更新之后。一个节点的DeltaPointer指向指向deltaUpdate中的最近的更新位置。 有DRAM日志和Delta graph的图访问 图访问机制在运行时，为了正确地重构图，需要知道delta graph的存在性。为了标志delta graph的存在，GraphSSD在GTT中设置了dirty bit。如果dirty bit被设置了，先找到当前的图，再利用DeltaPointer找到所有的对该节点的更新。 将原始图和delta graph合并 因为会导致比较大的写放大，因此最好周期性地将delta graph和原图进行合并。在更新过程中，我们循环过整个GTT，并且找到所有GTT项中dirty的项。对于这些节点，我们找到更新并把他们和原图的信息进行合并。在所有的节点更新做完之后，所有GTT中的dirty bit都应该被抹除 GraphSSD的缓存管理graph的数据会被缓存再host端一部分。要求连续的节点id，因为会有很多对于连续的节点进行更新的操作，所以这样的缓存机制还是非常有用的。为了减少开销，建立了graphSSD cache，缓存了GTT，并且缓存系统可以处理graph command。在cache有miss的时候进行nand的获取，单独的nand页获取可能会导致很多host端的访问，这样也减少了对于SSD的访问。HOST端的GTT是只读的，所有的更新操作会使cache在Host上的GTT无效，并且更新会被做在SSD上。 在SSD上的GC机制中，在一个NAND页上面的数据可能会被写到另一个NAND上。如果NAND页上保存的图数据被移动了，新的NAND页码应该在HOST上的GTT上被更新。为了更新主机端的内容，SSD控制器会向host端的cache管理器发送一个请求来自动使得主机端的缓存无效，再进行更新。 实验平台GraphSSD在OpenSSD上被评估。 平台：Xilinx Zynq-7000 可编程 SoC 核心：双核ARM酷腾A9处理器 PCIe 和 NAND flash信道设计为在可编程阵列的硬件逻辑；可嵌入的ARM核心运行SSD固件。此固件可以完成命令的处理，页缓存管理和FTL的功能。在GraphSSD中我们修改其固件的FTL部分，命令处理部分和主机端调用库，使得可以管理主机端的缓存情况。 内存：搭载1GB DDR DRAM和2TB Hynix NAND Flash DIMM,与可编程SoC相连接。 传输协议：与主机端使用PCIe Gen2 x 8接口相连，其支持4GB&#x2F;s的带宽。 OpenSSD上的NAND页大小为16KB；主机端系统使用4KB。 主机系统：Intel i7-4790CPU，16GB DDR3 DRAM。为了扩展NVMe命令，主机端安装NVMe ver.3.19 linux驱动器 数据测试图数据：com-friendster(CF) &#x2F; YahooWebScope(YWS) 结果本文章的实验首先从基础的GraphSSD提供的图访问API进行评估，主要是GetAdjacentVerteces和GetEdgeWeight这两个函数。之后GraphSSD在应用级别的表现。对于基础API，每条命令执行1M次，并且使用随机的点或者边作为此查询过程的起点。 基础API GraphSSD是baseline系统的1.85倍。性能提升原因的两个点： baseline使用block接口来访问rowPtr和colIdx；GraphSSD使用GTL直接找到节点的邻接信息。Baseline 必须访问两个不同的页面来找到rowPtr和colIdx，但是GraphSSD用图语义可知的系统通过GTL来减少NAND页的访问。即使有百万请求，host端的缓存系统能够帮助减少NAND页的访问。没有成倍的原因是rowPtr向量比较紧凑，可以被缓存系统缓存在HOST端上来减少rowPtr有关的NAND页接触。 序列化瓶颈：首先访问rowPtr，再访问colIdx，因此两个访问是有先后顺序的。着花样有可能减少SSD请求率，会导致SSD与HOST之间通信的低带宽。但是实际上因为多个请求并行发送，所以rowPtr和colIdx的请求是混杂在一起的，这种多线程的方式保证了很高的带宽利用。如下图所示。 应用使用Random-walk应用来呈现细节的结果，根据全部应用的实验结果进行详细的分析。A.Random-walk a图：Y轴为相对于baseline的加速比。X轴为迭代伦次。整体提升性能大概为1.6倍。性能的提升来自于两点：NAND page访问的数量和SSD带宽的利用。 b图：因为在randomwalk中，下一个节点是随机访问的，baseline系统缓存rowPtr的作用不大。而使用一个紧凑的GTL的表示，GraphSSD能够减少NAND物理页面的访问数量。 c图：在baseline里面，访问rowPtr到访问colIdx的顺序会导致对SSD带宽的利用不充分。GraphSSD可以更好的缓存GTL，并行地去访问邻接点，所以提高了带宽。 B.所有应用 对于BFS，将能够到达的位置占全图最长路径的长度作为横坐标。对于BFS &#x2F; CC &#x2F; MIS &#x2F; PR来说，性能分别提升1.40 &#x2F; 1.42 &#x2F; 1.56 &#x2F; 1.29倍。原因是更少的NAND页访问和更高效的带宽利用。 C. 与GraphChi对比 因为GraphChi是为Vertex-centric为主的模型设计的，例如BFS &#x2F; CC等这些不适合用这个模型的方案不能够使用。GraphChi会在每次迭代的时候会把所有的点都加入到内存中，但是其中只有一小部分的活跃点会被使用。和GraphChi来说跟它比较BFS不公平。与GraphSSD相比较的结果就是证明了其多样性。在一次更新过程中，将所有的节点都放入到内存中，对于BFS来说不适用。随着距离变长，GraphChi的性能逐渐变差，在每次迭代过程中，所有的块都会被读取。另一方面，PR这种更加适合vertex-centric的编程模型，许多点会被重复遍历，与GraphSSD相比，性能性能有了2.62倍的提升。shard在每次都要被放入内存中计算，即时在shard中的计算非常少。 D. GraphSSD的开销GTT的大小对于两个数据集来说分别是8MB和34MB，相对于整个DRAM来说非常小。原因是因为多个节点放在同一个页中，这就只需要一个项来实现映射。只有很少的节点需要多个GTT项(0% in CF and 0.01% in YWS)。如果每个节点一页，大概需要1GB到11.2GB。空间开销：当将节点的邻接点放进NAND页中，为了简化设计，如果邻接点放不进空闲空间中，我们使用新的一个NAND页。此设计的空间开销大概4.8%或者4.9%。如果使用更加负责的设计，例如追踪一个节点在不同页之间的邻结点划分，能够轻松的省下。 E. 图更新 a图：横坐标是一共有多少个delta图。可以看出查询操作GetAdjacentVertices的性能稳定地下降，因为在更新过程中访问图很有可能涉及到UpdatePointer的追踪操作。 b图：横坐标是在一个页中有多少空的位置，纵坐标是merge过程的时间。空位置越少，merge时间越长。 c图：因为merge过程中，如果要查询节点，如果她已经被更新过直接查找page即可，但是没更新的需要找pointers。横坐标表示了merge的比例，纵坐标是性能体现。 在SSD的merge过程中，有更新直接merge。在更新NAND过程中溢出会用额外的空页来补足。对于更新在很少的NAND页中的图，这也帮助了更快的merge。对于普通的CSR来说，更新需要将所有的图信息重写写到另一个地方。 结论GraphSSD是一个图语义感知的SSD架构，使得SSD控制器可以直接接触flash存储。GTL层负责节点信息向flash存储上物理页的转换，为了GTL的实现，提出了一个行之有效的编码方案，使得GTT的要求空间非常小。我们也提出了几个优化来处理图更新。我们完成了GraphSSD的设计在SSD的开发平台上，来展示其性能。实验证明对于简单的基本操作来说其性能提升1.85倍，对于BFS &#x2F; CC &#x2F; RW &#x2F; MIS &#x2F; PR应用其加速了1.40 &#x2F; 1.42 &#x2F; 1.60 &#x2F; 1.56 &#x2F; 1.29倍。","link":"","tags":[]},{"title":"(ISCA2019)GraphSSD:图语义感知的SSD","date":"2024-05-07T02:43:39.000Z","path":"2024/05/07/ISCA2019-GraphSSD-图语义感知的SSD/","text":"GraphSSD: Graph Semantics Aware SSD 作者：Kiran Kumar Matam, Gunjae Koo, Haipeng Zha, Hung-Wei Tseng and Murali Annavaram @ University of Southern California 现有的Out-of-core图处理系统试图通过分块的方式减缓的I&#x2F;O访存瓶颈，然而这种方法产生了很多冗余的数据访问开销。本文提出的GraphSSD是一个在SSD上进行图存储、图访问和图分析的一体化方案。其主要创新点有： GraphSSD在用新颖的vertex-to-page框架替换掉传统的物理页映射机制，将冗余数据访问降到了最小 最小化了不必要的页面替换开销来支持图更新 提供了有效的编程接口，支持应用开发人员像访问内存内的数据一样访问图。 Introduction在图分析领域中，由于许多图的大小已经远远超过了现有的单机主存的大小，需要来考虑以存储为中心的图处理方案。在这类问题中，访问图数据的I&#x2F;O时间相比于CPU计算和内存的访问时间成为了新的系统性能瓶颈。 存储层面上来看，SSD的价格下降明显(大约100$每TB)。并且伴随着NVMe接口的普及，SSD在带宽上有了明显的提升。除此之外，SSD内部还提供一些计算功能来管理闪存。这种SSD的出现使得构建感知图语义的存储系统成为了可能。 所谓的“感知图语义的存储系统”，具体来说就是将图结构直接看成一种本地数据，而实际上它在存储一端。相比于以往都是将图看成成页的数据，这种方式可以再编程友好的前提下融合更多底层的访存优化。 背景和挑战现代可编程的SSD平台 SSD内部通过多个闪存信道(Flash Channels)来提供高数据带宽。物理页面是一条闪存命令能够读写的最小单元。 SSD的写操作比较特殊：闪存页面在被写入操作之前要被完全擦除，但因为擦除闪存单元往往需要更高的能耗，也会污染邻近的页面，所以擦除过程以块单位上进行。故擦除过程很明显的比读写操作要慢，这叫做写放大现象。SSD都不会在每次写入操作时都执行擦除-写入的过程，而是会将更新的页内容写到空的物理页。每当页数据被更新的时候，FTL会将闪存页的逻辑块(LBA)地址映射到新的物理页地址(PPA)上，并老的页设定为失效。SSD控制器将这个映射机制放置在FTL中(闪存翻译层)的映射表里。 此外，SSD控制器内部对于失效的页建立了一个垃圾回收机制，来创造更多空的物理页面。这个机制擦除整个block，将该block中所有有效的物理页面写入到不同的block中的另一个空的物理页中。 图更新虽然许多图处理系统都假设图是静态的，但是在实际应用场景中，节点或边信息有可能会被更新。基于此，GraphSSD支持基于Delta Graph机制的图更新。 Delta Graph机制将图更新作为一系列的快照。一开始，所有的节点使用一个连续的向量来保存他的邻居信息。当图更新产生的时候，多个更新会合并形成一个snapshot。在常规的运行间隙，一个snapshot会被写入到存储中去，而新的snapshot会建立，以容纳新的更新。graphSSD在常规的间隙也会合并snapshot，来为每个节点创建单一的连续邻接矩阵。 当使用基于CSR的图表示来合并更新时，有可能需要遍历整个图以归并更新，将其写到新的位置。而出于一致性考虑，只有在全部的归并操作完成之后，才能从新位置读取节点的邻接矩阵。因此，支持归并会对节点的遍历过程带来很大的性能损失。 架构设计 组成部分： SSD端：图翻译层(GTL)，图命令解码器 HOST端：图缓存和图更新日志管理 图命令解码器 GraphSSD向上层提供一系列图访问的API。每一个API函数会进而激活SSD控制器来完成访存工作。这些函数会通过host端向SSD端发送的NVMe命令来实现。 所有的NVMe命令首先被GraphSSD图命令解码器解码，图命令解码器将GraphSSD和常规的NVMe命令执行不同的处理路线。因此GraphSSD和其他传统的存储可以共存。 一般来说，所有GraphSSD函数处理的第一步都是在闪存中访问节点及邻边。为了与原始的物理页面访问机制(FTL)进行区别，GraphSSD建立了GTL层(graph translation layer)。 GTL层和图存储格式GTL(graph translation layer)将传统的LBA-to-PPN机制更改了，其直接将节点id映射到保存其邻接节点信息的物理NAND页序号。 GTL结构GTL中的每一项都将节点id映射到物理页数，并且还保存有一系列的状态信息。(dirty &#x2F; extension &#x2F; valid)。因为很多点其邻接边非常少，可以将多个节点的信息全部定位到同一个物理页中。这样能够减少GTL的项数。GTL上只保存给定物理页中最小的节点作表项。在GTL中保存的节点信息是按照节点序号的大小从小到大进行编号的。如果是有向图，GraphSSD建立两个GTL，分别表示入边信息和出边信息。 物理页面存储 在一个物理页中， 最后一部分是本页中包含的节点个数； 第二部分中保存有很多二元组&lt;节点，偏移量&gt;，二元组记录节点的id和本节点在全页的邻接向量中的开始偏移量的比特数，最后一个二元组保存有整个页的有效数据的结束位置。 一页的开始部分保存有节点邻接信息。这样保存的好处：能够减少GTT的大小，使得dram中可以保持一个非常大的GTT；使得访问一个节点id的全部信息不需要额外nand页的访问； GTT的不同情况： 节点Vi的所有信息都能保存在页中，平凡不讨论； 节点Vi的信息太大，一个页保存不下。多页保存同一个Vi。建立物理页映射，多个项和物理页对应一个节点。在满的一页上面写一个节点的信息； 节点Vi的信息恰好在拐角处，不允许这样的情况发生，采取规避措施 用GTL进行访存的例子GTL使用二分查找的方式对节点进行查找。对于节点Vi，我们要找到下标使得j&lt;&#x3D;i&lt;&#x3D;k。当定位到Vj时，在上面的二元组进行二分查找，是否能够找到Vi。如果找得到则获取其邻接矩阵；如果Vi的邻结点很多那么将找到很多的页，来构建其邻接矩阵。 如何在上图中找到V3的邻接矩阵？在GTT的节点id上进行二分查找，因为V3&#x3D;V3 &lt; V6,所以V3应该在P2中，之后P2会被取到，P2中的最后一个位置中写着3，也就是本页中会有三个节点。之后GTL会搜索后半部分来查找V3的偏移位置，也就是1。通过其下一项V4的位置3，我们可以知道V3的邻结点有两个，即为V1,V2。 支持图更新的设计GTT支持的操作AddEdge(在两个点之间加一条边)和AddVertex(添加一个节点及其邻接边和权值) AddEdge(Vid1,Vid2)：将Vid2添加在Vid1的邻接矩阵中，并且也要添加值到value中。因为Vid1有可能存在一个物理页或两个中，所以有两种情况，如下。 当Vid1在单一页中时：Vid被添加到Vid1邻结点的最后。在Vid1之后的其他节点都向更高的页偏移进行移动。这些节点的指针也在本页的最后被更新。特殊情况：有可能发生溢出，溢出时重新添加一页，并且将这部分节点信息大概分成两页每页里面的空白空间相近。这是为了在之后的更新中尽可能减少溢出。 对于一个加入的页，我们应该将本GTT中的所有内容都向后移动一些，在最差的情况下，有可能移动所有的GTT项。为了防止这种现象发生，我们采用了GTT项中的扩展位，设置一个指针指向两页的位置。如下图。 当Vid1在多页时，添加在最后一页的最后，和上述操作基本相似。 AddEdge的例子： 一致性考虑将更新写在DRAM中就会有断电失去状态的风险。重要的是在重启之后要保持和重启前相同的一致性。因此每个更新都需要在重启后自动进行。我们在开始更新过程之前先创建redo-log，保存了正在更新的GTL项、deltaPointer和deltaUpdate数据。更新过程会先写新的页，之后清空GTL dirty位，将GTLentry的指针指向新的一页，最终将原来的老的物理页面设置为无效。 如果在写新的页或者在写完页，没有更新dirty位的时候断电，重启时，redo-log会从选择另一个页进行写入开始整个更新过程；之前写入的页会被垃圾回收机制回收。 如果在dirty位更新之后断电，并且GTLentry没有改动，GTL会把dirty为设置回来，重新开始更新过程； 如果在GTLentry更新之后改动，但老的页面并没有设置为无效，redo-log会简单的把老的页面设置为无效，把deltaPointer也设置为无效。 Garbage Collection机制：如果该页正在被用于保存图数据，在移动这一页的时候，GC会通知GraphSSD的运行时更新GTT。正在移动的页面已经包含有其包含的最小节点信息的内容。将该GTT项的节点信息改为这个节点信息并且更新page地址为移动到的位置。 保证更新效率前面提起的图更新过程会导致非常多没有必要的物理页写入。每次写入都会引起read-modify-write的过程，导致非常明显的写放大现象。为了减少写放大，直到更新数量足够多或者到达一定的时间要求，所有的更新都被记录在DRAM端的log文件中。主机端的GraphSSD log manager为了处理日志功能而创建。 HOST端的logger 更新需要先检查被更新的节点或者边信息还在不在图里。因此主机端的logger向SSD发送一个请求，确认节点或者边的存在性。Logger也同时启动另一进程来检查边或点信息在日志文件中是否存在，这是因为要被更新的节点或边信息可能仍然存在在DRAM的log文件中，但是这个更新还没有被映射到SSD中。如果不存在则返回FALSE。注意到这只需要一个简单的读操作，不会触发物理页面的更新。 Delta graph 当DRAMlog文件满了或者计时器到达了设定时间时，HOST端的logger会初始化一个更新队列。GraphSSD采用Delta graph来进一步减少写放大现象。我们在graphSSD中用两个数组来实现delta graph，包括deltapointer和deltaUpdates。一个邻接表的所有更新会被放在deltaUpdate vector里。对于一个节点，新添加的更新会被链接在之前的更新之后。一个节点的DeltaPointer指向指向deltaUpdate中的最近的更新位置。 有DRAM日志和Delta graph的图访问 图访问机制在运行时，为了正确地重构图，需要知道delta graph的存在性。为了标志delta graph的存在，GraphSSD在GTT中设置了dirty bit。如果dirty bit被设置了，先找到当前的图，再利用DeltaPointer找到所有的对该节点的更新。 将原始图和delta graph合并 因为会导致比较大的写放大，因此最好周期性地将delta graph和原图进行合并。在更新过程中，我们循环过整个GTT，并且找到所有GTT项中dirty的项。对于这些节点，我们找到更新并把他们和原图的信息进行合并。在所有的节点更新做完之后，所有GTT中的dirty bit都应该被抹除 GraphSSD的缓存管理graph的数据会被缓存再host端一部分。要求连续的节点id，因为会有很多对于连续的节点进行更新的操作，所以这样的缓存机制还是非常有用的。为了减少开销，建立了graphSSD cache，缓存了GTT，并且缓存系统可以处理graph command。在cache有miss的时候进行nand的获取，单独的nand页获取可能会导致很多host端的访问，这样也减少了对于SSD的访问。HOST端的GTT是只读的，所有的更新操作会使cache在Host上的GTT无效，并且更新会被做在SSD上。 在SSD上的GC机制中，在一个NAND页上面的数据可能会被写到另一个NAND上。如果NAND页上保存的图数据被移动了，新的NAND页码应该在HOST上的GTT上被更新。为了更新主机端的内容，SSD控制器会向host端的cache管理器发送一个请求来自动使得主机端的缓存无效，再进行更新。 实验平台GraphSSD在OpenSSD上被评估。 平台：Xilinx Zynq-7000 可编程 SoC 核心：双核ARM酷腾A9处理器 PCIe 和 NAND flash信道设计为在可编程阵列的硬件逻辑；可嵌入的ARM核心运行SSD固件。此固件可以完成命令的处理，页缓存管理和FTL的功能。在GraphSSD中我们修改其固件的FTL部分，命令处理部分和主机端调用库，使得可以管理主机端的缓存情况。 内存：搭载1GB DDR DRAM和2TB Hynix NAND Flash DIMM,与可编程SoC相连接。 传输协议：与主机端使用PCIe Gen2 x 8接口相连，其支持4GB&#x2F;s的带宽。 OpenSSD上的NAND页大小为16KB；主机端系统使用4KB。 主机系统：Intel i7-4790CPU，16GB DDR3 DRAM。为了扩展NVMe命令，主机端安装NVMe ver.3.19 linux驱动器 数据测试图数据：com-friendster(CF) &#x2F; YahooWebScope(YWS) 结果本文章的实验首先从基础的GraphSSD提供的图访问API进行评估，主要是GetAdjacentVerteces和GetEdgeWeight这两个函数。之后GraphSSD在应用级别的表现。对于基础API，每条命令执行1M次，并且使用随机的点或者边作为此查询过程的起点。 基础API GraphSSD是baseline系统的1.85倍。性能提升原因的两个点： baseline使用block接口来访问rowPtr和colIdx；GraphSSD使用GTL直接找到节点的邻接信息。Baseline 必须访问两个不同的页面来找到rowPtr和colIdx，但是GraphSSD用图语义可知的系统通过GTL来减少NAND页的访问。即使有百万请求，host端的缓存系统能够帮助减少NAND页的访问。没有成倍的原因是rowPtr向量比较紧凑，可以被缓存系统缓存在HOST端上来减少rowPtr有关的NAND页接触。 序列化瓶颈：首先访问rowPtr，再访问colIdx，因此两个访问是有先后顺序的。着花样有可能减少SSD请求率，会导致SSD与HOST之间通信的低带宽。但是实际上因为多个请求并行发送，所以rowPtr和colIdx的请求是混杂在一起的，这种多线程的方式保证了很高的带宽利用。如下图所示。 应用使用Random-walk应用来呈现细节的结果，根据全部应用的实验结果进行详细的分析。A.Random-walk a图：Y轴为相对于baseline的加速比。X轴为迭代伦次。整体提升性能大概为1.6倍。性能的提升来自于两点：NAND page访问的数量和SSD带宽的利用。 b图：因为在randomwalk中，下一个节点是随机访问的，baseline系统缓存rowPtr的作用不大。而使用一个紧凑的GTL的表示，GraphSSD能够减少NAND物理页面的访问数量。 c图：在baseline里面，访问rowPtr到访问colIdx的顺序会导致对SSD带宽的利用不充分。GraphSSD可以更好的缓存GTL，并行地去访问邻接点，所以提高了带宽。 B.所有应用 对于BFS，将能够到达的位置占全图最长路径的长度作为横坐标。对于BFS &#x2F; CC &#x2F; MIS &#x2F; PR来说，性能分别提升1.40 &#x2F; 1.42 &#x2F; 1.56 &#x2F; 1.29倍。原因是更少的NAND页访问和更高效的带宽利用。 C. 与GraphChi对比 因为GraphChi是为Vertex-centric为主的模型设计的，例如BFS &#x2F; CC等这些不适合用这个模型的方案不能够使用。GraphChi会在每次迭代的时候会把所有的点都加入到内存中，但是其中只有一小部分的活跃点会被使用。和GraphChi来说跟它比较BFS不公平。与GraphSSD相比较的结果就是证明了其多样性。在一次更新过程中，将所有的节点都放入到内存中，对于BFS来说不适用。随着距离变长，GraphChi的性能逐渐变差，在每次迭代过程中，所有的块都会被读取。另一方面，PR这种更加适合vertex-centric的编程模型，许多点会被重复遍历，与GraphSSD相比，性能性能有了2.62倍的提升。shard在每次都要被放入内存中计算，即时在shard中的计算非常少。 D. GraphSSD的开销GTT的大小对于两个数据集来说分别是8MB和34MB，相对于整个DRAM来说非常小。原因是因为多个节点放在同一个页中，这就只需要一个项来实现映射。只有很少的节点需要多个GTT项(0% in CF and 0.01% in YWS)。如果每个节点一页，大概需要1GB到11.2GB。空间开销：当将节点的邻接点放进NAND页中，为了简化设计，如果邻接点放不进空闲空间中，我们使用新的一个NAND页。此设计的空间开销大概4.8%或者4.9%。如果使用更加负责的设计，例如追踪一个节点在不同页之间的邻结点划分，能够轻松的省下。 E. 图更新 a图：横坐标是一共有多少个delta图。可以看出查询操作GetAdjacentVertices的性能稳定地下降，因为在更新过程中访问图很有可能涉及到UpdatePointer的追踪操作。 b图：横坐标是在一个页中有多少空的位置，纵坐标是merge过程的时间。空位置越少，merge时间越长。 c图：因为merge过程中，如果要查询节点，如果她已经被更新过直接查找page即可，但是没更新的需要找pointers。横坐标表示了merge的比例，纵坐标是性能体现。 在SSD的merge过程中，有更新直接merge。在更新NAND过程中溢出会用额外的空页来补足。对于更新在很少的NAND页中的图，这也帮助了更快的merge。对于普通的CSR来说，更新需要将所有的图信息重写写到另一个地方。 结论GraphSSD是一个图语义感知的SSD架构，使得SSD控制器可以直接接触flash存储。GTL层负责节点信息向flash存储上物理页的转换，为了GTL的实现，提出了一个行之有效的编码方案，使得GTT的要求空间非常小。我们也提出了几个优化来处理图更新。我们完成了GraphSSD的设计在SSD的开发平台上，来展示其性能。实验证明对于简单的基本操作来说其性能提升1.85倍，对于BFS &#x2F; CC &#x2F; RW &#x2F; MIS &#x2F; PR应用其加速了1.40 &#x2F; 1.42 &#x2F; 1.60 &#x2F; 1.56 &#x2F; 1.29倍。","link":"","tags":[{"name":"近存储计算","slug":"近存储计算","permalink":"https://recoderchris.github.io/tags/%E8%BF%91%E5%AD%98%E5%82%A8%E8%AE%A1%E7%AE%97/"},{"name":"ISCA","slug":"ISCA","permalink":"https://recoderchris.github.io/tags/ISCA/"},{"name":"图计算","slug":"图计算","permalink":"https://recoderchris.github.io/tags/%E5%9B%BE%E8%AE%A1%E7%AE%97/"}]},{"title":"Algorithm-based Fault Tolerant(基于算法的容错)","date":"2023-02-08T09:44:29.000Z","path":"2023/02/08/ABFT/","text":"对ABFT容错的浅薄理解原理是针对某一种算法中的高频算子进行可靠性设计，以在较低的成本下保障算法大部分的可靠性。目前看到的两篇论文都是通过冗余计算，计算校验和对比结果来实现容错机制。这种方法相比于架构层和电路层的方法来说代价要更低，更加灵活。 相关论文的主要方法是： 设计校对矩阵 设计checksum校对方法 分析一下校对算法开销如何、能够实现什么级别的容错(可检测or可修正) 设计相关的容错框架或者系统 通过实验验证可靠性保证以及额外开销。 以后也可以多思考一下相关的算子有没有这样的机会做冗余计算。 矩阵操作ABFT设计原文：Algorithm-Based Fault Tolerance for Matrix Operations(1984, TC) 校对矩阵设计 简单来说就是有个矩阵M，然后里面每个元素的最大值(位宽决定)为8。这个矩阵的行校验矩阵$M_r$就是增加新的一列，这一列是前面几列的和，这个矩阵可以用来定位错误所在的行；同样这个矩阵的列校验矩阵$M_c$也同理定义，用来定位错误所在的列。$M_f$是矩阵的全校验矩阵，可以实现错误的定位和修复。 检错方法 适用于矩阵乘、加、LU分解、标量乘、转置操作 可以实现单比特错误检测(右下角的数值C和整个结果矩阵的和对比)、定位、纠错(通过行和列定位后再算一下) 可以实现多比特错误检测。 卷积操作ABFT设计FT-CNN: Algorithm-Based Fault Tolerance for Convolutional Neural Networks 校对矩阵 在这个卷积操作里面假设输入矩阵D和W都是思维矩阵，校对矩阵中$C_{d1}$和$C_{d2}$分别是$D_i$的和以及加权和。如果在全校验的情况下，checkSum会有很多个；不同的checkSum的计算难度不同，其检错难度也不同，因此适用于不同的场景，或者可以分层次进行检测，以使本方法的开销更小一些。这就是本文的核心思想，其具体思路可以再重新找一下论文。 不同校验和组合产生的检错维度 检测原理：以Checksum of Checksum为例 检错能力和checksum的复杂度分析 分层的纠错框架 实验结果：开销","link":"","tags":[{"name":"容错","slug":"容错","permalink":"https://recoderchris.github.io/tags/%E5%AE%B9%E9%94%99/"},{"name":"SDC","slug":"SDC","permalink":"https://recoderchris.github.io/tags/SDC/"},{"name":"论文阅读","slug":"论文阅读","permalink":"https://recoderchris.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"name":"软错误","slug":"软错误","permalink":"https://recoderchris.github.io/tags/%E8%BD%AF%E9%94%99%E8%AF%AF/"},{"name":"2021","slug":"2021","permalink":"https://recoderchris.github.io/tags/2021/"},{"name":"TPDS","slug":"TPDS","permalink":"https://recoderchris.github.io/tags/TPDS/"}]},{"title":"SDC调研：Meta两篇论文","date":"2023-02-06T02:07:35.000Z","path":"2023/02/06/Meta关于SDC的项目基金和已有的研究成果调研/","text":"背景Meta的服务(facebook, instagram, whatsapp, messengers)都依赖于数据中心的服务。 然而，静默数据错误(SDC)无法被大型系统所发现，在硬件层面无法被捕获，有可能会随着软硬件栈一直传播，最终形成应用层面的数据损失。这些损失可能需要数个月来修复，是大型基础计算设施中潜在的问题。以下在数据中心中出现的重大事故都与SDC问题相关： AWS S3 因 bit corruption 造成 36 小时的服务中断 Facebook 遇到 10%-15% 的图片无法访问 (48 小时) Netflix 宕机超过 3 天 潜在研究方向我们固然可以执行非常严厉的计划来避免 SDC 问题，然而无论多么精巧的设计都不能 100% 避免问题的发生，而且会急剧降低系统的整体性能。这就要求系统设计者学会在性能与可靠性之间做工程上的平衡。 针对以上前沿研究问题，Meta的系统和基础架构实验室已经开展了大量的工作并运用于云服务场景中以保证服务交付的准确性。经过多年的研究，Meta认为解决SDC问题有以下四个方向： 从**计算架构(Architecture)**层面处理SDC问题 架构层面上处理和转移SDC的方法，例如enhanced compute block ECC机制 自测试(self-test)的架构模块和模式，例如lockstep computing, checkpointing和redundant computing，这些方法都需要评估计算成本和性能代价 新颖的处理计算和内存错误的架构解决方案，包括但不仅限于增强传统的RAS架构。 从**分布式计算(Distributed Computing)**层面处理SDC的传播 遏制SDC传播的分布式计算弹性模型和解决方案 跨越多个子系统的错误检测能力 分布式规模的错误控制和测试机制 自测试SDC的分布式系统架构和恢复方案 从**软件(Software)**层面提供弹性服务 软件层面实现对SDC的弹性，例如冗余机制、概率和算法的容错机制 实现抗SDC的通用计算和数据搬运库 针对SDC的实时检测和遏制软件方法(需要评估计算成本和性能开销) 从过去的SDC中归纳SDC解决算法 从**硅设计(Silicon Design)**层面提供更可靠的硬件工艺 面向SDC问题，优化硅设计和制造策略 用于硅制造过程的进阶模拟、仿真和测试策略 硅测试的覆盖率评估、硅模块内发生故障的概率评估模型 用于SDC检测的生产过程中的测试例程开发 硅模块使用过程中逐渐衰退的模型和评估机制 Facebook在大规模集群中对SDC问题的理解以下内容来自论文《Silent Data Corruptions at Scale》(Arxiv) @Facebook。 补充背景以往的工作往往研究由于辐射或者合成错误注入产生的软错误问题，Meta的研究则注意到，由于设备的特性，SDC可能会发生并且大规模重复，这种SDC是可复现、非瞬态的。此外，在之前的错误注入模型的研究中，CPU的SDC概率被定为到百万分之一的几率级别，而在真实场景下，由于CPU功能模块中为了性能往往采用最小纠错机制，CPU SDC的概率要比预计高几个数量级。 产生缺陷的原因设备缺陷(Device Errors)在生产和设计阶段，设备本身有可能会有潜在的缺陷： 有些设计有可能有corner case。例如，在某个特定的功耗状态下管理cache控制器的模块，其功能会受到限制，有可能导致设备运行被卡住或者产生功能上发生错误； 在CPU布局布线期间，不能确定信号的到达时间，有可能导致错误的位翻转。例如timing path error 制造过程中，有可能所有的晶体管的蚀刻不够可靠，使得晶体管没有相同的峰值工作电压或功率阈值，各个设备的模块特点不同，导致制造上的错误。 早期故障(Early Life Failures)有些故障在生产过程中就被发现了，有些故障要等到硬件真正运行服务的过程中才会出现。根据晶体管内部缺陷的类型，故障可能会在运行前几周、几个月或预期设备寿命结束前的任何时间出现。 这些故障全部被归类为早期故障(Early Life Failures)。 设备老化(Degradation)随着频繁的使用，设备会逐渐老化。频繁使用的计算部分比 CPU 的其他部分老化的更快。与早期故障相比，这些由于设备老化而产生的故障并不常见，但仍有相关的例子，例如针对 DDR4 内存的RowHammer攻击。 芯片内部使用纠错机制(ECC)，可以防止设备内部性能下降。 晚期磨损(End-of-Life Wear-Out)当设备在现场服务工作负载一段时间后，超出其额定寿命，整个硅开始出现磨损，这在大多数组件中都可以观察到。 Case Study：应用层看SDC造成的影响Facebook中有大量不同类型的应用。我们以查询设施为例，最基本的查询基础设施是用来获取并执行SQL查询的设施，例如Spark, Presto, Hive等。下面以spark为例来描述SDC会给应用带来什么样的影响。 Spark 在Spark架构中，数据集被分为多个部分，这种数据集叫做弹性分布式数据集(Resilient Distributed Dataset, RDD)，每个数据集是单独并行执行的。执行过程是每个worker node上会分布有几个RDD块，每个worker首先做map，然后在shuffle reduce阶段将结果进行综合，最后再对用户提供请求的结果。 FaceBook解压缩应用FaceBook使用Spark进行压缩。这里，我们主要讨论解压缩的部分。当解压缩请求运行时，多组压缩文件会被输入到解压缩流水线中。在解压缩之前，系统会检查压缩文件的大小看是否大于0。如果结果大于0，才会执行解压缩。 在Spark中，解压缩流水线会提供文件的尺寸作为解压缩算法的输入，算法将执行Scala库的幂函数。有趣的是，FB在日常维护中发现了这样一个case study：幂函数会对一个已知具有非零大小的压缩文件返回0值。由于文件大小为0，因此这个文件不会被解压，这意味着随机丢失了一些文件。接下来，为压缩文件保留着k-v对的应用会发现这个错误，并且注意到此错误已经不可还原了，使得应用程序出错。 对以上问题做debug所有工程团队用日志记录下每一个worker的每一步的结果，并复现以上的错误→从日志中锁定出错机器→单机上复现错误→锁定错误只在某一核心上被某一组特定值触发→scala无法使用GDB调试，但可以用JVM兼容JAVA字节码 工具为了找出根本原因，Meta团队希望在不改变复现实验的含义的情况下，将编程语言从Scala转换为Java，再转换为JAVA字节码，最终到达指令级别来再通过GDB调试，以锁定错误出现的范围。但是java是即时编译的，并不能深入到指令级别，所以还需要一个提前的编译器编译出这些指令；或者需要一个探测器，可以在执行JAVA字节码时提供已经执行的二进制指令。 语言转换：Scala Compiler(scalac)，转换后可以生成交叉编译的java类型文件，可以当做java字节码 提前(ahead-of-time)编译器：GCJ，可以将java字节码转换成对象文件和二进制码，二进制码可以用GDB来debug。然而这个工具已经很久没有维护了，其他也没有更好的编译器工具。 **探测器(probe)**：JAVA提供+printassembly选项，用HotSpot追踪可以打印出已经运行出的汇编代码。 通过以上三个工具实现了GDB对代码进行debug。但是出来的都是汇编代码，且数量非常大(430K)，为了定位到错误的汇编代码，本文通过先将相关的函数筛选出来，再进行反向工程，实现对错误的定位。 很好的一些调试经验(略过了，太工程，大部分谷歌翻译了) 绝对地址引用：将绝对地址留在代码中跳转会导致分段错误。如果发现汇编的该部分对再现性没有依赖性，则最好消除绝对地址引用。 意外的分支：如果意外的分支和跳转调用没有被映射，代码会因分段错误而崩溃。最好限制复现器的可变性。 外部库引用：建议最好不要依赖外部库。 编译器优化：高性能代码具有多次编译器优化功能，观察数学方程式的优化有助于理解复制器所需的关键组件。 在逐步执行汇编指令时，优化可能不直观。 冗余指令：最好消除冗余指令，例如stub指令 输入&#x2F;输出寄存器：我们需要为关键指令识别数据输入和结果寄存器。识别后，必须添加额外的指令来提供用户输入、获得结果。 这实现了稳定的reproducer代码，并能够识别SDC的数据依赖性。 管理堆栈帧：独立的Reproducer需要适当地管理堆栈帧。 管理堆栈帧中的事务以防止缓冲区溢出或下溢对于稳定性至关重要。 没有堆栈框架，复制器代码无法管理基于堆栈的请求或函数调用。 内存偏移量引用：寄存器通常在指令中使用内存偏移量。 必须适当地初始化偏移量。 如果未计算和初始化偏移量，我们将遇到由于未初始化数据而导致的分段错误或复制器损坏。 特殊功能单元：需要监控特殊功能单元（如 ALU、DSP、FPU、AVX 等）的事务，它们会带来近似值。 此外，特殊功能单元利用不同的位宽、特殊功能寄存器和堆栈结构。 主框架：如果没有合适的主框架和功能框架，一个独立的reproducer是不完整的。 这使得代码可执行。 通过逆向工程，可以获得一个更简单的reproducer代码，之后通过GDB可以获得导致错误的指令，最终锁定在60行的汇编代码中。 重新审视应用的错误此后，本文注意到具有不同精度的不正确值，结果不尽相同。因此，应用程序可能解压缩了大小不正确的文件，并且将文件在没有在EoF终止符的时候下被错误截断。 这会导致文件节点悬空、数据丢失等无法跟踪的错误。 因为复杂的数据依赖以及数据的输入，如果无法没有复现代码，这种数据损坏几乎不可能被检测并溯源，尤其是在集群拥有数十万台机器、每秒执行几百万次计算的情况下。因此，以上的方法可以更快地溯源集群内SDC的根本原因。 对抗SDC的硬件方法我们观察到，在大规模基础设施中，SDC并不是局限于百万分之一概率的罕见事件。 这些错误是系统性的，不像其他故障模式那样容易理解。以下几种硬件方法可以降低处理器内的软错误率，对SDC也有效： 保护数据路径：使用ECC增强设备内的块，保护数据路径，提高设备的弹性。 专用的筛选模式：在制造流程中，设计专用的筛选和测试模式 了解大规模下SDC的行为：与大规模使用设备的客户密切合作，了解和评估SDC的影响。 研究发生率、生产故障时间、对频率、电压和环境条件的依赖性，有助于深入了解 SDC 的表现形式。 架构优先级：将来架构选择中，会优先考虑防止SDC的架构。 探测SDC的方法(不增加其他额外开销)为了探测到SDC，我们需要额外的机器来进行计算，之后与参考值比较结果。以下三种方法可以实现对SDC的探测： 找机会找机会利用处于维护状态的机器，并使用随机数据输入，执行指令级准确性验证。 这里的挑战在于其覆盖范围主要取决于机器有机会处于维护状态的频率。 在大型的集群中，我们并不希望有很大比例的机器处于这些状态。 周期性实施一个调度程序，定期监视机器的SDC覆盖率，然后根据定期计时器安排机器进行测试。这种方案的的开销很高。 对生产友好的方案当测试可以优化为最小的大小和运行时间时，可以使测试指令与机器上的工作负载同时执行。 结果被发送到收集器以通知机器的通过或失败状态。 此方法需要与工作负载密切配合，以免对生产工作负载产生任何不利影响。 软件容错机制为了处理静默错误，我们需要重新考虑基础架构软件设计理念和软件抽象的鲁棒性。 冗余机制防止应用程序级故障的更好方法是实施软件级冗余，并定期验证正在计算的数据在多个检查点是否准确。在将这些方法应用于大规模数据中心时，要考虑精确计算的成本。 冗余的代价对资源有着直接的影响：架构越冗余，重复资源池的需求就越大。 但是冗余为应用程序提供了容错的概率。 容错库将容错机制添加到 PyTorch 等知名开源库中将极大地帮助应用程序防止暴露于SDC。 构建容错的算法会增加应用程序的额外开销。因此，如果在性能下降可忽略不计的情况下，这一点可以被实现。 这项工作需要SDC研究社区和软件库社区之间的密切合作。 Facebook检测SDC问题的方法以下内容来自论文《Detecting silent data corruptions in the wild》(Arxiv) @Facebook。 硅测试流程在投入使用之前，基于硅的电子设备要经历不同的开发阶段。因此，我们要了解不同开发阶段所使用的测试策略，以理解集群范围内的与测试有关的开销，由此了解为什么测试是一件很难的事情。测试流程看重三点：测试量、测试时间、在该阶段出错产生的影响。 设计和验证(Design and Verification)采用模拟和仿真(Simulation and Emulation)的手段来进行方案的测试。测试的时间会很长，但是由于方案不断改变、新的器件会加入，所以测试周期一般会很短。在该阶段出错的代价相对来说是很低的 硅上验证(Post Silicon Validation)此阶段产生少量的样本进行验证。与之前一个阶段相比，设计加入了生产过程的变量。从开销上来说，这一阶段的验证会引入生产的开销。如果出错，需要对原始方案推倒重新设计，此外，在真实电路上开展测试的测试开销也很大。通过测试的方案被视为可以进行大规模生产。 生产厂商测试(Manufacturer testing)大规模生产之后，每个设备将用更加高级的测试设备进行自动测试。测试时间对生产通量有明显的影响，测试总量也从之前的几百个上升到上百万的量级。测试的开销随着数量也线性扩展。在这个阶段如果出错，开销会更大，往往导致重新设计或者重新生产该芯片。 集成测试这一阶段设备被运送到终端消费者手中，消费者用此设备集成到自己的开发环境中，集成阶段往往由集成者来协调。在这一阶段，需要跟其他不同的设备相配合，测试的复杂度也升高了；此外，测试的开销也从单设备到多种设备、多种配置的综合测试。此阶段出错，可能会导致不同的机架重新组装或者重新安装。 基础设施引入测试这一阶段主要是将机器接入网络，进行应用层面的测试。往往要持续几个小时到一天的时间。由于故障的来源多，所以锁定错误要更难一些。 基础设施集群测试正常的测试流程到上面就结束了。但是因为有SDC，如果不运行某个特定的测试程序是找不到SDC问题，以保护基础架构上运行的应用的。因此，按期进行检查是很有必要的。这种测试所需要花费的开销会更大，因为需要在保证程序正常运行的情况下对程序进行复杂的编排和调度。 此外，由于硬件的复杂度和配置的复杂度，分类和溯源错误是很昂贵的。因此，需要花费比较昂贵的开销，采用更多高级的方法来探测SDC。 SDC为什么是一个很难的问题？SDC可能的来源： 数据相关：SDC有可能天然与数据相关，也就是说这个CPU坏了，有可能大部分的计算还是正确的，但是就是一小部分是错误的。这使得测试的空间非常大。 电气因素：改变的电压、频率和电流会引起更多的错误，虽然在某一组电气配置下结果对了，但是其他情况不保证。这也使得测试空间非常大。 环境因素：地理位置的差异也会加速SDC问题的出现，这与温度、湿度都有很强的相关性。在大型集群中，也有可能因为workload不均匀一些机器温度高，使得某些计算中心结果和其他不一样。 寿命因素：随着时间，硅片的性能和可靠性也会改变。此外，SDC问题可能跟现有模型的预测有出入。因此今天计算正确，明天就不一定正确了。 从以上四点，本文总结保护集群以对抗SDC的唯一方法是通过不断改进的测试例程和高级测试模式生成来反复测试基础设施。 基础设施集群测试的方法(Infrastructure Fleet Testing)两个主要的测试方法： 停产测试(Out-of-production Testing) 不停产测试(In-production Testing) 停产测试(Out-of-production Testing) 停产测试是指让机器接受一组固定特征的输入，将其输出与已知参考值进行比较的过程。这种测试往往在机器没有执行生产负载的时候执行，并且测试要经历不同的温度、电压、机器条件和地域环境等复杂条件。 测试的特征往往是根据生产经验以及对硅架构本身的理解来生成的，以匹配常见的一些缺陷。一般来说，在大型基础设施中，总是会有一部分机器处于维护状态，在维护开始之前，机器上的负载会被迁移下来，这叫做draining phase。之后会进行维护，例如固件升级、内核升级、环境配置、修理等措施，之后再重新将工作负载迁移回来。 之后进行停产测试：用于停产测试的工具是Meta开发的Fleetscanner。Fleetscanner会找机会识别那些进入和退出维护状态的机器，并且使已经完成维护的机器进入停产测试SDC阶段。在所有情况下，根据可用的测试时间、识别的机器类型，fleetscanner会运行优化的测试，并提供设备对敏感的架构代码路径的响应快照，并验证计算是否准确。此时，大量的机器相关的参数会被瞬间捕捉到，以在分析过程中理解设备出错的具体条件。任何被鉴别为出SDC错误的机器会被放进quarantine pool中，以待进一步的观察和测试。 不停产测试(In-production Testing)不停产测试是指在集群内机器仍然运行工作负载的情况下，同时进行测试。如果不对测试例程和工作负载有细致的了解，则很难实现这一点。Meta实现了一种叫做ripple的测试方法，与工作负载同时存在，并且每次实现毫秒级别的测试时长。在停产测试中使用的测试序列被修改，以通过ripple测试。 影子测试 Meta实施了可以跨多组工作负载的ripple，并且精心设计了ripple测试架构，以防止整个集群的错误。 影子测试的一个主要挑战是实现托管。 基于工作负载的扩展，测试规模必须缩小。 对于 Meta 的每种类型的工作负载，确定了缩放因子的评估过程。此外，Meta还建立了与测试相关的开销分析，每种工作负载类型的托管都会提供开销，该工具的目标是将其开销降至特定阈值以下。通过反复试验，我们建立了控制结构和保障措施，以便为不同的工作负载启用不同的选项，然后将解决方案扩展到整个集群。 持续不停产持续不停产的纠错机制始终处于开启状态，只有它运行的规模是通过集群内的配置动态控制的。 这种方法在查找需要对相同数据输入进行数千次迭代的缺陷，以及识别正在退化的设备等方面非常有效。 在几周内，通过测试随机化和缺陷模式匹配，可以将设备中识别的新签名扩展到整个设备群。 这种方法在识别硅过渡缺陷方面也非常有效。 效果对比 根据三年的真实测试对比两种策略。 使用 ripple 可以在 15 天内完成 70% 的常见覆盖检测，Fleetscanner 在 6 个月内将覆盖范围扩大到剩余的 23% 独特的 7% 覆盖率是通过集群内重复的ripple实例实现的。 通过不断地重新审视和评估这些覆盖率指标，可以知道围绕测试向量、测试节奏和测试运行时间的集群范围内的测试策略。 对于不同类型的缺陷，覆盖率会有所不同。","link":"","tags":[{"name":"容错","slug":"容错","permalink":"https://recoderchris.github.io/tags/%E5%AE%B9%E9%94%99/"},{"name":"SDC","slug":"SDC","permalink":"https://recoderchris.github.io/tags/SDC/"},{"name":"论文阅读","slug":"论文阅读","permalink":"https://recoderchris.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"name":"软错误","slug":"软错误","permalink":"https://recoderchris.github.io/tags/%E8%BD%AF%E9%94%99%E8%AF%AF/"},{"name":"2021","slug":"2021","permalink":"https://recoderchris.github.io/tags/2021/"}]},{"title":"(HPCA06)从体系结构角度看软错误问题","date":"2023-02-04T13:34:34.000Z","path":"2023/02/04/软错误问题/","text":"以下内容来自HPCA’06的论文《The Soft Error Problem: An Architectural Perspective》。 背景知识什么是软错误高能粒子(e.g. 宇宙射线中的中子和封装材料中的α粒子)通过半导体设备(电路)时，会在其上产生电子空穴对(electron-hole pairs)，这会可能会导致瞬态错误(Transient Fault)。具体来说，电子空穴对会对晶体管的源(source)和扩散节点充电，如果充电量到达一定的阈值，会使得设备上某个组件(e.g. SRAM、锁存器、门电路)的逻辑状态发生改变，最终使得整个电路的运行结果产生逻辑错误，这种因为瞬态错误继而产生的运行错误被称为**软错误(soft error)**。 1PS: 与此相对应，硬错误(hard error)是指电路原件发生破坏，错误无法在短时间内修正的错误。个人理解是：软错误相对于硬错误来说是一种逻辑错误，而不是真实硬件发生了错误。软错误在接下来也许很难复现。 一般来说，每个电子元器件器件的纯软错误率(raw error rate)是一个常数。随着工艺水平的提升，单位面积的电路中将会集成更多的电子元器件，因而整个电路的纯软错误率线性增长。为了降低纯软错误率，一些DRAM厂商通过减缓单位电容的提升、降低供电电压等措施来降低每比特的软错误率，以应对工艺迭代会造成的软错误率提升。 软错误：从工业界到学术界工业界中亟待解决的因为软错误而产生的问题： 在芯片设计中，厂商希望能够了解软错误会对其设计产生多大的影响 希望能够从现有的技术中选择一种合适的策略来降低软错误产生的影响，以达到通过最小代价换取其可靠性指标的目的。 而学术界要解决以下的具体问题： 开发更好的设计框架、分析技术和软工具，以对软错误造成的系统影响产生更直观的理解和更量化的测量 探索和刻画软错误避免、探测和恢复的技术，在合理的性能、功耗、面积和复杂度要求下，满足对不同可靠性指标的要求 单比特软错误的分类SDC &amp; DUE **SDC(Silent Data Corruption，静默数据错误)**：没有被探测到、没有被修复，且对最终结果产生了影响的软错误。 **DUE(Detected Unrecoverable Error，被探测到且无法修复错误)**：被探测到了，但是无法被修复的错误。 目前，业界认为软错误率为SDC和DUE之和。DUE错误因为无法修正，所以它并不会影响整体的错误率。对抗DUE错误的最基本的对抗办法是fail-stop(重新运行)，以保证最后结果是正确的，但是它实际上并没有降低错误率。 DUE有两种分类： 第一种分类：会对最终结果产生影响的被称为true DUE，不会产生影响的被成为false DUE。保守的系统会对所有的DUE错误进行处理，一些系统则会分辨false DUE，例如错误选择路径的指令，来分门别类进行处理。 另一种分类：根据重新运行方式被分为process-kill和system-kill的DUE。所谓的process-kill是指一些DUE(e.g. 奇偶位校验错误)产生后，操作系统会隔离开这个错误，锁定一组出错的进程将它们杀死，保留其他进程正常运行，这种DUE被称为process-kill DUE；而另一种DUE是只能将整个系统关闭并重启才可以修复的错误，被称为system-kill DUE。 衡量软错误的单位：FIT和MTTF **FIT(failure in time)**：是指在109小时内产生的错误次数。SDC和DUE都可以用它来量化，FIT具有可加性，例如芯片的FIT可以用它上面所有组件的FIT求和得到。SDC和DUE的FIT数值和是整个芯片或系统的SER(soft error rate，软错误率)。 **MTTF(mean time to failure)**：是FIT的倒数，一种更加直观的展示。含义是经过多久系统会发生一次软错误。例如：1FIT&#x3D;114 years 一般来说，厂商都会在推出产品前规定自己产品的软错误率预算。例如IBM的power4宣称其SDC MTTF&#x3D;1000yrs，process-kill DUE MTTF&#x3D;10yrs，system-kill DUE MTTF&#x3D;25yrs。有更好的错误检测机制的系统，其SDC FIT会越低，但是DUE会更高。只有同时具备良好的检测(detection)和恢复(recovery)机制的系统，SDC和DUE FIT都会更低，使得SER会更低。 计算或估计SDC和DUE FIT芯片或系统设计团队要面临的一大问题是：设计出来的产品能否满足其软错误率预算的要求。当然，最准确的测量设计的软错误率的方法应该是这样的：使用加速中子或者放射性阿尔法粒子对真实电路进行实验。但是这种方法的实验代价太大，且需要将产品研制出来之后再测试。这种方法显然既耗时间，又不经济，不符合在芯片设计阶段的要求。 要计算或者估计设计的软错误率，目前普遍采用的是建模和计算的思想。首先，软错误率为SDC FIT和DUE FIT之和，之后再将整个系统或芯片的FIT拆解成每个组件的FIT。而每个组件的FIT又可以分为以下两个部分： 原始设备错误率(raw circuit error, 指瞬态错误能够导致组件状态转变的可能性) 体系结构脆弱因子(AVF, 指这个组件的状态变化，能够产生架构层面上可见的SDC&#x2F;DUE的可能性) 由此可见，计算设计的软错误率需要计算原始设备错误率和体系结构脆弱因子两项。 原始设备错误率每个组件的原始设备错误率由两个因素来决定：环境中的粒子流量和由工艺和实现决定的底层电路错误率。 粒子流量(particle flux)错误率基本和环境中的粒子流量成线性关系。环境中的粒子可以通过施加适当的屏蔽机制来实现一定程度的屏蔽，但是一般来说效果并不好。(需要很厚的混凝土隔离，不好实现)此外，随着海拔高度升高，粒子流量也会上升。 底层电路错误率(circuit error rate)电路错误率又由两个因素来决定：原始电路错误率和时间脆弱性因子(TVF, time vulnerability factor)。 原始电路错误率：指的是特定的电路单元会发生比特反转的可能性。这个跟比特反转的充电量Qcrit有关系，而这个数值由电路的电容和电压决定。一般来说，为了估算原始电路错误率，可以通过在不同时间点模拟电路单元(cell)的所有节点上的所有尺度的电流脉冲来计算原始错误率。但是这个花费的时间复杂度太高。因此，对于全芯片模拟，通常我们使用近似模型或蒙特卡洛模拟技术。 TVF：它是指在一个周期内一个比特反转会被捕获到的概率。例如，对于DRAM来说，因为它每个周期都会刷新一次，所以其被捕捉到的概率是100%；对于锁存器，只有在其内部保存数据的时候电路翻转才会被捕捉到，故概率大概是50%；对于一些静态逻辑电路，例如NAND电路，它只有在其前向电路(例如：前向电路中有锁存器)捕捉到这一变化时才有效。 123例子：计算原始设备利用率一般假定FIT/bit在0.001~0.01之间(原始电路错误率)，我们假定最小值0.001。TVF=50%，目标系统有4个CPU，SDC FIT预算为114FIT，我们可以算出每个处理器中最多有57,000(=114FIT/(0.001FIT/bit*0.5TVF*4CPU))个不受保护的锁存器。 SDC AVF SDC AVF：一个比特反转真正能够导致系统实质性执行错误的可能性，因为这个反转既没有被处理，也没有被探测到。 比特反转并不一定造成实质性执行错误：例如，如果分支预测的寄存器发生比特反转，不会影响程序的执行结果(SDC AVF&#x3D;0%)；而如果程序计数器的寄存器发生比特反转，程序大概率会被影响(SDC AVF&#x3D;100%)；指令队列中的寄存器比特反转就不一定了，如果里面存储的是错误路径的指令，那么SDC AVF很低；而如果是关键路径，那么SDC AVF就很高了。 ACE(architecturally correct execution)：ACE代表任何一种能够向用户产生正确系统执行结果的执行。 ACE bit：是指其中包含的信息一旦改变，将会影响程序的最终结果。un-ACE bit反之。 一个存储单元的SDC AVF是其中ACE bit的占比。如果一个程序执行了1000万个周期，其中的一个存储单元包含100万个周期的ACE位，则该单元的SDC AVF为10%。一个设计的SDC AVF是所有单元SDC AVF的总和。 DUE AVF DUE AVF是指比特反转会导致DUE的概率。它是true DUE和false DUE的总和。 True DUE AVF实际上是SDC AVF中被探测到的那部分，它在数值上等于老的SDC AVF 计算SDC和DUE AVF有三种方法：统计性错误注入、分析模型和性能模型(模拟器)。 统计性错误注入(Statistical Fault Injection, SFI) SFI是一种历经时间检验的测量脆弱性参数的方法 方法：在被研究的RTL设计结构中引入比特反转(时间空间随机)，之后运行设计，比较没有错误的模型和当前模型的体系结构状态。在运行几个模拟cycle之后，如果对比没有差异，说明错误潜伏在处理器中，或者已经被掩盖了。后者可以通过继续比较微结构状态来说明。此架构的AVF就是结果差异数量和比特反转数量的比值。（Architectural state includes main memory, architectural registers, and the program counter. Architectural state is defined by the instruction set architecture and can be manipulated by the programmer using instructions.） 优点：有力、不需要对处理器内部设计了解。 缺点：只能在详细的模型(RTL级，需要建模所有的比特)生效，对于每一个注入错误比特，需要上万cycle的测试，相当耗费时间；有错误和无错误的模型之间的不匹配不一定意味着存在错误，因为体系结构状态实际上可能包含非ACE位，例如动态死寄存器值。 分析模型 场景：比特流未经修改且没有重复地流经电路 Little’s Law: N &#x3D; B x L N – 结构中的平均比特数量 B – 每个周期进入结构的平均比特带宽 L – 每个比特通过结构的平均延迟 分析模型：$$\\frac{B_{ace}\\times L_{ace}}{number,of,bits,in,structure}$$ 举例：对于指令队列，Bace是 IPC（每周期指令数）乘以每条指令的 ACE 位数。 Lace 是指令在指令队列中的驻留周期。 当性能模型和 RTL 都不可用时，此方法在设计的早期阶段很有用。 性能模型 基本思想：识别流经机器的比特哪些是 ACE状态，哪些不是 ACE状态。 根据定义，每个比特位包含 ACE 状态的时间占比是位的 AVF，此过程被称为寿命分析。 主要挑战：确定每个位寿命的un-ACE 部分。导致un-ACE 状态的例子(在指令中)有dynamically dead&#x2F;wrong-path&#x2F;falsely predicted instruction。因而生命周期分析需要深入了解架构和微架构。 优势：与 SFI 不同，性能模型中的 ACE 分析要快得多，因为可以在一个实验中计算大量处理器结构的 AVF。 此外，性能模型可以实际运行数千万个周期，此它可以提供比 SFI 更高的准确性。 减小软错误率的方法以上只是介绍了如何估计一个设计的软错误率。而真正要减少软错误率，可以从以下三个方面入手：制造工艺、电路层面和架构层面。 处理工艺层面 方案：绝缘体上硅 (silicon-on-insulator, SOI)。 原理：因为其硅层薄的多，SOI 器件从 alpha 或中子粒子撞击中收集的电荷较少。 效果或优势：SOI 工艺可以使 SRAM 设备的 SER 降低 5 倍。(IBM) 缺点：尚不清楚是否会从 SOI 锁存器和逻辑器件中获得类似的 SER 降低；SOI 芯片的批量生产仍然是一个挑战。 电路层面 方案：调整设备参数、创建抗辐射（或抗辐射）单元。 原理：增加设备的电容和&#x2F;或电源电压，这两者都会提高 Qcrit，来降低软错误率； 抗辐射单元(cell)可能包含冗余状态，可以用来从软错误中恢复。 缺点：抗辐射单元会带来显着的面积和功耗开销。 架构层面架构解决方案可能比电路级解决方案更有效： 错误的【定义】通常存在于体系结构中（例如，对分支预测器的攻击不会导致微处理器中出现错误）。 典型的架构层面解决方案（例如奇偶校验或 ECC）的开销通常可以分摊到大量的位上。例如，ECC 的开销为每 64 位数据额外多出8位（即 13%），而抗辐射单元可能有 30-100% 的面积损失 微观层面(micro solution) 奇偶校验(parity)：奇偶校验可以检测任何单比特错误，但只能检测，不能修正。 受奇偶校验保护的位，通常具有SDC AVF&#x3D;0，但DUE AVF不等于0。 但是，奇偶校验保护结构的 DUE AVF 可以通过架构知识减少到零。 例如，受保护的直写缓存可以使奇偶校验错误的块无效，并从较低级别的缓存中重新获取正确的块。 SECDED ECC(single error correct, double error detected)：通常用于处理器高速缓存，可以纠正所有单比特错误并检测所有双比特错误，为单比特错误提供零SDC 和 DUE AVF。 ECC 可以inline或out-of-band实现：inline ECC 需要在读取返回数据之前计算并验证 ECC 代码是否正确，这通常会在处理器流水线中产生一个或多个额外的周期；out-of-band ECC 检查允许处理器继续读取数据，如果 ECC 检查检测到错误，读取不正确数据的指令重新提交，缓存数据被更正。 π 比特：π 比特是一种错误传播机制，可减少false DUE。 检测到错误后，不会立即发出错误信号，而是将错误发布在π位中并传播，直到有更多有价值的信息出现。 例如，在奇偶校验中，不会保护的寄存器文件中直接引发错误，而是可以通过在读取特定寄存器的指令时设置π位来发布错误。如果确定有问题的指令在错误的路径上，则忽略π位，避免false DUE 事件的发生。 宏观层面(macro solution)微观层面的方法可能需要大量的面积和设计工作负担。 因此，在某些情况下，使用 CPU 或线程进行故障检测可能会更简单。两种广泛的故障检测解决方案包括流水线的逐周期锁步(lockstepping) 和**冗余多线程 (redundant multithreading, RMT)**。 逐周期锁步： 在逐周期锁步中，同一的程序在相同的流水线上运行，每个周期都会检查二者的输出是否一致。 RMT：在指令的提交点检查选定的指令的输出是否匹配。 与锁步不同，RMT 并不需要两个线程每个周期进行同步。 以上方法降低了 SDC 率，但增加了 DUE 率。 降低处理器的 DUE 率需要通过硬件或软件进行错误恢复实现。从检测到的错误中恢复需要识别有问题的处理器并保持可以启动恢复的正确状态。 当外部检查器检测到错误时，可以从内部错误信号中识别有问题的处理器；或者可以定期检查处理器的状态，并在错误回滚时从检查点重启流水线或线程。 未来方向 计算不同架构的 SDC 和 DUE AVF：需要详细的生命周期分析和潜在的新技术，以识别处理器结构中的 ACE 和非 ACE 组件。 适用于不同处理器结构的AVF缩减技术 保护数据通过的微架构状态：现代处理器芯片既包含处理器内核，也包含系统组件，例如内存控制器和路由器。 保护流经“非核心”部分的数据可能并不难，我们可以通过在流水线中创建数据的位置，生成错误保护位提供端到端的错误保护，并让其保持不变地流动，直到数据被使用。 然而，保护数据通过的微架构的状态可能需要进一步研究。 RMT 的软件版本：锁步基本上是一个硬件概念，而 RMT 可以在硬件或软件中实现。与之前的软件故障检测实施相比，RMT 模型允许设计人员减少必要的软件检查次数。 此外，软件无法完全了解硬件，因此硬件可能必须有选择地去保护RMT无法涵盖的一些结构，这涉及软硬件协同。 了解和描述软错误与功率的权衡关系： 当电源电压降低时，软错误率会急剧上升。 二者如何权衡是下一步要解决的一个问题。 来自其他问题的软错误：例如电源噪声、耦合等。这需要详细了解不同的故障模型，但关于软错误的许多定义可以被转移到这些类型的故障中。","link":"","tags":[{"name":"容错","slug":"容错","permalink":"https://recoderchris.github.io/tags/%E5%AE%B9%E9%94%99/"},{"name":"SDC","slug":"SDC","permalink":"https://recoderchris.github.io/tags/SDC/"},{"name":"论文阅读","slug":"论文阅读","permalink":"https://recoderchris.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"name":"软错误","slug":"软错误","permalink":"https://recoderchris.github.io/tags/%E8%BD%AF%E9%94%99%E8%AF%AF/"},{"name":"Umich","slug":"Umich","permalink":"https://recoderchris.github.io/tags/Umich/"},{"name":"2006","slug":"2006","permalink":"https://recoderchris.github.io/tags/2006/"}]},{"title":"(MICRO22)Networked SSD:面向高带宽SSD设备的Flash memory互连网络","date":"2022-12-02T14:29:49.000Z","path":"2022/12/02/networked-SSD/","text":"Networked SSD: Flash Memory Interconnection Network for High-Bandwidth SSD 作者：Jiho Kim, Seokwon Kang, Yongjun Park, John Kim @ KAIST(Korea) 相比于以往flash存储总线中专用信号通信的机制，本工作利用packetized通信机制来扩展flash memory的带宽；提出了适用于flash memory的互连网络拓扑，提升了数据转移路径的多样性；在此基础上，设计了最小化IO干扰的GC机制。 论文为什么要做这件事？ 图 过去15年Flash memory的写带宽以及flash channel总线带宽发展趋势 趋势+Flash channel与flash memory一对多，使得flash channel总线上的带宽成为系统瓶颈 图 SSD内部结构 现有方案的限制：flash channel使用多种专用信号进行控制，这导致了很低的带宽利用率。 例：NV-DDR4接口中，10&#x2F;18个引脚被用来数据传输，其他的引脚为专用控制接口，不能用来数据通信 图 在Open NAND Flash Interface(ONFi)中的Flash接口信号描述（10&#x2F;18） 替代方案：将控制信号的信息融入报文(packet)，使用packet在SSD控制器和flash memory之间通信，尽可能多的利用现有物理信道所能提供的带宽，起到扩大flash channel带宽的作用。事实上，通过提升硬件频率或者增加新的信道引脚也能提升性能，但是这些方案受到功率或物理引脚的限制。 packet通信的另一好处：支持flash之间的互连。当实现flash之间的互连后： 可以将SSD的内部通信(指flash之间的通信，例如GC)和外部的通信(指flash与host的通信，例如IO)分离开 可以提供数据转移路径的多样性，以实现不同数据channel之间的负载均衡。当信道带宽如果是系统的瓶颈时，负载均衡带来的影响将更加严重。 图 8C8W-SSD系统中读写操作时信道的负载均衡表现。写均衡，读不均衡 当信道带宽提升2倍时，在下面8个应用中，性能预期能够平均提升85%。(减轻了负载不平衡的负担)在个别非常不均衡的应用上，性能能提升6倍。 图 不同workload下，当提升带宽时，模拟应用的性能表现。最高模拟到信道带宽提升2倍 这个事情之前别人是怎么做的，存在什么问题？ **互连网络(Interconnection Network)**：提出了大量网络拓扑结构。包括直接拓扑、间接拓扑、NoC拓扑等。Network-on-SSD也是第一个提出替代传统的flash memory总线的SSD内部网络的方案。核心区别是：本文认为多跳网络在SSD中并不实用。之前也没有利用多维总线拓扑来实现SSD内部flash的互联系统的方案。 基于packet的(packet-based communication)通信：基于packet的接口在NoC、memory-semantic fabric(GenZ, CXL)等实现处理器、加速器、内存之间互连的方案中都有涉及。但是由于协议开销，它们不一定适用于SSD内的内部芯片间通信。此外，SSD中芯片到芯片的接口限制带来了独特的挑战，包括有限的引脚接口、对硬件路由器或交换机的有限支持等。 该论文解决了什么难点，难点存在的原因是什么，作者是如何发现这些难点的？难点1：最小化额外开销 packetized通信的额外开销需要最小化。 例如，在flash中使用带有缓冲区的传统路由器或交换机是不可行的。 难点2：网络拓扑设计 虽然packetized接口为实现flash之间互连的网络打下了基础，但由于flash的引脚带宽有限，先前提出的网络拓扑不适用。 难点3：Flash之间信道如何利用？ 当前的系统对新型的网络拓扑无法利用。本文提出了新的GC机制，说明了如何利用这种连通性，通过分离I&#x2F;O和GC来显著降低尾延迟。 针对以上难点，作者各做了哪些优化，优化背后的思想是什么？本工作主要贡献点包含三个方面： 用packetization的思想拓宽了信道的带宽，实现了packetized SSD 建立在packet之上，设计了flash之间互连的网络拓扑 应用新的网络拓扑实现了一种新的垃圾回收机制 Pakectized SSD架构基于packet的Flash互连 图 方框图的接口(a) 基于信号的传统SSD机制 (b) 基于packet的接口，修改了控制器和flash存储 两个保留的控制信号：CE和R&#x2F;B，实现Flash控制器和Flash memory之间的握手通信机制，以决定哪个flash memory能够访问信道。其他的控制信号全部被打包进packet。数据通道引脚（DQ）因此而扩展了一倍，从8位变到了16位。 图 读事务的时序图。(a)基于信号接口的传统**SSD (b) 基于packet接口的SSD 时序图分析：基于packet接口的SSD由两种信息类型，一种是控制packet，另一种是数据packet。控制packet负责必要的命令和地址，数据packe包含带有数据头的页数据。不同点：(a)用RE_n信号指示数据读取，(b)增加了一条新的指令Read data transfer来指示数据读取。 微架构 Flash信道控制器的微架构 握手机制：通过CE和R&#x2F;B信号实现 其他改动：将原来在Flash控制器中的时序生成器移动到flash memory内部，flash控制器负责生成带有数据头的packet，数据头用来表示这个packet的类别是数据或控制包。 这样做提供了两倍的flash channel带宽。 Flash memory的接口 图 (a)flash memory架构的接口图 (b) die上的控制器 (c) die上的数据平面 不变的部分：虽然flash controller和flash memory之间的接口被修改了，但由于在外部引脚和内部flash die之间引入了控制器逻辑，因此内部的flash和组织未进行修改。 on-die controller：负责解译packet头，为底层的flash memory生成相应的控制信号信息。其输入为DQ和握手信号，实现方面主要是使用状态机生成合适的控制信号。 On-die data plane：包含一个额外的buffer，我们叫做V-page寄存器组。在后面的flash间通信会用到这个模块，它可以提供flash之间通信的路径。如果仅仅是想扩大带宽，这一部件也可以不用。 Packet On-die controller通过packet头的type位来判断包的类型是控制包还是数据包。 如果是控制包：包的大小由T,C,R四位来决定，在Flash memory内部生成信号的模式跟在flash controller中生成信号的模式非常相像。 如果是数据包：包的大小由第二个字节决定。 额外开销：只有包头的部分，大概是两个字节。但是在packet下信道被拓宽为之前的两倍，一次可以发送16bits的数据，开销可以忽略不计。 在packet将带宽拓展为之前的两倍后，考虑以下两种选项： 带宽扩展为原有带宽的两倍；(pSSD) 带宽与baseline仍然一样，用额外扩展出来的一倍带宽创建网络，提供flash之间的互连。(pnSSD) Flash之间的互连网络拓扑本工作仍然利用现有的总线结构，但是将其扩展到二维，创建2D总线拓扑结构来解决SSD内部网络的问题。 Omnibus网络拓扑 除了水平的总线，试图增加垂直总线，将带宽平均分为两份。(图b) 挑战：总的带宽恢复为原始状态；垂直总线的使用方式是新的挑战。例如，水平总线的大多数控制来自flash controller，但在（b）中是不可能的，在b的拓扑结构中每个列中的flash之一需要有效地充当每个垂直通道总线的控制器。 方案：修改为图(c)的拓扑结构，这利用了在packet化之后，flash控制器引脚带宽能够变为之前的两倍且可用。此时一个flash channel控制器可以控制两条总线，一个水平总线，一个垂直总线。 注意：跟以往网络拓扑方案中每个节点既可以做控制也可以做数据交换不同，此网络中控制(flash controller)与数据交换(flash memory)层完全分离 数据平面：路径的多样性 图 展示在flash内部互联上的路径多样性 例1：将A中的数据迁移到B上 例2：将C中的数据读出来 split选项：可以将一个信息分为两半从两条路径传输过去，同时利用水平和垂直信道。 控制平面 图 在flash channel控制器之间的控制平面的作用 当进行一个垂直数据移动时(从源控制器到目标控制器)： 例1：C0为源控制器(C0-&gt;C1)：1-2. 确定on-die plane状态 3. 访问v-channel 0，设置vCE使能信号 4. 发送转移page命令 5. 进行通信 例2：C0为目标控制器(C2-&gt;C0)：接受到C2的请求后，在on-die plane buffer准备好之后，确保vCE使能信号激活， 例3：C0为中间控制器(C2-&gt;C3)，在到达C3之前先到达C0 建立互连网络上的垃圾回收机制(Spatial GC机制)IO与垃圾回收分组 将进行IO和进行GC的flash memory分开，使得当I&#x2F;O组进行I&#x2F;O服务的时候，GC组不会打扰I&#x2F;O请求的进行。（b-a-b-c） 在有垂直信道的情况下，在GC组可以只通过v-channel进行垃圾回收，不影响水平信道。 完全消除了垃圾回收对写操作的影响，但是垃圾回收对读操作的干扰仍然无法避免。一般将GC组设计为四分之一大小。 I&#x2F;O与垃圾回收同时操作 使用设计的互连网络，这种垃圾回收机制的优势能够更加凸显出来，因为它们占用的是不同的信道。此时I&#x2F;O与垃圾回收可以同时进行，最小化干扰。 实验结果实验配置SimpleSSD-standalone模拟器 I&#x2F;O表现 pSSD: 69%, pnSSD: 60%, pnSSD-split:82% 垃圾回收效果 相比于baseline和pSSD，pnSSD平均提升性能9.7X、 5.9X","link":"","tags":[{"name":"论文阅读","slug":"论文阅读","permalink":"https://recoderchris.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"name":"近存储计算","slug":"近存储计算","permalink":"https://recoderchris.github.io/tags/%E8%BF%91%E5%AD%98%E5%82%A8%E8%AE%A1%E7%AE%97/"},{"name":"KAIST","slug":"KAIST","permalink":"https://recoderchris.github.io/tags/KAIST/"},{"name":"MICRO","slug":"MICRO","permalink":"https://recoderchris.github.io/tags/MICRO/"},{"name":"2022","slug":"2022","permalink":"https://recoderchris.github.io/tags/2022/"},{"name":"数据管理","slug":"数据管理","permalink":"https://recoderchris.github.io/tags/%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86/"},{"name":"存储技术","slug":"存储技术","permalink":"https://recoderchris.github.io/tags/%E5%AD%98%E5%82%A8%E6%8A%80%E6%9C%AF/"}]},{"title":"(MICRO18) Amber:一个可以准确、详细地模拟全部SSD内部资源的模拟器(SimpleSSD-v2)","date":"2022-10-20T14:26:31.000Z","path":"2022/10/20/amber/","text":"Amber: Enabling Precise Full-System Simulation with Detailed Modeling of All SSD Resources 作者：Donghyun Gouk, Miryeong Kwon, Jie Zhang, Sungjoon Koh, Wonil Choi, Nam Sung Kim, Mahmut Kandemir and Myoungsoo Jung****鄭溟隨 @ KAIST(Korea) SimpleSSD主页：https://docs.simplessd.org/en/v2.0.12/ camelab研究组主页：http://camelab.org/pmwiki.php Amber是SimpleSSD的扩展版本。相比于以往的SSD模拟器，Amber对SSD内部软硬件资源的运行时间和功耗进行了详细的建模，实现了对SSD设备单体更加精确的模拟；通过修改gem5模拟的主机DMA引擎，和对功能型(functional)和计时型(timing)CPU系统总线模型提供支持，实现了真实数据转移的仿真，进而支持主机+SSD的全系统模拟。 论文为什么要做这件事？SSD模拟器可以为研究提供更加高效、低价的选择。然而，准确的SSD模拟框架有以下要求： 虽然SSD往往被认为是整个系统的存储或者内存子系统部分，但是相对于传统存储来说，它本身是一个有自己架构和系统的计算机系统。 SSD不仅包含存储实体后端，也包含复杂的计算部分。例如嵌入式CPU核心、DRAM和内存控制器。SSD内部的计算部分是与主机并行运行的，因而模拟结果很容易不准确。 SSD通过多种存储接口和协议与主机系统进行数据交换，包含SATA，NVMe，OpenChannelSSD (OCSSD)等。在全系统性能模拟时，我们需要考虑这些存储接口和软件栈的差异。 SSD上运行的固件有不同的设计方案和运行机制，因而固件的软件栈对于整个系统的性能有很大的影响。 这个事情之前别人是怎么做的，存在什么问题？论文写作时(2018)，也存在各种SSD模拟器(SimpleSSD v1.0 &#x2F; MQSim &#x2F; FlashSim &#x2F; SSD-ext)，但是它们的模型或多或少的简化了以上的问题场景： 缺乏SSD内部的计算资源的模型，使得这些模拟器无法胜任全系统模拟。特别是无法模拟固件的运行时间。 之前的SSD模拟器并没有建模实际的存储接口模型和实际数据转移的仿真，无法集成进全系统模拟的环境。 没有SSD模拟器能够既支持**功能型(functional)，又支持计时型(timing)**的CPU模型接口。大部分都是只支持功能型的CPU模型，这种假设过于简化了。 该论文解决了什么难点，难点存在的原因是什么，作者是如何发现这些难点的？难点：用FIO工具进行探究过程中发现，在相同设定下，当改变I&#x2F;O队列的深度时，现有模拟器得到的带宽和延迟数值不论是从变化趋势，还是某个设定下的真实效果，都与真实硬件存在非常大的偏差。 难点产生的原因： 缺乏SSD内部计算单元延迟的建模； 采取不完整的固件软件栈； 忽略了存储接口和协议的建模； 针对以上难点，作者各做了哪些优化，优化背后的思想是什么？Amber新增的模拟对象主要包含三个方面：SSD硬件架构(包含计算单元和存储单元)、固件软件栈、主机和SSD之间存储接口和协议(数据转移机制仿真)。 对真实SSD的分析 SSD内部硬件架构：可以分为计算部分(Computation Complex)、存储部分(Storage Complex)。SSD的固件由SSD内部计算核心做计算；DRAM作为SSD和主机数据传输的数据缓冲区，也保存了一些其他重要信息；存储部分由flash芯片组成，有一些特别的读写特性，比如写放大等。(略) SSD内部软件栈(固件)：软件栈主要包括主机-SSD接口层、Flash翻译层、Flash芯片接口层。内部也有比较复杂的小机制，比如缓存管理、垃圾回收、地址翻译、Flash请求队列管理等等。固件运行在嵌入式处理器上，用DRAM保存相关数据。 主机-SSD之间的接口和协议栈： SSD与主机之间的硬件接口可以分为与南桥(ICH)或北桥(MCH[PCIe])连接，二者分别由硬件的主机控制器和软件的设备驱动程序驱动，所以可以将SSD分为**硬件驱动型(h-type)和软件驱动型(s-type)**。硬件驱动型的SSD协议有SATA和UFS，一般都是传统的SSD；软件驱动型SSD协议包括NVMe和OCSSD。相比来说，软件驱动型要更快、吞吐量要更高，在另一方面，其占用的主机资源也越多。由于真实的SSD与主机之间的硬件接口和协议不同，其性能也完全不同，因此一个完整的模拟器应该建模所有的协议和接口情况。 比如，硬件驱动的SSD由主机控制器发送I&#x2F;O请求的，所以它的特点是：在主机和存储之间的数据转移需要在主机控制器的硬件队列和DRAM中进行数据的冗余复制；对SSD发出的I&#x2F;O请求必须全部串行。 另一方面，软件驱动的SSD和主机之间的数据传输是由设备驱动程序完成的。主机和SSD之间的通信一片主机、SSD共同可见的内存地址(BAR)实现，I&#x2F;O请求的接收和发送通过这篇地址中的请求队列实现，此外还通过寄存器门铃机制和MSI中断来控制主机和SSD对请求队列的同步。 在建模时这些接口和协议栈也将影响SSD和主机通信的精确性能，并且可以看到，不仅需要建模SSD端，主机也必须建立相应的模拟组件才能实现全系统的精确模拟。 SSD硬件架构模拟 嵌入式处理器建模(指令级模拟)：基于ARM v8指令集架构，将每个模块的函数做指令解码，并将不同的固件组件分配给一个CPU核心执行。每个函数的调用在每次运行都是动态的，模拟器通过保留每个函数在动态调用过程的轨迹，来建立评估指令执行的时间和功耗的模型，指令包括算术、分支、load-store等，进而实现每个请求和事务的时间详细评估。因为处理器执行和FLASH读写数据可以同时发生，因此要捕捉每个请求每个事务的处理时间。此外，在CPU模型中还集成了多核动态能耗和面积模型。 SSD DRAM建模： 建模了带有DRAM计时参数的DRAM和内存控制器模型，与处理器通过SSD系统接口相连接。内存模型中包含缓存的数据、额外数据和页面映射表，他们可以被固件动态更新。此外还集成了DRAM功耗模型。 FLASH设备建模：FLASH建模了多通道多路的flash存储器，模型包括写入时间、访问时间、数据转移时间等参数，这些参数使得Amber支持TLC，MLC等存储颗粒，并且也建立了动态功耗评估模型。 SSD固件(软件)栈模拟 HIL(host interface level，主机接口层)：基于主机存储接口定义的队列协议来调度I&#x2F;O请求（比如h-type用FIFO，s-type用Round-Robin），I&#x2F;O请求来自于SSD设备管理器建模的SATA&#x2F;PCIe物理界面模块，并执行数据转移；再将请求转换为基于flash页面大小的内部请求。 ICL(internal cache level，内部缓存层)：将HIL得到的内部请求缓存到DRAM模型里，并且将主机到SSD或者从flash到主机的数据缓存到DRAM里。 FTL(flash translation layer，地址翻译层)：地址转换，将逻辑块地址使用DRAM中的映射表翻译为物理页面地址；还有垃圾回收、磨损管理等机制。 FIL(flash interface layer，Flash接口层)：调度flash访问事务，并行对flash做访问。 注意：在Amber模拟器中，以上模块都可以高度重构。Amber本身提供了一些模块的其他选项可供选择，用户也可以自定义一些函数功能等。 SSD和主机之间的接口和协议模拟(仿真数据转移机制) Amber可以处理主机和存储器之间I&#x2F;O请求中的实际数据内容，这对于在全系统环境下运行应用层和操作是非常重要的。因此，Amber建模了DMA控制器，并将其集成进了gem5模拟器，它可以实现主机系统内存与SSD模型的DRAM的数据转移。 实现过程中，主机控制器或者软件驱动内部的所有I&#x2F;O请求形成了一个指针列表，每个项指向一个待搬运的系统页面。DMA控制器会分析这个指针列表，根据不同协议的定义，项的结构是不同的。DMA模块在分析后完成数据转移。 DMA控制器模型实现过程中的一个挑战是：在gem5中，当实现不同的协议时，不同的CPU模拟模型有不同的存储访问计时模型和I&#x2F;O处理过程。例如，计时型的CPU模型(Timing CPU)要计时内存页面访问、页面转移和通过MSI完成I&#x2F;O请求等操作；而功能型的CPU(Functional CPU)只需要将每个请求的数据转移操作集成进一个I&#x2F;O任务，因而需要DMA模块对每个请求的数据量做估计和模拟，用一个简单的请求延迟来实现计时。Amber实现了对不同种类CPU模型的支持。 举例：NVMe的实现 使用了PCIe interface，PCIe多路链接到PCIe终端，终端上有8KB FIFO来接受请求； 实现了gem5的系统总线和DMA控制器，用设备控制器来支持NVMe命令。 实现了在内存中的NVMe队列(包括完成队列和提交队列)，并通过实现一系列门铃寄存器和MSI机制来同步驱动或者设备控制器上的队列状态。 实验结果 准确性 带宽准确率：72%~96% 延迟准确率：64%~94% 带宽与主机CPU性能的关系 使用2 GHz CPU,由于内核执行（用户级）和协议管理（接口级），其带宽相比于设备层降低了41%。 当以更高的频率（8GHz）运行相同的内核时，用户级性能提高了12%。 可以研究内核栈和协议栈给SSD的带宽带来了多少影响。 模拟执行的时间 单独执行：基本上是最慢的模拟器了，比MQSim强一些； 全系统：在gem5的基础上再加三分之一。 自己的思考 Amber支持对SSD全面和详细的建模，效果也很贴近真实硬件；但是由于其复杂性和与gem5进行了全系统集成，真实数据的转移仿真过程其消耗的模拟时间可能会很长，加上采用SSD做应用的模拟往往是大数据应用，比较怀疑其实用性。 附：本论文没有对比FEMU。FEMU、Amber和MQSim同年(2018)发表，通过本文可以知道Amber在准确率和模拟时间上比MQSim可能都要更好一些。而Amber没有对比FEMU，FEMU是一个基于QEMU的SSD模拟器，可能模拟运行的时间会更好一点。 目前就二者的引用量来看，FEMU(65)要比Amber(33)更多。","link":"","tags":[{"name":"论文阅读","slug":"论文阅读","permalink":"https://recoderchris.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"name":"近存储计算","slug":"近存储计算","permalink":"https://recoderchris.github.io/tags/%E8%BF%91%E5%AD%98%E5%82%A8%E8%AE%A1%E7%AE%97/"},{"name":"KAIST","slug":"KAIST","permalink":"https://recoderchris.github.io/tags/KAIST/"},{"name":"MICRO","slug":"MICRO","permalink":"https://recoderchris.github.io/tags/MICRO/"},{"name":"2018","slug":"2018","permalink":"https://recoderchris.github.io/tags/2018/"},{"name":"模拟器","slug":"模拟器","permalink":"https://recoderchris.github.io/tags/%E6%A8%A1%E6%8B%9F%E5%99%A8/"}]},{"title":"(ASPLOS18)面向不规则访存问题的事件驱动可编程预取器","date":"2022-08-12T14:18:58.000Z","path":"2022/08/12/event-prefetch/","text":"A Event-Triggered Programmable Prefetcher for Irregular Workloads 作者：Sam Ainsworth and Timothy M.Jones @ University of Cambridge(UK) 大数据应用中大量的内存不规则访问产生了memory-bound问题。为了减少一般性的复杂访问特征应用的的cache miss率，本论文提出了一种以事件触发的可编程预取器，其可以将通用处理单元的灵活性将基于事件的编程模型相结合来支持更加复杂的预取特征。此外，本工作还借助编译器，自动地从源代码中解析出事件。 论文为什么要做这件事？大数据分析中的性能瓶颈现代大规模数据分析中存在大量不规则、难预测的数据访问，这大大增加了传统存储结构中的缓存未命中次数，造成了CPU暂停的时间增加，造成了严重的memory-bound问题。 解决方法1：多线程线程级并行可以将不同线程间的访存和计算重叠，以掩盖CPU的暂停时间。这种方法的问题是：不规则的多线程数据访问难免出现竞争关系；数据分块方法又很难确定，这些都是线程级并行的硬伤。 解决方法2：软硬件预取（规则-&gt;不规则-&gt;专用-&gt;可编程）软硬件预取器也是提升cache命中率的有效方式。然而，现有的预取器对于复杂内存访问的内存级并行探索并不深入。大体上，现有的预取器可以分为以下几类。前三种都采用硬件手段实现。 传统预取器：这类预取器基本上基于传统应用中的时间局部性或者空间局部性进行内存预取。常见的有：stride prefetcher(空间局部性)&#x2F; history-based prefetcher(时间局部性) 不规则访问预取器：这类预取器对一些常见的不规则访问进行预取。例如pointer-chasing预取器、indirect array预取器等。 专用预取器：针对某一类应用，提取特殊的数据访问特征来定制硬件预取器。这类预取器往往在目标应用上能够取得很好的效果，但是往往需要算法是成熟的，不需要进一步改动了。 软件预取器：软件预取器在其他CPU线程上进行访存，为当前线程提供数据。这种方法可以让用户显式地自定义预取特征。 综上所述，现有的硬件预取器由于缺乏灵活性，对于现代数据应用中复杂的数据访问特征往往不见效，有时候还有相反的效果。软件预取器虽然支持用户对待预取的数据进行显式的自定义，但是因为毕竟在一个核心上，也是有数据等待的开销的。同时，软件预取器无法对提取到的数据进行反应，例如软件预取器就无法对指针追踪类的应用产生效果。 本文的基本想法用通用的计算单元作为可编程的预取主体，用户或编译器来定义预取函数，在特定事件触发时进行对应的预取，从而实现对于一般性的复杂数据访问特征有效的预取器。 这个事情之前别人是怎么做的，存在什么问题？预取器的研究前沿方向主要有以下几类： 明确指定特征的专用预取单元 方法：通过分析应用中的特定访问特征，使用预取器进行并行数据加载。 优点：专用硬件能够对特定应用实现很强的性能提升 缺点：缺乏一般性 不明确指定特征的不规则访问预取器 方法：对不规则的数据访问流进行分析以提取广泛的公共特征（pointer-chasing、indirect memory access等），对这些特征做加速 优点：不需要明确指出应用相关的特征，不需要额外配置寄存器，没有手工开销 缺点：在复杂的数据访问中，有时候会产生过度预取现象。 用helper线程实现预取（软件方法） 方法：应用额外的CPU线程进行预取数据。 优点：可用户自定义、编程难度大大降低。 缺点：在高性能核上添加额外线程，会提升系统的能耗。并且有可能会有同步开销。此外，软件预取也要在当前线程内暂停等待结果，这有可能使得计算线程赶上预取线程。因此性能相对于硬件预取器要差一些。 可配置预取器 方法：对某数据结构进行可编程预取器设计 优点：可自定义、性能好。 总结：虽然现有的预取器都能或多或少的对某些特定应用、某个访存特征做加速，但是仍然不是完整的解决方案。 该论文解决了什么难点，难点存在的原因是什么，作者是如何发现这些难点的？下面以数据库操作中的hash-join为例，来指出大规模数据操作的难点和各种预取器存在的问题。 上面是一个在数据库操作中常用的hash-join函数内核。我们先忽略红色的优化策略，整个算法的流程是这样的：对每个顺序访问的键值key求哈希值，根据此哈希值做数组访问，找到htab。这里涉及了一个数组的非线性随机访问，此处容易产生cache miss。下一步对于htab上链的一整个链表做访问和计算，这里也是一个指针追踪，容易产生多次的cache miss。 下面我们将逐步分析用预取器优化这个内存访问会有什么样的难点，图中浅绿色代表indirect access找到htab的过程，深绿色代表pointer-chasing的过程。斜线代表由cache miss产生的CPU stall。 （注意：这个图画错了，d和e应该反过来的。） 原始情况下会有大量的CPU stall 软件预取器：软件预取器的缺点是无法对指针追踪过程做预取（因为指针追踪需要依赖之前预取的数据，软件方法无法获取这个信息），因此只能加速非直接内存访问的部分。可以看到获得的收益比较小，并且由于预取也是当前线程去发射的指令，也有开销，因此效果提升不是非常明显。 多线程预取：这里用多线程预取的方法使得线程之间共享内存访问，使得不同线程的计算和访存可以并行。问题在于在pointer-chasing中难免会有数据的冲突，难免引入同步的开销。 helper线程预取：搞一个专门的线程只为计算线程做预取。这个线程也是在当前计算核心上被执行，但是只进行访存不进行计算。这个问题在于helper线程也是单线程的思路，必须等待上一个访存结束再进行下一个访存的，必须等待预取结果再进一步预取。由于主线程不再被cache miss所限制，因而其会逐渐被访存线程赶上。 理想情况：保证helper线程的自定义特征之外，我们想要充分挖掘内存中的并行机会。额外的并行机会在于，可以看到其实不同的链之间是完全可以并行的，他们之间不存在数据依赖。helper线程的问题在于也只能是等待上一个链完全做完再做下一个链的，我们不希望计算的CPU有暂停以造成CPU资源的浪费，还是希望有一个单独的硬件来做预取，保证CPU的高效运转。 针对以上难点，作者各做了哪些优化，优化背后的思想是什么？系统概况文章提出了一个可编程的预取器，这个预取器被特定地址的数据结构的读事件触发，并且自定义预取操作，在额外的低功耗通用计算核心上进行预取。 主要运行流程：主核心中的读取请求和预取单元中的预取请求中的地址，在通过L1dcache的时候先进入预取器的地址过滤器。地址过滤器会对感兴趣的地址进行过滤，这个感兴趣的地址一般是在程序开始之前由编译器的一条单独指令初始化，往往是目标数据结构的地址空间。之后，将感兴趣的地址放入观察队列中。观察队列满了的时候就将最老的观察抛弃，因为预取本身不影响程序的正确性。之后调度器负责将感兴趣地址送给可编程的预取单元，进行相应的预取操作。当有空闲的可编程预取单元的时候，调度器才会将这个请求映射到可编程预取单元上。可编程预取单元是这个预取器的主体，但是他并不需要等待访存的结果，而是只发送访存的请求。可编程预取单元在L1cache中有空闲的MSHR时将请求发送给预取请求队列，之后预取请求队列再通过TLB地址转换和数据预取。注意到，可编程预取单元还连接了全局寄存器和EWMA计算单元，全局寄存器是负责存放一些重要数组的首地址，EWMA是负责为预取单元提供预取步长的。预取单元使用单独的指令缓存。 地址过滤器 初始化：通过编译器分析产生的感兴趣地址使用指令进行配置，为过滤器明确几段指定感兴趣的逻辑地址，以及每段感兴趣的逻辑地址对应的函数指针。 过滤后的地址被放入观察队列，如果它位于两段感兴趣的地址空间，则生成两个项。 观察队列和调度器 当有空闲PPU时，调度器将感兴趣数据的虚拟地址或者内容（cacheline）导入ppu的本地寄存器。将ppu 的地址设置为对应的函数入口位置，这个函数是用来计算预取地址的。 如果观察队列满了，则将最老的请求抛弃。 可编程预取单元（PPU） PPU负责产生新的预取请求。其上运行的函数主要通过当前数据的内容、当前数据的地址来计算预取的数据地址。 PPU被设置为不能访问内存，他上面只有一个全局寄存器，在PPU之间共享，负责记录几个关键数据结构的位置；一个本地寄存器，负责做简单计算，还有从l1dcache中获得的数据内容。所以PPU上运行的函数往往只涉及简单的算术运算操作。 PPU只有icache没有dcache。PPU代码跟主核心的代码是分离的，但是任何一个观察值的操作都可以在任何一个PPU上跑。 硬件要求：只需要简单的ARM Cortex M0+即可。只包括不超过12000个门电路。面积大小不到L1数据cache。 步长监控（EWMA）步长监控模块负责预取即用的数据，为PPU模块提供步长信息。 内存请求标签主要针对指针型访问。数组型访问可以通过感兴趣的内存地址来找到，而指针型的感兴趣标记可以通过MSHR中的特定标记来实现。 编程模型目前主要讨论手写预取函数，之后会要讨论编译器生成手写函数。 因为PPU之间是完全独立的，每个都可以相应预取请求，所以预取器不需要stall来等待结果。 PPU编程模型是基于事件的，有一些限制。比如PPU的编程中不能有数据访问，栈或者存储。不能有函数调用，不能有系统调用。只是简单的算数运算。","link":"","tags":[{"name":"论文阅读","slug":"论文阅读","permalink":"https://recoderchris.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"name":"2018","slug":"2018","permalink":"https://recoderchris.github.io/tags/2018/"},{"name":"ASPLOS","slug":"ASPLOS","permalink":"https://recoderchris.github.io/tags/ASPLOS/"},{"name":"Cambridge","slug":"Cambridge","permalink":"https://recoderchris.github.io/tags/Cambridge/"},{"name":"预取器","slug":"预取器","permalink":"https://recoderchris.github.io/tags/%E9%A2%84%E5%8F%96%E5%99%A8/"},{"name":"数据访问优化","slug":"数据访问优化","permalink":"https://recoderchris.github.io/tags/%E6%95%B0%E6%8D%AE%E8%AE%BF%E9%97%AE%E4%BC%98%E5%8C%96/"}]},{"title":"eBPF+CSD调研","date":"2022-07-11T14:15:15.000Z","path":"2022/07/11/eBPF/","text":"很多人认为融合eBPF技术会是未来计算型存储器**(Computational Storage,CSD)的一个重要发展方向。本文调研了eBPF技术的原理和应用场景**，并指出“Why eBPF+CSD”。此外，本文还调研了当下在CSD中使用eBPF技术的难点和可能的替代选项。 什么是eBPF技术？前身：BPF(Berkeley Package Filter)BPF背景：于1992年提出，当时unix操作系统提供了捕获数据包的设施以监控当前的网络情况。但是，网络监控程序(如tcpdump)处于用户态，数据包必须频繁地被从内核态被拷贝到用户态。因此，一些被称为【数据包过滤器】的内核代理被提出了，以尽早在内核就丢弃不想要的数据包，避免从操作系统内核向用户态复制其他对用户态程序无用的数据包，从而极大提高性能。BPF是其中一种。 图 经典的BPF BPF实现：用户态的进程可以提供一个过滤程序，BPF功能用对于BPF虚拟机机器语言的解释器来实现。 extended BPF: 从3.18版本开始，linux内核中提供了一种扩展的BPF虚拟机(eBPF)，可以被用于非网络相关的内核其他用途，比如附着在不同的tracepoint上获取内核运行的信息，eBPF也被windows操作系统所采用。 eBPF技术简介现代eBPF：eBPF现在已经不再是任何东西的缩写，而是一项单独的技术。它可以在不修改内核源代码或者加载内核模块的情况下，在特权上下文(如操作系统内核中)运行沙盒程序，被用来安全高效地拓展内核程序。可编程的部分直接添加到现有层中。 优势：由于内核具有监督、控制整个系统的特权，操作系统一直是实现可观察性、安全性和网络功能的理想场所。但是内核由于其核心作用和对稳定性和安全性的高要求，修改内核很难。eBPF则改变了这一点，并且由操作系统来保证其安全性和执行效率。 可编程性：因为eBPF虚拟机使用的是类似于汇编语言的指令，可编程性差。现在的编译器正在兼容高级语言生成BPF字节码。例如，LLVM在3.7版本开始支持BPF字节码作为后端输出。 eBPF的实现： 图 eBPF开发流程 eBPF技术现有的应用领域 网络（协议解析、转发逻辑） 跟踪和分析（可以同时跟踪内核和用户程序的执行，提供强大的解决内核问题的能力） 安全性（更好的保护系统，允许结合所有方面的可靠性） 传统存储（实现数据过滤） …… 图 eBPF面向的应用场景和技术栈 为什么说eBPF+CSD是一个重要的发展方向？当前CSD发展所目前面临的难点目前，CSD的可编程性仍然很差： CSD的设计没有统一的标准。由于不了解设备内部的复杂程度，且很难去对内部编程，探索硬件模型和在主机上统一集成接口非常困难； CSD没有标准的编程抽象、API或者编程模型。之前的工作往往都是选定编程模型，而不是让开发者自己来自定义编程以适应当前工作负载。 可编程性意味着可以动态、安全地运行用户提供的代码，能够方便的将用户需要的任务卸载到存储器上。而eBPF可以将用户定义的任务“注入”内核，刚好符合这样的要求；又因为eBPF有现有的工具链和库，CSD研究者们希望用eBPF来为CSD提供任务卸载。 为什么选择eBPF？ 不影响内核：CSD需要在主机上开发程序并让存储控制器执行它们，而不影响控制器上原有固件的执行；而使用eBPF技术可以将eBPF程序挂载到内核中，而无需修改内核。 架构无关：eBPF与架构无关。在计算型存储器中我们希望使用与硬件无关的指令集，而不是预先知道CSD上处理器的型号再来下载对应的代码(X86或ARM)。 想象一个可能的应用场景：假设我们要对压缩加密后的大型数据库表进行数据分析。首先，借助CSD的可编程部分，我们可以在离线阶段在CSD中定义一个通用的控制程序，例如将数据读取到CSD的内存缓冲区，并解压解密数据放入CSD上的一个本地位置；在线阶段用户可以通过eBPF自定义计算程序，对数据进行过滤，最后再将过滤后的数据传输到主机。这样可以通过自定义的方式，能够更好地贴合数据特征，进一步节省内存和带宽，比之前的方式灵活的多。 CSD与内核的相同点和不同点 相同点：eBPF指令集和工具链已经相对来说比较成熟。我们像kernel中一样地把 C 程序编译成eBPF 目标文件，就可以在CSD中使用 eBPF 生态中已有的东西。 不同点：eBPF发源于内核，而内核对 eBPF 程序允许执行的操作有非常多的限制。例如，eBPF 指令集是允许用户写无限循环的，但Linux内核中的虚拟机在检查时会拒绝该程序。这是由之前的应用场景导致的：因为内核在处理数据包的时候没有那么复杂的操作。但是在CSD的场景下，我们可能希望运行 eBPF 做计算，这就需要对一些复杂数据结构进行处理，要放宽原有的限制。 CSD+eBPF产生的研究性问题 如何让eBPF执行计算任务：eBPF目前的应用主要是一些计算复杂度低的控制类程序，且必须要通过现有的应用于Kernel场景的eBPF虚拟机进行验证。但计算程序更复杂，运行时间更长。因此，如何将eBPF应用到计算任务上，以使得程序运行效率接近基于目标硬件指令集的程序是目前eBPF开发人员关注的一个重要问题。 可能的方法：修改eBPF虚拟机定义的正确运行时限制、定义新的计算指令（跟CSD无关） 任务卸载：对于一般的大型计算任务，哪些工作负载卸载在CSD上会更有意义？我们如何为这些任务定义一个通用的 API 或框架？ 通信问题：在存储设备中的CPU往往算力较小，因此其需要辅有其他专用硬件来完成计算。eBPF程序与存储上其他设备的通信方式也是难点之一。 一个可能的eBPF+CSD prototype来自阿姆斯特丹自由大学的团队探索了**基于QEMU仿真ZNS，来实现eBPF+**CSD的一个prototype。(仍然在开发中，文档健全；官网给出运行的视频样例)(https://github.com/Dantali0n/OpenCSD)。我们可以通过这个项目去展望一下eBPF+CSD的设计： 图 技术报告封面 配置 QEMU：仿真Zone NameSpace SSD（ZNS）。 SPDK：一种与用户空间的存储设备接口（在本项目中就是指能够与QEMU仿真的ZNS）的技术。 uBPF(user-space BPF)：一种ebpf虚拟机。 图 ZCSD的工作流程 主机端 编写程序BPF程序：用户编写的BPF程序中应当包含所有需要在CSD上运行的API； 编译：Clang+LLVM编译生成bpf字节码； 格式转换：Bpftool转换将bpf字节码转换为要用户程序包含的头文件，其中包含有elf格式的字节码； 包含头文件：头文件包含在用户写的程序中，用户有了调用CSD上eBPF程序接口的能力 之后编译好的BPF程序可以通过用户写的程序连同NVMe API来使用了。 存储端 程序提交：用户程序以elf格式提交用户程序 虚拟机****执行：uBPF被用来执行BPF程序，提供BPF API的实现 取数据：BPF程序通过SPDK在底层的ZNS中取数据，ZNS返回的数据由驱动程序和SPDK处理。 自定义计算：BPF程序进行计算或过滤，将数据返回给用户程序。 用户接收数据：用户程序接收数据。 NVMe扩展了原有的NVMe指令集： 下载eBPF程序：以elf格式发送BPF程序； 返回处理数据：将eBPF程序的返回数据提取出来。 eBPF不是唯一的选择现有的一些讨论将eBPF视为对CSD进行卸载的主要方案，这一观点并没有仔细考虑其他的选项。https://arxiv.org/abs/2111.01947这篇论文从**定性**和**定量**的角度来综合比较了**eBPF**和**WebAssembly(另一种易于移植的、低层次的字节码，用于基于堆栈的****虚拟机****)**的区别，给出了他们对于这个问题的见解。 CSD的其他选择 eBPF：现在指允许在Linux内核总运行沙盒程序的技术的总称。但是我们在这里尤其对基于寄存器的eBPF字节码感兴趣。eBPF现在应用在网络包过滤以及系统监视等多种应用场景中。以下展示了用eBPF写I&#x2F;O tracer的工作过程：用户用C语言写一个I&#x2F;Otracer，然后用clang编译成eBPF字节码，这个字节码将会被libbpf加载进内核执行。虽然目前来看eBPF字节码常常被用于linux kernel的场景下，但是也有很多工作去尝试独立运行eBPF的字节码。 WebAssembly：WebAssembly是一易于移植的、低层次的字节码，用于基于堆栈的虚拟机。它是为在浏览器中安全高效的运行而设计的。已经有工具从python或C语言来生成WebAssembly的代码，为了在浏览器环境之外执行这些字节码，可以使用wasmtime&#x2F;wasmer&#x2F;GraalVM等带有JIT的虚拟机。 计算型存储器背后的思想是代码迁移，将处理直接卸载到存储设备中。过去的方案试图为特定类型的工作负载提供专用软硬件，而缺乏对通用计算卸载的讨论，例如用户可编程的、动态可卸载的计算。 分析目标通过定性和定量评估eBPF和WebAssembly两种卸载机制来重启关于通用计算任务卸载机制的讨论。具体来说是回答以下问题： 这两个卸载机制当前处于一个什么样的状态？ 二者的优缺点分别是什么？ 在计算型存储器的背景下怎么去改进这些两种机制？ 定性分析 安全性这里的安全性是指抵御人为错误的机制：如果卸载的bytecode里有bug，能发生的最坏结果是什么？ WebAssembly：卸载的代码会在一个隔离的内存空间中运行，并且默认是不允许访问文件系统。因此bug可能最差会引起CPU时钟周期的浪费，但是并不会在runtime之外造成什么影响。 eBPF：主要取决于验证器。Linux内核验证非常严谨，禁止很多类的循环和数据访问；而使用这样的内核进行数据处理显然不适用。新的验证器仍然待开发，因此还不能说eBPF的安全性如何。 工具链 开发工具：对于WebAssembly，有一系列叫做wabt(WebAssembly Binary ToolKit)的工具可以来检查和转换WebAssembly的二进制文件。对于eBPF，bpftool能够用来检查eBPF程序，但是本质上它与内核的使用绑定，所以在独立的场景下没法使用。Llvm-objdump对于ebpf来说是更好的选择。 虚拟机：如之前所提到的，WebAssembly有至少三种适合不同场景的虚拟机实现。虽然他们都有一些同样的特点例如JIT支持，但是他们也有很多独立的特征；但是eBPF却仍然还有很长的路要走。uBPF和rBPF都有JIT支持(仅x86_64)，但是他们仍然缺乏分析和调试等方面的功能。 兼容性这里的兼容性指重用现有生态中库的能力，任何外部依赖项(包括C库libc)都要被重新编译并静态链接。 WebAssembly：使用wasi-sdk进行编译，但是也有一些函数例如fork&#x2F;pthread无法build。 eBPF：没有wasi-sdk这样的工具，由于无法调用C库，所以大量现存的库也无法使用了；除此之外，由于clang -target bpf不支持一些种类的数值操作和浮点操作，一些数据类型也可能不支持，这主要是因为eBPF缺乏合适的指令让clang来生成。因此，我们并不知道这些操作将如何被eBPF来使用除非eBPF有重大的扩展。 可移植性由于二者都是字节码，所以都比较好移植 eBPF：并不是大小端独立的。ebpf的大小端依赖会影响他的移植 WebAssembly：总是用小端模式，如果host架构是大端的，runtime可以来负责转换 用户体验 编译：WebAssembly可以由多种语言产生，可以用两种方式(Emscripten和wasi-sdk)来编译进行C库支持。除此之外，支持链接时间优化、关闭系统C库、无main入口支持等选项，最后还可以对生成的code码来缩减大小；ebpf可以用clang的bpf选项来编译，但是实际上是对内核内部的应用进行编译的。 字节码检查：wabt工具链中有一个wasm2wat的工具，能够将二进制webassembly文件转换为可读的文本文件。也可以使用wasm-objdump进行反编译。ebpf虽然还没有正式的文本格式，如果ebpf二进制文件是由debugging信息编译成的，可以用交叉源代码查看反汇编的二进制文件。 语言不可知性 webassembly：可以用大量的源语言。但是实际上，源语言必须支持导出malloc()以分配buffer，并且在主机和虚拟机之间传递程序。 ebpf：目前只支持C语言为源语言。 定量分析内存占用：rbpf和ubpf接近原生表现 运行时间：实际计算所花费的时间，rBPF和uBPF表现非常好，以至于能够击败host上的binary。 总时间：rBPF和uBPF在x64架构上经常击败基于host的方式 讨论虽然eBPF虚拟机在定量分析中显示占有更少的内存、计算性能更好，但是作者从定性分析的角度，认为eBPF存在更多的缺陷，而webAssembly可能是更成熟、更容易上手的选择。 图 定量分析实验其中之一 图 对eBPF和WebAssembly定性分析的总结 下面是一些潜在的可能提升机会： webassembly：作为一个相当成熟的技术，仍然有以下改进方向 实现SIMD和多线程 提供与host数据共享的接口 解决安全问题 eBPF：根据前面的评估，eBPF在成为适合的数据处理工具之前还需要做很多工作 成熟的虚拟机，能够支持debug和性能分析 扩展指令集，以支持多种数字运算 明确定义允许操作的规范 与规范同步开发的可靠验证器 C库和其他系统服务的支持","link":"","tags":[{"name":"论文阅读","slug":"论文阅读","permalink":"https://recoderchris.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"name":"近存储计算","slug":"近存储计算","permalink":"https://recoderchris.github.io/tags/%E8%BF%91%E5%AD%98%E5%82%A8%E8%AE%A1%E7%AE%97/"},{"name":"eBPF","slug":"eBPF","permalink":"https://recoderchris.github.io/tags/eBPF/"},{"name":"操作系统","slug":"操作系统","permalink":"https://recoderchris.github.io/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"前沿系统","slug":"前沿系统","permalink":"https://recoderchris.github.io/tags/%E5%89%8D%E6%B2%BF%E7%B3%BB%E7%BB%9F/"}]},{"title":"(JSA)PASM:针对hybrid SSD中DNN训练加速的并行度优化存储空间管理策略","date":"2022-06-15T14:05:38.000Z","path":"2022/06/15/pasm/","text":"[JSA] PASM：针对hybrid SSD中DNN训练加速的并行度优化存储空间管理策略 PASM: Parallelism Aware Space Management strategy for hybrid SSD towards in-storage DNN training acceleration 作者：肖春华 等人 @ Chongqing University(China) 近存储端的DNN加速器提升了系统整体的性能，但大量的写操作导致了密集的写入放大现象，这对Flash芯片的寿命和性能产生了极大的挑战；目前广泛使用TLC（甚至QLC）来提升flash芯片存储密度的趋势进一步加剧了这个问题。 本文基于SLC-TLC混合粒度型SSD，提出了一种垃圾回收与写入并行的存储空间管理策略来解决这一问题。 论文为什么要做这件事？在近存储端采用DNN加速器的方案在性能和功耗上都表现不俗，但目前仍然存在以下两个由Flash芯片本身的性质引起的问题： NAND flash与DRAM相比寿命短，且各种复杂的物理特征（如页面只能够被写入一次、擦除和写入的粒度不同等等）影响了整体性能。传统FTL中的flash数据管理策略对于具有大量连续写入的近存储DNN训练并不适用。 例：DNN 训练需要至少数千次完整的flash芯片写入，大量垃圾回收不可避免的。 传统的并行化数据布局会在垃圾回收过程中造成数据迁移，导致写入放大。 而传统的垃圾回收是基于阈值触发的策略，只有在可用空间逐渐耗尽且低于预设阈值时才会执行，在阈值达到时，持续写入带宽将因为数据迁移和擦除操作显着降低，导致系统的性能急剧下降。 高密度的flash芯片（例如TLC&#x2F;QLC颗粒芯片）逐渐在市场占据主流，相比于SLC芯片其性能和寿命更差，加剧了上述问题。 表1 不同的存储单元的在读、写、擦除和寿命上的表现这个事情之前别人是怎么做的，存在什么问题？ DNN+ISP：Behemoth利用flash芯片的高带宽和大容量进行语言模型的训练；HyperTune在计算型存储设备上完成了分布式训练等等。然而，DNN+ISP的工作往往局限于使用近存储计算来优化DNN中存在的memory-bound问题，却并没有充分考虑flash芯片的物理特征给DNN训练效率以及flash寿命带来的影响。 混合颗粒型SSD(hybrid SSD)是解决高密度flash性能问题的一个有效途径。这种存储利用了flash的dual-mode特性，将flash上的一部分TLC颗粒转换为SLC颗粒，把SLC部分当做flash上的缓存，以利用SLC的高性能和长寿性；运算过程中逐渐地将SLC的数据迁移到TLC部分以保证SLC的容量。 图1 混合颗粒型SSD的架构(来源：[19’ICCD] SPA-SSD) 然而，hybrid SSD应用在有大量连续写入的DNN加速方案中仍然有很多问题： SLC存储块作为缓存，将数据存入到TLC中这引入了新的数据迁移，会阻碍DNN的连续I&#x2F;O请求； 一旦SLC块用完，并且进行垃圾回收的时候闪存的性能会迅速降低； 当SLC块将要用完的时候，为了接受新的写入请求，一些TLC块将不得不转换为SLC块来写入数据，这并没有充分利用SLC的优势。 综上，用hybrid SSD来解决这一问题效果并不好。 该论文解决了什么难点，难点存在的原因是什么，作者是如何发现这些难点的？下面的探究验证了hybrid SSD在性能和寿命上的存在的问题。 难点1. DNN进行持续写的场景下，混合型SSD的性能损耗 图2 混合型SSD和TLC-SSD在多iteration后的表现 上图反映了用10%SLC-90%TLC和用TLC作为存储系统时的IOPS。当迭代次数增加时，混合颗粒型SSD的的性能迅速下降，甚至降低到TLC存储的效果差不多。原因有以下两点： 当SLC块逐渐被消耗的时候，系统只能用TLC来完成I&#x2F;O请求 Iteration越大，垃圾回收占用的时间越长，对性能造成了很大的影响。 SLC的效果没有很好的发挥出来。 难点2. DNN持续写的场景下，对flash寿命的挑战 本文在这里粗略计算了一下，在resnet50网络上训练imagenet-2012数据集，只需要13个epochs就可以将2TB的TLC NAND flash用完一遍。 传统的垃圾回收机制会在容量到达某一个阈值之后进行。这导致flash始终保持很高的设备占用率，导致在持续写的场景下设备耐久度会下降。这并没有充分发挥混合粒度型存储的潜力。 为了解决以上的问题，作者认为，需要在 DNN 训练过程中并行执行垃圾收集和数据访问，以确保始终有空闲的 SLC 块用于写入请求。 针对以上难点，作者各做了哪些优化，优化背后的思想是什么？融合PASM的近存储DNN计算 图3 PASM计算架构 DNN加速器：DNN加速器发送数据写入请求时，需要将该数据的寿命包含请求中。一般而言，数据的寿命很难获得，但是在DNN这种数据访问模式非常规律的应用中，数据的寿命可以简单的分析出来。 DNN加速器发出数据写入请求后，数据布局模块(CFP-DL)根据数据生命周期指导数据分配，数据生存周期也记录在映射表中，随着训练过程的进行动态更新。 在训练过程中，垃圾回收模块(LAD-GC)会在后台持续监控闪存阵列中数据的有效性。如果满足预设的擦除条件并且后续 I&#x2F;O 没有中断，数据回收模块将使用擦除命令将要擦除的块的物理地址发送到FCC。 当达到预设的迭代次数或模型收敛时，训练结束。 表2 DNN训练中各种数据的寿命 CFP-DL：无冲突的、I&#x2F;O请求与擦除操作可并行的数据布局 图4 数据布局 防止数据迁移 问题：当没有足够的SLC块来写入的时候，系统会进行垃圾回收来重新整合数据，最大的问题是写入放大（擦除并重新写入）的过程。重新写入这个过程将占用chip的外部通道，与新写入其他die的I&#x2F;O请求冲突。 根源：数据的寿命不同。传统的FTL将不同寿命的数据放在一起，当寿命短的数据结束使用变为无效的时候，会在寿命长的数据中间留下大量的碎片，这是导致数据迁移的根源。本文的方案试图按照数据的寿命长短归类数据，在垃圾回收的时候只需要做擦除而不需要做写入，防止数据迁移，缓解写入放大的现象。 方法：将不同寿命的数据合并放在不同的物理block区域，分为短期数据块、中期数据块和长期数据块。其中短期和中期数据块放在SLC区域，长期数据块放在TLC区域。根据分析文章认为，DNN训练中每层的输出属于短期数据块；产生的梯度和每轮迭代得到的模型参数为中期数据块；检查点和数据集为长期数据块。 I&#x2F;O与擦除操作并行 问题：数据搬运减少，但是对于大量写入的DNN场景下，flash的擦除操作仍然不可避免，仍然影响性能。文章试图进一步对数据布局和垃圾回收机制进行优化，使得垃圾回收与数据访问能够并行执行。 根源：擦除操作占用同一个die上的数据通道，I&#x2F;O与擦除无法并行。 方法：本文做了一个权衡，即在同一个chip中，只有一半的die会被物理地址索引选中用来写入，另外的那些die则执行擦除操作。在DNN训练过程中，二者的状态会在某个情况到达时进行交换。这使得这两部分的die能够互不影响，一部分做擦除、一部分做数据访问，虽然一定程度上减半了die级别的并行，但是这保证了SLC能够持续提供很好的效率。 此外，作者还通过模型参数来估算每种block的数量以及关于磨损度平衡的优化，细节不再展开。 效果：将不同生命周期的数据分布到不同种类的物理块中，尽可能避免数据迁移； 通过利用闪存中固有的并行性来实现并行读&#x2F;编程和擦除操作，从而消除了连续 I&#x2F;O 请求和垃圾回收之间的冲突，能够在训练期间提供稳定的 I&#x2F;O 性能。 LAD-GC：数据寿命感知的垃圾回收机制 图5 垃圾回收机制 垃圾回收机制的执行具有严格的时序和顺序，可以防止擦除操作引起的 I&#x2F;O 阻塞，擦除的时机由数据的生命周期和数据量决定。 第一步：初始权重和数据集被分别保存在中期块和长期块中。上图假定每个chip中只有两个die。我们先选择die0为工作状态来存储输出，die1为空闲状态。 第二步：当DNN训练开始的时候，我们将每层的output和梯度分别写入短期块和中期块。每一个batch反向传播结束之后，我们将输出占据的page状态置为无效。 第三步：在训练的一次iteration中，我们反复进行以上过程，直到垃圾回收机制发现在工作die的短期块中有一个完全无效的数据块的时候，如果有则直接执行垃圾回收机制，并交换die0和die1的工作状态。 第四步：第二次迭代开始时，上一次迭代产生的梯度数据已被用于更新权重，变得无效。在 Die 1 有无效的短期块之前，垃圾回收机制会检测 Die 0 中是否存在无效的中期块，如果有，则擦除 Die 0 中的无效中期块。 需要注意的是第三阶段的交换时机很重要。如果原die中还有一些需要读取的数据，当这个die被设置为GC状态时，读取请求的服务会被阻塞。因此，对于每个 die，垃圾回收机制只在 BP 结束并完成标记数据的读取请求后寻找一个完全无效的短期块。通过这种方式，可以确保不再有读取请求被阻塞。 效果：垃圾回收机制通过感知数据生命周期，在不干扰连续 I&#x2F;O 请求的情况下执行擦除操作，能够及时有效地回收无效块，以保证 SLC 的高耐久性可以得到连续不断的利用。 实验结果仿真平台Flash Controller仿真：HybridSim DNN加速器仿真：ScaleSim 将ScaleSim的输出接到HybridSim的输入上去。 参数设置： 数据集：Imagenet-2012 模型：resnet-50 验证CFP-DL和LAD-GC的有效性Baseline: SPA-SSD是一种hybrid SSD实现方案，利用SLC部分作为缓存。 SPA-SSD-h是SPA-SSD只用一半的die，但是全部都采用SLC作为存储颗粒的、具有无限容量的理想方案，即其不需要做垃圾回收。作者用这个方案来说明PASM与理想方案之间的差距。 并行数据布局的效果 iteration增加时，相比于SPA-SSD来说，PASM的写入平均响应时间更为稳定，读出的平均响应时间好像略有增加。这主要是因为并行度降低了。 PASM与SPA-SSD-h相近，说明数据布局可以很大程度地减少垃圾回收引起的冲突，稳定地提供I&#x2F;O性能。 垃圾回收机制的效果 iteration非常高的时候，SPA-SSD的擦除操作陡然上升。 整体来看，PASM中大部分擦除都是与访问并行的，充分利用了并行性。 效果 IOPS SPA-SSD的性能在iteration到达100以上时，与传统的SSD没什么差别了。但是PASM则一直表现稳定。 SLC擦除次数 当iteration很高的时候，SPA-SSD和TLC-only SSD的擦除次数是PASM的6.6倍，这说明PASM可以延长寿命6倍左右。 自己的思考本文的主要两个创新点对于具有大量数据写入的应用有较好的借鉴意义。 但是文中一些设计考虑属于比较启发式的，感觉一些机制没有特别扎实的论证，比如垃圾回收机制；实验探究比较少。","link":"","tags":[{"name":"论文阅读","slug":"论文阅读","permalink":"https://recoderchris.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"name":"加速器","slug":"加速器","permalink":"https://recoderchris.github.io/tags/%E5%8A%A0%E9%80%9F%E5%99%A8/"},{"name":"DNN","slug":"DNN","permalink":"https://recoderchris.github.io/tags/DNN/"},{"name":"近存储计算","slug":"近存储计算","permalink":"https://recoderchris.github.io/tags/%E8%BF%91%E5%AD%98%E5%82%A8%E8%AE%A1%E7%AE%97/"},{"name":"2022","slug":"2022","permalink":"https://recoderchris.github.io/tags/2022/"},{"name":"数据管理","slug":"数据管理","permalink":"https://recoderchris.github.io/tags/%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86/"},{"name":"存储技术","slug":"存储技术","permalink":"https://recoderchris.github.io/tags/%E5%AD%98%E5%82%A8%E6%8A%80%E6%9C%AF/"},{"name":"CQU","slug":"CQU","permalink":"https://recoderchris.github.io/tags/CQU/"},{"name":"JSA","slug":"JSA","permalink":"https://recoderchris.github.io/tags/JSA/"}]},{"title":"(ISCA22)SmartSAGE:在近存储计算架构下训练大规模图神经网络","date":"2022-05-20T13:57:21.000Z","path":"2022/05/20/smartsage/","text":"SmartSAGE: Training Large-scale Graph Neural Networks using In-Storage Processing Architectures 作者：Yunjae Lee等人, lead by Minsoo Rhu @ KAIST(Korea) 既要提升图神经网络（GNN）训练扩展性，又要保证系统整体的性能。 主要做法是通过采用SSD保存原始图数据，提升了GNN训练的扩展性； 通过用近存储计算的方法完成GNN的采样步骤，保证了方案的处理性能。 背景图神经网络训练图神经网络的训练主要有选择目标节点、目标节点邻居采样、特征查找，最终将邻居节点的特征通过多层感知机实现训练，以得到目标节点或边的嵌入向量。 架构导致的瓶颈问题 实际训练流程往往是按照以上这张图的步骤来进行的，主要分为两个大的环节：数据准备和GNN训练。而这两个环节有完全不同的计算特征：在数据准备阶段中的邻居采样是在整个大图上进行的，且访问不规律；而后面GNN训练阶段的特征聚合以及GNN训练是在前面采样出来的一个很小的子图上完成的，其计算特征固定，可用GPU进行加速。 传统的训练流程和架构的映射如图a所示。考虑到访问的不规则，图神经网络框架（e.g. PyG，DGL）往往把采样和邻居特征查询这两个步骤放在内存中完成的，然后再将查出来的子图的特征通过PCIe传送到GPU的memory上进行计算。 这样做的瓶颈在于：采样这一步的扩展性差。主要问题是采样是针对整个大图的，对于大型的图内存很有可能放不下，设计人员不得不限制图的大小，那么这等于限制了图神经网络表达能力，进而限制网络的实际应用效果。 要解决的基本问题 一个自然的想法是上图所示的架构：采用SSD作为存储数据的主体，DRAM作为缓存来存正在计算的数据或者缓存一些局部性高的数据，采样仍然在CPU上完成。然而一个很明显的问题在于：SSD的吞吐量显然没有DRAM高，对性能会产生影响。由此产生的一些疑问是：这样做是否会极大程度的影响整体性能？有什么方法可以进一步优化此架构下的性能表现？ 下面的motivation实验，文章主要展现了用DRAM和SSD存放图数据进行图神经网络训练时，系统的一些特征。 论文为什么要做这个事情？端到端性能分析作者先分析了DRAM和用SSD来存放数据时，进行GNN训练时一个端到端的时间模型。如下图所示： 图a展示了in-memory的运行模型，b则是采用ssd作存储设备的运行模型。实际运行中，训练和采样是并行的。可以看到，由于SSD的带宽低于DRAM，CPU上的数据准备步骤会引发高延时，以至于让GPU陷入大量的空闲等待，产生对计算资源的浪费。 下面文章剖析了为什么会出现这种现象： 当用DRAM存图数据时 为了更好的理解采样过程的特点，对传统方案，文章测试了系统的cache miss率和带宽利用率两个参数，探究计算延时是由什么导致的。 文章发现在in-memory时cache命中率非常低，且内存带宽的利用率也非常低。从算法角度很好解释。而进一步，这说明：采样操作的性能主要被纯的memory latency限制，而不是被throughput限制。这一点将指导基于SSD的方案设计。 SSD存图数据文章首先给出了全文的一个baseline：即将图数据保存在SSD上，并在主机上进行采样操作；访问数据时采用mmap对图数据集文件在用户空间上做地址映射，这样能够利用操作系统的page cache进行缓存，以提升局部性。 文章下面对这个方案的性能进行了仔细分析。下图展示了in-memory和baseline两个方案训练延时的breakdown以及端到端计算延时的倍数增长： 可以看出，将存储主体从DRAM改为SSD之后，整个延时的开销主体变为了采样这一步，并且延时有了平均9.8倍的增加。原因是sample的局部性非常差，操作系统中页面缓存的命中率很低，用mmap非但不能从局部性获得好处，反而会引起频繁的缺页中断和操作系统的用户-内核态切换。因此，图采样成为了训练过程中的瓶颈问题，最终将引发GPU计算资源的空闲。如下图所示： 文章采用近存储计算的方法来解决这个问题，旨在弥补DRAM和SSD方案之间巨大的性能差异。 该论文解决了什么难点，难点存在的原因是什么，作者是如何发现这些难点的？文章要解决的主要难点是：既要用SSD来打破内存容量的限制，又要用近存储计算的手段来缩近SSD与DRAM之间巨大的性能差距。 软硬件协同： 第一：硬件上，用近存储计算的手段将随机采样的计算的过程卸载进SSD里面。将采样这一算法步骤部署在存储器的固件中，其目标是提升子图生成过程中的有效吞吐量，减少无关数据的转移； 第二：软件上，摒弃了之前以优化局部性为导向的方案，设计了以优化延时为导向的运行时系统和主机驱动栈。不再采用操作系统的页缓存，使用Linux的直接I&#x2F;O命令并设计了用户空间的数据缓存。这使得运行时系统可以直接将数据读取到缓存上，减少了查询软件栈的开销；同时合并了多条NVMe指令，减少了多条I&#x2F;O指令调用的开销。 针对以上难点，作者各做了哪些优化，优化背后的思想是什么？硬件上：ISP作者通过修改SSD控制器中的固件代码，实现了对采样(sampling)过程的近存储计算。 Why not FPGA-based but firmware-based?第一、采样这个过程的计算的密度很低。主要复杂度在于随机数据的查询；没必要用FPGA。 第二、用FPGA需要额外的数据转移：即从SSD通过PCIe switch到FPGA，再从FPGA到主机端CPU的过程，本身数据转移的延迟高，用FPGA是否能获得性能提升有待推敲。文章在实验阶段也验证了这一点。 而计算密度低的采样过程刚好与羸弱的嵌入式处理器性能匹配。 近存储计算设计 图a是baseline的方案。当我们要在这两个Batch，4个目标节点上完成采样的时候，我们首先要将这4个目标节点的邻居节点的地址找到，然后依次发送以block为级别的I&#x2F;O请求，把数据全部从SSD传送到CPU一侧，然后在主机上完成采样。 图b是文章建立了近存储计算模块之后的，我们可以直接在存储端完成采样，并把采样后的子图发送给主机一侧。 这样做提升了系统的有效吞吐量，并且由于发送给DRAM的只有采样出的很小的子图，这也可以合理利用宝贵的带宽。 主要两个模块：近存储控制模块和子图生成器；主要流程： 主机端发送 【子图生成】 的I&#x2F;O命令，SSD接收（1）； 存储端通过DMA从主机端获取目标节点等关键信息（2），子图生成模块将此解码并获得目标节点的邻居节点所在的物理页面号（3）； 发送物理页面的读取命令给底层的读取队列（4）； 读到物理页面信息，先缓存在SSD的DRAM里面（5），子图生成器模块对数据进行采样，并将获得的子图存放在子图缓存区中等待发送；（6） 计算完毕后，将采样得到的子图发送回主机端。（7） 软件上：以延迟优化为导向的运行时系统和主机驱动栈以延时优化为导向是针对于“以访存局部性优化为导向”这一方案提出的。 第一，“以访存局部性为导向”实际上就是baseline系统中基于memory-map的SSD数据访问，采用操作系统页面缓存来优化局部性的这种方案。这种方案在图采样应用上，对于减少I&#x2F;O访问时间几乎没有什么用，因为随机采样有很差的局部性，反而由于页缓存的存在，在读取数据的时候需要一些复杂的缺页中断处理等操作。 第二，baseline的方案需要大量的I&#x2F;O请求，这是因为每个I&#x2F;O请求对应的是以chunk为单位的数据，而很多数据都是无用的，这又增加了另一个层面的计算延迟，即每个以chunk为单位的请求穿过主机驱动栈的时候，都需要经历从用户空间到核空间的双向转换。 下面的优化主要针对运行时系统和主机驱动栈，以延时优化为导向进行设计。主要两个方面：越过软件栈完成直接I&#x2F;O；同batch的I&#x2F;O请求的合并。 直接I&#x2F;O用linux的直接I&#x2F;O特征，使得文件写和读操作可以直接从用户空间的应用访问到存储设备，不需要经过操作系统的页面缓存，也不需要从用户空间到核空间的双向转换。 I&#x2F;O命令合并对于同一批采样，调用SSD做计算的命令合并在一条I&#x2F;O指令中。系统预先将计算需要的参数保存在本地上；调用唤醒近存储计算模块的I&#x2F;O指令时，通过DMA将参数全部发送过去，近存储端再进行计算。整个batch只需要一条命令，这样大大减少了调用I&#x2F;O的命令的条数，缩减了主机驱动栈的开销。 实验结果实验设定主机端：CPU-GPU系统，包含Intel Xeon Gold 6242 CPU with 192GB of DRAM and NVIDIA’s Tesla T4 GPU CSD平台：Cosmos+ OpenSSD 对比对象： baseline方案； 基于FPGA的CSD：Samsung-Xilinx’s SmartSSD in-memory方案：用intel optane PMEM，提供768GB的空间。 Mini-batch大小：1024 单线程性能对比 单线程性能对比： software-only:1.5x hw-sw: 6.6x to software-only 命令合并的效果如上图所示，横坐标是命令合并的大小，纵坐标是性能表现（越高越好）。 多线程性能对比 **software-only:**3x Hw-sw: 1.3-1.4x to software-only 上图显示，当线程数逐渐增多时，加速比降低。这是因为存储端的算力限制导致的，多个线程同时做计算会挑战存储端的计算能力。下面的一段论述描述了如果采用NGD的Newport上的嵌入式处理器（四个ARMv7 M7运行固件管理程序，双核ARM v8 Cortex A53专门用来近存储计算），效果将会更好。 Latency Breakdown对比 这里添加了一个smartsage(oracle)，文章在这里估算了如果采用Newport系统进行近存储计算的表现。 首先，sampling这个瓶颈问题被很大程度上缓解了； 其次，这里也估算了用DRAM做存储的方案，相比于DRAM的性能只差60%，但DRAM方案不现实； oracle方案相比于傲腾（PMEM）只是1.2倍的延时开销，但是价格大大降低了。 Why not FPGA? 相比起来，FPGA将大量的时间用在了SSD到FPGA之间的数据传递上，整体的时间开销可能比sw版本还要大。 参数敏感性 采样率升高的时候，系统加速效果会逐渐减弱。（算力限制、子图变大导致从SSD到主机的数据传输量变大） 自己的思考 优点：动机实验做的很详细；实验充分；把一些问题写的很清楚（为什么不用FPGA、构想了如果有更好的处理性能效果将会怎样） 创新性？：硬件部分本质上是对随机采样过程做了近存储计算，且每个batch的计算量是一个常数，从应用的角度来说对计算总量有很强的限制。相比于我们做的random walk计算量要小的多（random walk是随着图的尺寸扩展计算量在增大的，且在random walk实际的调度率接近100%）；软件部分，对软件栈的优化比较简单，命令合并在设计中也显得很自然（我们也想到了，但感觉没必要着重写），整体来说好像新意不是很足。。。但是结合GNN训练的背景，文章的写作值得学习。 忽略了嵌入处理器的性能问题：文章对算力的问题没有额外的讨论，通过采样这一个对算力依赖非常小的应用达到了很好的实际效果；但是从最后的参数敏感度实验可以看出，采样率变高的系统的加速效果是逐渐减弱的。","link":"","tags":[{"name":"论文阅读","slug":"论文阅读","permalink":"https://recoderchris.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"name":"近存储计算","slug":"近存储计算","permalink":"https://recoderchris.github.io/tags/%E8%BF%91%E5%AD%98%E5%82%A8%E8%AE%A1%E7%AE%97/"},{"name":"KAIST","slug":"KAIST","permalink":"https://recoderchris.github.io/tags/KAIST/"},{"name":"2022","slug":"2022","permalink":"https://recoderchris.github.io/tags/2022/"},{"name":"ISCA","slug":"ISCA","permalink":"https://recoderchris.github.io/tags/ISCA/"},{"name":"GNN","slug":"GNN","permalink":"https://recoderchris.github.io/tags/GNN/"},{"name":"图随机游走","slug":"图随机游走","permalink":"https://recoderchris.github.io/tags/%E5%9B%BE%E9%9A%8F%E6%9C%BA%E6%B8%B8%E8%B5%B0/"}]},{"title":"(ASPLOS22)GenStore：面向基因序列分析的高性能近数据处理系统","date":"2022-03-18T13:51:05.000Z","path":"2022/03/18/genstore/","text":"GenStore: A High-Performance In-Storage Processing System for Genome Sequence Analysis 作者：Nika Mansouri Ghiasi等人, lead by Onur Mutlu @ ETH Zurich(Switzerland) 本文指出了基因序列分析的性能瓶颈为存储设备的I&#x2F;O，为了解决这一问题，通过在近数据端添加两个加速器实现对数据的筛选，实现了性能的大幅提升。 基因序列分析背景知识端到端的基因序列分析应用场景药物开发、病毒溯源和进化研究 主要目的识别样本基因组和已知基因组之间的可能匹配点和差异点。 基本步骤 采样以及DNA样本测序：现代测序技术无法测序整个DNA，只能在这个DNA上随机采样一些短的子序列，并对他们进行测序。 转换：将测序到的波形图转换成碱基对（A,G,C,T等基本组成单元）的表示，这些表示被称为read(译：待匹配序列)，以便存储和处理； 图1 DNA序列转换 序列匹配(Read Mapping)：识别待匹配序列与参考基因组之间的匹配点和差异点，进而还原出整个DNA。 图2 序列匹配 对于同一样本，测序和转换只需要进行一次。而序列匹配出于需要与多种参考基因组进行配对、提升匹配的精度等考虑，可能要进行多次。又因为其匹配算法时间的复杂度高，成为了整个端到端基因序列匹配的瓶颈。因此本工作主要针对序列匹配进行加速优化。 序列匹配由于参考基因组与待测序列大小不匹配，直接匹配的搜索空间太大。一般采用对参考基因组建立索引的方法来缩小搜索空间。即把整个参考基因组分成若干个长度为k的碱基序列（k-mer），然后把待测序列也分成若干个长度为k的碱基序列，对小片段进行匹配，最后通过算法拼接找到匹配位点。下图展示了怎么把一段序列分成多个5-碱基序列。 图3 产生5-碱基序列 我们用Key-Value方式记录索引结构，Value为此k-碱基序列的匹配位置。上图的一个key-value对为：&lt;AGACC, 5&gt;，即AGACC在5号碱基位置可以完全匹配上。 整个序列匹配过程如下 寻找潜在匹配位点：待测序列的k-碱基序列通过遍历查询KV表，找到所有可能匹配的k-碱基序列和位点。 过滤掉不可能匹配或完全匹配的待测序列 序列对齐：所有筛选过的位点与待测序列用计算复杂度很高的近似字符串匹配算法（ASM）计算得分，最终返回ASM达到最高分的位点，即为待测序列的目标匹配位点。 在以上计算步骤中，最耗时的是采用ASM方法进行序列对齐，是序列匹配中的一个性能瓶颈。 机会：过滤序列由于ASM过程是处理过程中的性能瓶颈，为了最大程度地减少进行ASM的待测序列数量，过滤阶段就尤为重要。可以通过简单算法过滤掉的序列分为两种： 完全匹配的待测序列：待测序列可以完美与参考基因组匹配，不需要ASM来验证了； 无法匹配的待测序列：待测序列和参考基因组之间在匹配位点上就有很多差异，不需要ASM做匹配。 前人的大量工作主要集中在两个方向： 设计不同的过滤机制 → 减少ASM的调用次数 设计加速器或算法 → 提升ASM的性能 这个事情之前别人是怎么做的，存在什么问题？有三种方案来提升序列匹配(read mapping)的性能： 设计高效的、启发式的ASM算法； Fast Gap-affine Pairwise Alignment Using the Wavefront Algorithm【Bioinformatics, 2021】 主机侧的ASM硬件加速器； Genesis: A Hardware Acceleration Framework for Genomic Data Analysis.【ISCA, 2020】 主机端的过滤器，剪枝掉不需要ASM的子序列。 Acceleration of Long Read Pairwise Overlapping in Genome Sequencing: A Race between FPGA and GPU.【FCCM, 2019】 以上方案都没有考虑系统的I&#x2F;O开销！实际计算中，大量数据从存储系统流向主机端memory和加速器的计算单元。主机端完成的过滤器并没有从根本上减少数据在主机和存储设备之间的流动，而其中有很多数据是可以被过滤掉且不会重用的。 论文为什么要做这个事情？为了验证采用高效的近数据处理过滤机制能够明显提升基因序列匹配的性能。文章首先通过一系列Motivational Studies来证明这一点。 评估了5个系统实现方案 Base：主机侧SOTA软件Minimap2 SW-filter：主机侧SOTA软件Minimap2 + 主机侧过滤器 Ideal-ISF：主机侧SOTA软件Minimap2 + 近存储端过滤器 ACC：主机侧SOTA硬件加速器 Ideal-ISF + ACC：主机侧SOTA硬件加速器 + 近存储端过滤器 4种系统配置： SSD-L，SSD-M， SSD-H代表三种SSD，分别采用SATA3、PCIe Gen3 M.2和PCIe Gen 4的I&#x2F;O接口；以及DRAM：所有处理数据已经提前被load到DRAM中，无需I&#x2F;O。 图4 Motivational Study结果 观察1（Ideal-ISF vs. Base &amp; SW-filter &#x2F; Ideal-ISF + ACC vs. ACC） 近数据过滤器相比于其他系统表现出了大幅度的性能提升。 分析：① 从根源上减少数据的搬运；② 减少了大概80%的无用数据的过滤，减轻了其他部分的算力负担。 观察2 （In Base &amp; SW-filter, SSD-L vs SSD-H） 使用更好的SSD以提升外部带宽，能够很大程度上提升系统性能，换用DRAM反而没有很大的性能提升了。 分析：采用低端SSD时，应用是I&#x2F;O瓶颈的。然而换用更好的SSD后（SSD-H或DRAM），系统的性能瓶颈又变成了CPU或主存。 观察3 （SW-filter DRAM vs. Base DRAM vs Ideal-ISF） 采用主机侧过滤器仅能够提升41%的性能，而采用存储侧过滤器时则体现出大幅度性能提升。 分析：在主机端使用过滤器会与主程序抢占系统资源；而在存储端添加过滤器能够减轻整个系统在过滤上的负担。 观察4（ACC vs. Ideal-ISF + ACC） 当主机端有加速器时，采用SSD-H比采用DRAM方案慢23%。 分析：主机端有加速器的情况下，I&#x2F;O带宽重新成为了系统瓶颈。而采用Ideal-ISF + ACC的方案能够有效提升系统性能，说明在采用主机端加速器场景下，数据转移是新的系统瓶颈。因此，减少数据转移，进而加速本应用是值得一试的系统设计方案。 综上所述，在存储端做过滤器有两大好处： 从观察2、3可以得知，当系统带宽足够大的时候，处理性能成为了系统的瓶颈。采用主机端的过滤器会与系统抢占资源，因此，我们可以采用近存储的过滤器来减少系统的工作负担； 从观察4可知，当系统的计算资源充足的时候，I&#x2F;O总量重新成为了系统的瓶颈。采用主机端过滤器并不会减少存储到主机的I&#x2F;O总量，因此，我们可以采用近存储的过滤器来减少系统的I&#x2F;O总量。 该论文解决了什么难点，难点存在的原因是什么，作者是如何发现这些难点的？主要以下三个难点： 性能上：存储端的过滤器性能要高，使得近存储端的过滤能够与主机端的序列匹配在时间上重叠； 序列的多样性：由于测序仪器的不同，产生的序列长短、错误率也差异明显。测序仪器明显分为两种： 产生短序列且错误率低，匹配位点会很多； 产生长序列但错误率高，不太好与基因组进行匹配。 针对不同长度和不同的错误率，要求近存储过滤器能够分门别类处理。 硬件开销上：最好不要有明显的额外硬件开销。 针对以上难点，作者各做了哪些优化，优化背后的思想是什么？系统概况文章提出了第一个基因序列分析的近存储处理系统：GenStore，通过利用低开销且精确的近存储过滤器，减少了数据转移和计算开销。 图5 GenStore整体架构图 主要的改进点：一个SSD级别的加速器；多个channel级别的加速器。用新的GenStore-FTL来管理数据流和控制流以及与主机通信。 处理流程： ① HOST发出分析指令； ② SSD进行准备：将传统的FTL数据刷新到Flash上，将GenStore必要的额外数据加载进来； ③ 从所有NAND flash芯片上读取数据，充分利用NAND flash的带宽。（与④同步） ④ 通过设计两个加速器，对不需要进一步分析的待测序列进行过滤。（与③同步） ⑤ 将没有被过滤掉的数据传送给HOST，通过ASM算法进行详细分析。 ③能够实现充分利用带宽，是因为在数据预处理的时候对数据存放位置进行了仔细设计，使得每次访问都能够充分利用channel间的并行性。下面主要分析④中两个加速器的详细设计的详细设计，这两个加速器针对之前我们说的两种情况： ① 完全匹配(Exactly Matching, EM)的序列：能够与参考基因组完美匹配或者只有1~5个碱基差异的序列； ② 无法匹配(Non-Matching, NM)的序列：由于基因突变或测序时的误差，导致无法与参考基因组匹配序列。 完全匹配序列筛选加速器：GenStore-EM针对场景测序产生的短待测序列，有很低的错误率。根据统计，在数据集中有接近80%序列是短序列，且这部分序列能够完全匹配到目标基因组上。 设计挑战识别每一个序列的完美匹配都需要对两个大型数据结构进行多次随机访问： ① 访问k-碱基序列索引列表，找到匹配的位置。这是一个随机的搜索； ② 访问参考基因组，找到匹配位置的完整序列，然后进行是否完美匹配的判定。 主要设计通过仔细设计额外数据的结构和数据的形式，可以顺序化数据访问特征。文章设计了一个新的排序的k-碱基索引结构。 图6 排序的k-碱基索引结构 SKIndex是参考基因组的k-碱基索引结构，其中k的大小设定为了序列长度，图中假设序列长度为3。SRTable是待匹配序列表。完美匹配的流程通过流式处理两个表来实现。 主要流程如下所示： ① 若待匹配序列表中的值跟索引结构的键相同（R&#x3D;K），则发现了一个完美匹配。可以直接返回结果； ② 若待匹配序列表中的值大于索引结构的键**(R&gt;K)**，指向键值表的指针往后移； ③ 若待匹配序列表中的值小于索引结构的键**(R&lt;K)**，说明这个序列找不到完美匹配，指向待匹配序列表的指针往后移。 完美匹配的判定可以通过采用比较器逻辑来完成，可以充分流水化筛选过程。 实现 图7 GenStore-EM的实现细节 ① 取数据：从NAND Flash中，以batch为单位读取SRTable和SKIndex两个数据结构； 在这一步为了充分利用了SSD的内部带宽，作者采用了两个方法。一是SRTable Batch和SKIndex Batch都是每个plane的整数倍，以保证100%的flash利用率；二是GenStore用了两个Buffer，一个处理，另一个通过flash读取数据。 ② 完美匹配筛选：使用了比较器的逻辑完成完美匹配的筛选。 步骤①和②都是流水线处理的，完成后将未被筛选的数据返回给主机端。步骤①的时间要比步骤②长，通过提升存储器的内部带宽能够实现性能的进一步扩展。 无法匹配筛选加速器：GenStore-NM针对场景测序产生的较长的待匹配序列，错误率也比较高。以往都是采用链式分析(chaining)的方法：首先找到所有匹配位点，之后采用动态规划的算法，将重叠的匹配位点组合成长链，算法的优化目标是相似度。最终筛选掉相似度低于阈值的待测序列。 设计挑战链式分析加速器需要执行昂贵的动态规划算法，可能会产生显著的性能开销。 主要设计GenStore-NM设计为仅对扫描出极少量匹配位点的序列执行链式分析算法，将匹配位点多的序列直接发送到主机系统。上述设计可以由下图来验证： 图8 匹配位点与匹配成功概率的关系 图的横坐标为匹配位点数量，纵坐标为与参考基因组匹配成功的概率。可以看出，当匹配64个位点以上时，这个序列基本上能够与参考基因匹配的。因此，我们可以通过简单的匹配位点数量判断把这部分筛选工作去掉，而匹配64个位点及以上的序列占到了整个数据集的85%。 综上，通过简单的对匹配位点的数量加以限制，加速器能够减少85%的工作量，也能够保证原有工作的精度。 加速器实现 图9 无法匹配筛选加速器和系统图 ① 取数据：将输入序列集合从flash上读取出来，因为这些序列比较长，因此先生成待匹配序列的k-碱基序列，这里采用滑动窗口的方式来生成。之后将待匹配序列的k-碱基序列与目标基因组的k-碱基序列相匹配，找出所有的潜在匹配位点，这里采用哈希加速器来完成。终止条件是查到了N个位点（送至HOST直接处理）或者达到了序列的尾端；所有的匹配位点被存进本地的buffer中； ② 通过匹配位点数量简单筛选：如果潜在匹配位点数量太少，以至于序列不可能匹配上，可直接筛选；如果潜在匹配位点数量太多，以至于序列大概率能匹配上，直接给HOST做处理； ③ 通过链式分析进行筛选：通过加速器设计，完成链式分析的动态规划算法，将所有相似度低的序列筛选掉，这样就完成了无法匹配序列的筛选。无法筛选的部分直接被送入HOST来完成最后ASM算法的匹配。 链式分析的动态规划公式以及对应的PE如下图所示： 图10 链式分析加速器的PE设计 步骤①、②、③都是流水线处理的，且可以完成channel级别的加速。 实验结果实验设定 Base：Minimap2, GenCache（短序列匹配加速器）和Darwin（长序列匹配加速器） GS-Ext：Base + 存储器外部的过滤器（用与GS作对照，看I&#x2F;O是否是性能瓶颈） GS：Base + 存储器内部的过滤器 验证平台： 采用MQ-Sim作为SSD的模拟器平台，Ramulator作为DRAM的模拟平台。软件方案是完全采用实际系统进行测试的。仍然对SSD-L， SSD-M， SSD-H三种带宽情况进行分析。 面积开销 图11 面积开销 总的面积开销非常小，小于在SATA SSD控制器中的ARM R4处理器的9.5%。 性能对比 GenStore-EM SIMD是在HOST端采用了以往的过滤机制的方案。 根据图(a)，GS比Base快2.07-2.45倍，比SIMD快1.66-2.09倍。这是因为减少了数据搬运，且减少了主机端的工作负担。GS-ext由于I&#x2F;O带宽的影响，在低端SSD上也差于GS方案，但是这一点可以通过改用更好的SSD来获得改进。SIMD不如GS-Ext方案效果好，这是因为SIMD没有优化随机访问等影响性能的因素。 图(b)是在启用了硬件加速器后的实验结果。首先，GS比Base性能提升了1.52~3.32倍。第二，GS-Ext要比Base慢很多，这是因为在存储器外的加速器需要访问很大的额外数据结构，这个访问会受到带宽的限制。 因此，采用近存储端筛选的方法能够有效解决序列匹配中的I&#x2F;O瓶颈问题，尤其是当采用硬件加速器时，I&#x2F;O成为了系统的新瓶颈，采用近存储的方法是进一步提升系统性能的有效手段。 GenStore-NM 根据图a，GS比baseline性能提升22.4~29.0倍左右。其性能提升原因同上。 根据图b，GS性能提升了6.85~19.2倍，GS-ext在SSD-L和SSD-M都没有很大的性能提升，这是因为被I&#x2F;O带宽限制了性能，而采用近数据的方案可以很大程度上提升系统性能。有更广泛的应用场景。 自己的思考 近数据处理的定位是轻量级计算，实际上比较适合进行数据的筛选；有时候可能不需要在SSD内部完成全部计算。（减少数据传输+减少计算量） 文章中关于I&#x2F;O瓶颈还是计算瓶颈的分析值得学习，在实验对比中考虑了一个很重要的环节：加速器为什么要放在SSD上而不是HOST上，采用对比GS和GS-ext，用控制变量法来说明； 在计算本身不好优化的情况下，可以多考虑怎么从数据集的特征上完成剪枝。比如GenStore中根据匹配位点的数量进行初步筛选，可以将筛选的工作任务减少85%之多。这是因为作者发现了“匹配位点超过64的序列基本上都能够被匹配上，不需要做筛选”这一规律。","link":"","tags":[{"name":"论文阅读","slug":"论文阅读","permalink":"https://recoderchris.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"name":"加速器","slug":"加速器","permalink":"https://recoderchris.github.io/tags/%E5%8A%A0%E9%80%9F%E5%99%A8/"},{"name":"近存储计算","slug":"近存储计算","permalink":"https://recoderchris.github.io/tags/%E8%BF%91%E5%AD%98%E5%82%A8%E8%AE%A1%E7%AE%97/"},{"name":"ETH","slug":"ETH","permalink":"https://recoderchris.github.io/tags/ETH/"},{"name":"ASPLOS","slug":"ASPLOS","permalink":"https://recoderchris.github.io/tags/ASPLOS/"},{"name":"2022","slug":"2022","permalink":"https://recoderchris.github.io/tags/2022/"}]},{"title":"(Report)从研究到概念验证:在商业搜索引擎上分析FPGA的部署策略","date":"2021-12-10T13:47:53.000Z","path":"2021/12/10/Report-FPGA/","text":"From Research to Proof-of-Concept: Analysis of a Deployment of FPGAs on a Commercial Search Engine 作者：Maschi Fabio; Alonso Gustavo; Hock-Koon Anthony; Bondoux Nicolas; Roy Teddy; Boudia Mourad; Casalino Matteo @ ETH Zurich 本文解决了如何以经济高效的方式将 FPGA 集成到实际应用场景中，并保持其在算法方面的性能优势。 FPGA在性能、架构、能耗等方面有很多优势，但是利用FPGA为现有的应用提供所有的优势并不简单。很少有研究来分析 FPGA 该如何部署、需要做什么改进才能使其在实践中具有成本效益，也很少有人分析在实际计算系统中应用 FPGA 的效率。 本文报告了将研究理念转化到实际场景后，对产生的性能和可能部署的估计成本进行了评估。 该论文解决了什么难点，难点存在的原因是什么，作者是如何发现这些难点的？本论文主要报告了以下难点，并进行了讨论： 模型的差异：需要将在学术上使用的简化模型转变成功能完备的组件，本文讨论了这些变化对整个系统设计的影响 软件到硬件要考虑的选项：当一个设计模块从一个软件设计模块迁移到一个在 FPGA 上运行的组件时，有很多选项可以选择来将它集成到整体设计中。 FPGA的利用率：FPGA的性能的利用率可能比较低，导致整个系统的性能达不到峰值。 成本问题：使用FPGA来部署云服务，系统的成本可能不降反升。本文对系统的成本进行了简单的估计和对比。 本文的背景： Amadeus(一个提供路径规划解决方案的公司)需要为他们的航班搜索引擎(Flight Search Engine)提供加速。 航班搜索引擎（大问题背景）：给定一个查询（这里称为用户查询），查询中指定出发地、目的地和相应的日期，它会响应出发地和目的地之间的一些可能航班路线，航班路线中包含航班信息和价格。 功能模块（架构） Domain Explorer：探索满足要求的所有可能的转机机场、航空公司和航班组合；预先选择一些潜在的路线。MCT(Minimum Connection Time)：最小转机时间模块，用于搜索的早期阶段，在搜索引擎的运行性能和总成本方面起着关键作用。 Route Selection：该使用一些策略（用Route Scorin作为标准）进一步减小可能购买的潜在路线集。 Pricing Engine：估计价格 MCT模块（加速器） 如上图所示，根据每个航线提供的航班信息来决定机场的最小转机时间，这些信息被称为“规则”。（比如r2代表ZRH机场在2021年夏天的时候，去往国内且航站楼为T1的情况下，换乘时间为有很高的可能性为40分钟）。定义 MCT 的“规则”会一天修改一次，但是基本上变动比较小。现有的MCT模块中规则条数超过160万条。 本文的问题背景是，已经在研究中利用单独的FPGA对MCT模块进行了充分的加速。接下来需要将这个模块部署到服务器上，在实际的搜索引擎中，借此来讨论FPGA在部署到实际应用场景中导致的很多普遍性问题。其中讨论了三个最主要的指标：吞吐量、处理每个请求的时间和价格 针对以上难点，作者各做了哪些优化，优化背后的思想是什么？现有的FPGA加速引擎：ERBIUM 离线部分 NFA(非确定有限状态自动机)优化器：在规则集上优化 NFA 形状（实际上是调整“规则”中选项的顺序） Constraint Generator 约束生成器：根据规则结构（即标准数量、数据类型、运算符）和 NFA 形状定制硬件内核。 NFA 解析器：根据当前硬件设置和规则集构建 NFA 的内存文件。 在线部分 Host Executor（CPU）：将 NFA 数据加载到 FPGA 内部存储器中，以及发送 MCT 查询并获取其结果 FPGA Kernel：加速器 理论与现实模型的差异理论分析实际应用场景的的规则结构引入了选项之间的相互依赖关系，以及不同的优先级权重等级等等新的因素。对于软件来说比较好更改，但是对于硬件来说可能会导致进一步的后果。主要的更改： Criteria Merging：将从前的按照顺序判断选项改为了用一个范围来判断。需要采用NFA解析器模块，生成的NFA随之增大，会增加内存使用，影响延迟； Precision weight for ranges：新版本的规则中，预测的精度与航班号范围相关，而航班号之间可能是有范围上重叠的。需要动态地去除选项之间的重叠范围，再次使用NFA解析器模块，通过解析生成无重叠的大量新规则。但是新的规则同样会使得NFA增大，影响延迟。 Cross-matching criteria：请求中的某一个选项不一定只匹配同一个规则中的域。需重新建模且更改硬件kernel的逻辑。与前两种情况不同，这种变化通常不会导致延迟、内存要求以及更重要的精度权重结构发生任何重大变化。 综上所述，以上的很多问题都可以通过NFA解析器来解决，这是此系统设计的一个优点，就是不需要每次改动都改变FPGA的电路，以达到最高性能。证明了原有方案具备一定的通用性。 但以上的实际场景问题确实会导致NFA模型变大，延迟提高。除此之外，在实际场景中shell是阻塞处理，与之前设计中的流式处理不同，同样会影响性能。 实验验证 实验配置：用真实场景下(MCT V2)，测试一个kernel中采用1&#x2F;2&#x2F;4个引擎的条件下系统的吞吐量和单条指令处理时间，以及在原有的研究场景(MCT V1)下两项指标的结果。 现象：当Batchsize增大到1024之后时，我们观察到与原始设计相比有较为显著的性能损失（吞吐量差11%）。这可以看作是原型机和需要考虑所有极端情况的系统之间差异的说明。其主要原因是由于更大的NFA模型、从流式处理接口换成了阻塞式接口导致的。 讨论在将系统的一部分移至 FPGA 时，如果没有合适的NFA生成器，新标准引起的每一个变化都需要对 FPGA 上的电路进行彻底的重新设计，由于开发成本高，这将使整个方法非常不切实际。这说明使用适当的工具并采用易于调整的灵活设计，开发成本的可以降低到最低限度。 舍弃最高性能，换取通用性和易于维护性，这在软件中很常见，但在许多基于 FPGA 的系统中仍然需要考虑这一点。 正如我们的用例所示，设计的实际可行性不仅取决于其性能，还取决于其扩展性和鲁棒性。 在实践中，所有组件的负载往往会随着时间的推移而增长。虽然在NFA生成器中会产生更大的 NFA，影响处理请求的效率，但是频率的影响可以通过使用更大的批量大小得到补偿。 作为吸取的教训，在考虑设计时，重要的是要考虑系统如何随着时间的推移进行灵活变化和修改，需要从头开始重新设计以适应变化的超优化设计不太可能在实际部署中获得回报。 软件到硬件：需要考虑的配置理论分析 Injector：输入 Domain Explorer：类似于数据管理系统中的进程池。每个进程会会触发对一到五个 MCT 查询。 ZeroMQ：通信。用于Domain Explorer进程和router之间的同步通信，以及router和多个MCT工作线程的异步通信。 MCT Wrapper：用循环方式在不同的工作线程之间分发传入的 MCT 查询，以支持异步通信，从而确保wrapper始终准备好来处理新查询。 Encoder：位于MCT Wrapper，在将 MCT 查询发送到加速器之前，必须对其进行编码。因为它将软件中使用的数据表示调整为更适合 FPGA 处理的格式。 不然FPGA 上的设计会陷入处理复杂数据类型的开销中。 XRT：处理工作线程与 FPGA 内核之间的通信。 它调度数据移动和请求的执行。 FPGA：加速器本体。在FPGA上有多种可能的组合来充分利用计算资源。 例如，可以有一个带有四个并行引擎的内核； 或两个内核，每个内核有两个并行引擎。 第一种被调整为尽可能快地处理一批查询（从而改善延迟），第二个设置优先考虑两批查询的并行执行（从而提高吞吐量）。 实验探究 我们首先对最基本的场景进行性能评估，其中每个并行元素都减少为一个。可以看出： 小批量（最多 4,096 个 MCT 查询）的实际处理时间主要由数据移动开销决定。 大批量中编码器施加了线性增长且非常长的执行时间，甚至比 FPGA 内核的实际 MCT 查询处理时间还要长。 说明：FPGA 本身的延迟的实际贡献可能不是整个系统中的最大问题，从而消除了提出最优化设计的一些压力。 图七：固定进程和工作线程数量，测量单个内核在改变其内部的引擎数量情况下的吞吐量增益。 结果显示，虽然吞吐量呈现明显的增长，但是由于电路复杂度随着引擎数量的增加而增加，导致工作频率降低 30%。 图8：固定每个内核的引擎数量，改变内核数量的性能。每个内核都有自己的一个进程和一个工作进程作为输入。 实验结果跟上面差不多，但是和改变引擎数量对比（图7），增加内核数量的方案吞吐量要更高。 图9：在内核数量和引擎数不变的情况下，改变主机端到FPGA数据流的并行度。 左面的图显示这样的配置能够最大化全局吞吐量，可以达到每秒 4000 万次 MCT 查询。 另一方面，右边的图展示了单个请求的效率降低了，说明XRT调度器的同步开销会根据线程数量的增加而增加。 图10：探讨对MCT线程施加压力。 使用多个进程调度单一worker，可以看到单个 worker 不会被单个进程饱和，当耦合到多个进程时可以提供更大的吞吐量。当到达16个进程的时候达到饱和，不再能够提供更大的吞吐量。 讨论在搜索引擎的每个组件上要实例化多少并行元素，不仅必须考虑并行处理元素引入的吞吐量的直接增益，还要考虑会对查询的响应时间造成多大的影响。 图 11 展示了这两个维度之间的折中。对于给定的最小吞吐量，例如每秒 2000 万次查询，可以确定具有【4进程 4线程 1kernel 4引擎】 的配置将施加最短的执行时间v；但是，如果将最大执行时间固定在500 𝜇 𝑠，则具有 【2进程 2线程 1kernel 4引擎】 的配置是产生最佳吞吐量的配置。因此，参数配置是非常多变的。 从研究的角度来看，这些结果的一个重要提示是：整体性能不仅取决于 FPGA 设计，还取决于系统的外部元素。 上图显示了延迟和吞吐量方面的性能选项选择范围很大，有许多是由 FPGA 周围的系统决定的（比如进程数、线程数），而不是由 FPGA 本身的设计决定的。 结合第一部分的讨论，这些结果证实了 FPGA 灵活性、易于维护性、扩展性以及它在系统其余部分中的集成度比起他的绝对性能来说更重要。 FPGA的利用率理论分析FPGA的利用率跟他的性能收益相关。事实上，与任何基于 PCIe 的设备相比，CPU 具有更大的灵活性和更少的通信开销。只有当MCT请求达到很大的量之后，FPGA相比于CPU才能有性能上的优异表现。这就需要我们考虑在实际场景中搜索引擎是否会给FPGA产生足够的请求。 搜索引擎的工作过程如下：航班搜索引擎先产生旅行解决方案 (TS)，再产生MCT请求。 在 MCT 调用之前，Domain Explorer会生成潜在 TS 的列表。 该列表首先按照内部启发式进行排序，然后按照顺序为非直航的 TS 调用 MCT 模块。为了探究是否有充足的请求，本文从实际场景中中收集了搜索引擎工作负载的快照。 其中大约 17% 的路径规划对应了直航，即不会生成 MCT 调用。而其他的每个请求会产生 1.24 个 MCT 查询。 实验探究 图 12 显示了 CPU 和 FPGA 的MCT 模块处理单个用户查询的执行时间与真实生成的 MCT 查询数量的关系。除此之外还绘制了执行用户查询所需的 FPGA 调用次数。 对于少于 400 个 MCT 查询的小工作负载，CPU 实现的执行时间更短，这是因为 PCIe 总线对 FPGA方案施加了重要的通信开销导致的。对于更大的工作负载，FPGA 的性能优于 CPU，即使是在同一个请求中多次调用MCT模块时。 讨论考虑使用 FPGA 时，获得必要的性能是至关重要的。 比如：设计编码器是使 FPGA 发挥效果所必须的一步。 在这里，我们看到了另一个起到关键作用的方面：批量大小。 所有的实验结果都表明批量大小必须足够大才能使 FPGA 的性能超过CPU。 但是，批量大小由搜索引擎及其工作方式决定。在小批量的情况下，没有足够的工作负载来让FPGA充分利用，并且还会使得通信开销占主导地位。在短期内，负载不均匀会是不好改变的现实，这是因为通常情况下，并没有足够的 MCT 请求供FPGA处理。 成本问题理论分析目前的方案是将FPGA作为协处理器通过 PCIe 连接到 CPU 。 截至 2021 年 2 月，此设置是 AWS F1 和 Azure NP 系列中唯一可用的设置。下面对本地部署和云端部署两种方案进行了成本上的对比。 实验探究表 2 简单估计了基于当前负载（需要 400 个服务器）和 FPGA 在最佳接管负载比例下成本的简化计算。 我们以每台 10K 的采购价格使用 400 台大型多核服务器作为基准。 本地方案中，新设计只有在使用较小的 FPGA 时才具有成本效益，这是因为因为我们不能充分利用 FPGA，使用性能好的FPGA也是一种浪费。本地在采用U50的时候，其价格能够低于只用服务器的版本。 但是在云中，现有的服务器配置对于提出的设计来说是不够的。 云上baseline分别采用AWS和Azure，其baseline分别用c5.12xlarge 和 F48s v2 实例，每个实例具有 48 个 vCPU 和 96 GiB 内存。 对比来看，在云上可用FPGA的 CPU 数量非常小，只有8个或10个CPU，假设 FPGA 承担了 40% 以上的负载，我们需要 1,464 个 f1.2xlarge 或 1,171 个 NP10s 实例才能处理当前负载。与纯 CPU 设计相比，其成本对应增加3倍或2.5倍。 讨论云上成本问题的主要原因来自于 CPU 和 FPGA 之间的巨大不平衡。云上的CPU实例无法在 FPGA 上施加足够的负载，结果导致FPGA 部署成本过高。 在研究层面上，结果指出了考虑如何在 FPGA 上施加足够负载的重要性，尤其是在 PCIe 遇到了带宽瓶颈且无法使用流传输的情况下。 直接的一个想法是：使用FPGA 作为加速器的更好方法可能不是以协处理器的身份连接到 CPU 上，而是可直接从网络上进行访问。可以考虑更改搜索引擎，以远程使用FPGA模块。这是本文作者打算在未来同时使用 TCP&#x2F;IP 通信和 RDMA探索的想法。 自己的思考本文对FPGA部署在实际应用场景后存在的各种问题，包括性能、配置和开销上都做了详尽的探讨。","link":"","tags":[{"name":"论文阅读","slug":"论文阅读","permalink":"https://recoderchris.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"name":"FPGA","slug":"FPGA","permalink":"https://recoderchris.github.io/tags/FPGA/"},{"name":"ETH","slug":"ETH","permalink":"https://recoderchris.github.io/tags/ETH/"},{"name":"云计算","slug":"云计算","permalink":"https://recoderchris.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"}]},{"title":"(FAST21)Behemoth:面向超大规模DNN、以闪存为中心存储的训练加速器","date":"2021-09-24T14:22:33.000Z","path":"2021/09/24/FAST21-Behemoth/","text":"Behemoth: A Flash-centric Training Accelerator for Extreme-scale DNNs 作者：Shine Kim, Yunho Jin, Gina Sohn, Jonghyun Bae, Tae Jun Ham and Jae W. Lee @ Seoul National University, Samsung Electronics 本文针对以自然语言处理领域模型为代表的超大规模深度神经网络(DNN)模型的训练问题，构建了一种以闪存为中心存储的训练加速器。 论文为什么要做这个事情？DNN在自然语言处理、计算机视觉、推荐算法等应用领域有广泛的应用。而增加模型参数的数量对于提升模型的准确度起到至关重要的作用。自然处理领域中各种基于Transformer的模型，其网络中的参数数量达到了超大规模(Extreme-scale)的级别。例如，GPT-3模型的参数数量达到了1,750亿以上。不仅如此，DNN模型的参数规模仍然在继续扩展，如如图1所示： 模型参数数量的扩展对存储系统提出了以下的两个挑战： 内存容量墙(Memory Capacity Wall) 以GPT-3为例，其模型参数大小为700GB。而一个NVIDIA A100的内存容量大概在40GB左右。由此我们可以看出以下几点： DNN模型的大小远远超过了一个GPU计算设备的内存容量； 用户将不得不采用模型并行的方式，将模型分布在多个GPU上完成训练。但是完成模型并行，需要细致地设计负载均衡机制。 GPU&#x2F;TPU上配备有高带宽存储系统(HBM)，其价格相当昂贵。 存储带宽的浪费(Memory B&#x2F;W Underutilization) 随着DNN模型的尺寸扩大，Transformer中各种全连接层矩阵的重用也会变得更加频繁； 因此，随着训练计算总量的不断扩张，实际存储的带宽需求并未大幅度提高；而GPU上的高带宽存储系统也随着计算量而不断增加，造成了明显的带宽浪费问题。 本文希望能够将超大规模DNN训练的中心存储由低容量、高花费的HBM替换为高容量、低开销的闪存块，并辅助加速器设计完成单SSD节点上的训练任务。 这个事情之前别人是怎么做的，存在什么问题？对于超大规模DNN模型训练而带来的内存容量墙问题，有以下两种解决方案： 针对张量管理的异构存储系统 由于用多个设备进行分布式训练对训练设备的内存带宽造成了极大的浪费，因此有一些方法考虑建立一种在异构内存系统中进行张量数据迁移的高效存储管理系统*。但是，超大规模的模型甚至在主机的存储器中也放不下，因此并不是解决超大规模模型的训练问题的长久之计。 参考文献: Hotness- and lifetime-aware data placement and migration for high-performance deep learning on heterogeneous memory systems, [TC 2020] vDNN: Virtualized deep neural networks for scalable, memory-efficient neural network design,[MICRO 2016] *SuperNeurons: Dynamic GPU memory management for training deep neural networks.[PPoPP 2018] 以二级存储为中心的机器学习系统 BLAS-on-flash：构建一个库，用于针对大型数据集的机器学习算法（例如 ISLE、XML）实现高效的闪存速度。 Cognitive SSD： 构建一个引擎，用于检索存储在闪存上的 DNN 模型推理所用的非结构化数据。 本工作的主要着眼点在于用闪存实现数千亿参数的超大规模神经网络语言模型的训练。 *: BLAS-on-flash: An efficient alternative for large-scale ML training and inference? [NSDI, 2019] Cognitive SSD: A deep learning engine for in-storage data retrieval**[USENIX ATC, 2019]** 该论文解决了什么难点，难点存在的原因是什么，作者是如何发现这些难点的？本论文主要解决了以下难点： SSD平台上模型的训练问题：需要设计一个加速器来完成模型的数据并行计算、控制模块间的通信以及充分利用带宽； Flash的低带宽问题：SSD有极低的带宽，特别是非顺序访问。除此之外，由于SSD底层有垃圾回收(Garbage Collection)、磨损平衡(Wear Leveling)等机制，其持续写带宽也明显比峰值带宽低很多。 Flash的耐久性(endurance)问题：SSD只能进行一定数量的P&#x2F;E操作，这被定义为SSD的耐久性或寿命。用SSD作为DNN训练的存储器明显将缩短其寿命，特别是在随机写时，有写放大(Write Amplification)的现象。 针对以上难点，作者各做了哪些优化，优化背后的思想是什么？Behemoth计算平台硬件模块Behemoth系统设计的整体目的是在单节点上完全容纳超大规模的DNN模型，实现数据并行训练。Behemoth计算平台的整体结构图如下所示： 由于执行过程的底层逻辑是一致的，所以计算加速器的硬件部分可以分为三个部分：计算核心、张量缓存以及平台专用的Flash存储系统。 计算核心(Compute Core)：计算张量；控制数据转移；指令翻译。 张量缓存(Tensor Buffer): 计算核心与NAND Flash之间的一块DDR DRAM区域，将张量暂时保存。 平台专用的Flash存储系统(Flash Memory System)：代替HBM成为训练过程中的主要存储，用来存储张量信息。相关优化在后面会提及。 整体上来说，Behemoth系统采用了DDR DRAM+NAND Flash的双层存储结构，这一点是与以往训练方案中采用HBM作为单层存储结构本质上不同的。 面向DNN训练的硬件模块组装利用基本硬件模块，本加速器组装出多个激活计算节点和单个权值计算节点，以实现DNN模型训练的数据并行。如图4所示： 权值计算节点(Weight Node)：更新并存储权值； 激活计算节点(Activation Node): 计算并存储激活值，激活值还将用于反向传播；多个激活计算节点实现了DNN模型训练的数据并行。 主机端(Host)：负责将训练命令序列(包含计算以及DMA命令)发送到加速器上，在训练结束之后，加速器将训练结果返回给主机端。 训练步骤DNN模型的训练是一个重复的正向&#x2F;反向传播的过程。图5(a)展示了单层训练的步骤，其正向传播包含以下几步： 从权重节点闪存中读取权重值，到权重节点的张量缓存中； 将权重节点张量缓存中的权值通过广播发送到激活节点的张量缓存中； 权重张量被加载到片上的SRAM中，由计算核心准备开始计算； 激活节点的计算核心完成计算，生成激活张量； 激活张量从片上的SRAM中被拷贝到激活节点的张量缓存中，准备写入激活节点的闪存块上； 张量缓存中的激活张量写入闪存块中，供反向传播使用；权值张量从张量缓存中被释放。 反向传播与正向传播的步骤类似。当一个迭代结束后，图5(b)展示了权值更新的过程： 最终权值的梯度下降张量从激活节点的张量缓存被送至权重节点的张量缓存； 收到所有权值的下降梯度后，权值节点将权值下降梯度加载至片上的SRAM； 由计算核心来更新权值； 更新后的权值张量被写入张量缓存； 将更新后的权值由张量缓存写入NAND Flash中以供下一轮迭代使用。 平台专用Flash管理系统——带宽问题相比于高带宽存储，闪存存在带宽低的问题。根据之前的分析，GPT-3模型训练要求的带宽大概在50GB&#x2F;s左右，而NAND Flash大概只能提供几个GB&#x2F;s的带宽，无法满足训练的需求。 事实上，NAND设备的带宽峰值并没有得到很好的发挥。为了优化NAND设备的带宽值，可能的方向有以下两个： 尽可能的顺序化对NAND设备的写操作：随机写会造成大量的带宽浪费以及写放大现象； 防止NAND设备固件成为性能的瓶颈：固件中的FTL一般都有如垃圾回收、磨损均衡、备份数据管理等机制，这些机制都会影响NAND设备发挥其峰值带宽。 数据分类 DNN训练的计算过程中涉及的数据读写类型非常固定，可以进行一定程度的分类。 在活跃计算节点上，主要有训练的输入和训练过程中生成的激活值两种张量数据。训练的输入其生命周期长，持续整个训练过程，这样的数据被称为NV数据流(非易失性数据流)。 相比来说，训练过程中生成的激活值是一种中间结果，其生命周期很短，往往只持续几分钟，这样的数据被称为V数据流(易失性数据流)。同样地，在权值节点上也有这两种数据流。上表分析了活跃计算节点和权值节点上数据的不同类型，我们可以发现： 在DNN训练中所有数据的写都可以用添加式的顺序写完成。 在活跃节点和权重节点上，都各有一个NV数据流和V数据流，两种数据流的数据保留时间相差很大。 由以上分析，NV数据流和V数据流在保存时间、读写权限上有很大的不同，存储在同一段地址空间中并不合适。在存储上，Flash管理系统设计了一种按照流来分层的存储方式，下图展示了激活计算节点的Flash分层管理结构： 这样设计的效果是： 用物理块地址明确的分离了每种数据，每个数据流在单独的存储空间上运行； 每个流有自己的逻辑地址空间、访问权限和基于数据保存时长的P&#x2F;E周期数。 轻量级FTL 通过对数据的分析可知，对Flash存储的写保证是添加式的顺序写。因此，在之前Flash翻译层(FTL)中复杂的垃圾回收和磨损均衡机制将不再必要。在本方案中移去了垃圾回收机制，并用最简单的轮盘赌(Round-robin)算法来实现充分的磨损均衡，极大程度简化了FTL。如图8中，假如共有4个物理块，一轮迭代需要写入3个逻辑块，采用轮盘赌算法在第一轮迭代写入0&#x2F;1&#x2F;2后，第二轮迭代将写入3&#x2F;0&#x2F;1块。 写路径的硬件自动化一般来说，通用的SSD平台实现了读路径的硬件自动化，这样能够最大程度的利用NAND设备的带宽。但是，由于写路径需要去考虑垃圾回收、磨损均衡、数据一致性、冗余数据管理等处理机制，其硬件自动化很难实现。在本平台专用的Flash管理系统中，基于轻量级的FTL，实现写路径的硬件自动化不再是问题。 硬件自动的写路径主要由两条流水线组成： 写命令流水线：将数据从计算节点的张量缓存转移到Flash存储的SRAM中，其带宽可以达到56GB&#x2F;s； NAND设备写入流水线：从Flash存储的SRAM转移到NAND设备中，其带宽可以达到64GB&#x2F;s。 注意到，以上设计没有讨论数据恢复的问题。这是因为在DNN训练中的数据多为临时数据； 并且，一旦数据丢失，可以从检查点重新开始执行。 平台专用Flash管理系统——耐久性问题一般来说，P&#x2F;E操作会对Flash颗粒造成永久性损伤，Flash颗粒所能承受的最大P&#x2F;E操作数量被称为Flash的寿命或耐久性。然而，这种损伤的实际体现为颗粒对于数据的保存时间变短，而当数据保持的时间低于预设值的时候，此颗粒就会被认定为已损坏。 因此，当把数据保存时间的预设时间限制降低时，从某种意义上来说，Flash颗粒仍能够发挥数据保存的作用。许多研究已经证明SSD的耐久性会随着对数据保留时间要求的放松而提高。 平台专用的Flash管理系统将NV数据流和V数据流分离开，在V数据流中，如下图所示，数据保留时间的限制仅仅为分钟级别，假设为5分钟。 图10 张量的寿命 据相关研究表明，当数据保留时间限制为5年时，一般的SSD设备能够承受5万个P&#x2F;E操作； 当把数据保留时间的限制从5年降低到3天的时候，Flash颗粒所能承受的P&#x2F;E次数大概能够提升40倍。 而因此保守估计，数据保留时间降低为3天的情况下，V数据流的颗粒承受P&#x2F;E次数大概为200万次左右，而V数据流的数据量为1.85TB。假定数据带宽为17.6GB&#x2F;s下，Flash管理系统中V数据流颗粒有5年以上的寿命。[这里写放大系数(WAF)为1，如图11所示，此Flash只支持添加写的顺序写操作，而不涉及垃圾回收等机制。]由此论证了存储系统的耐久性问题。 实验结果实验设定验证方法： 将Behemoth在存储上的费用开销与传统的基于TPU的DNN训练系统相对比；(验证Flash替换HBM作为主要存储的作用) 将Behemoth专用Flash管理系统训练吞吐量与传统Flash管理系统相对比。（验证平台专用的Flash管理系统的作用） 验证平台： 采用MAESTRO作为NPU模拟器平台，采用MQ-Sim作为SSD的模拟器平台。 训练模型： 图12 评测使用的训练模型 （PxQ：P代表全连接层扩展的宽度倍数，Q代表网络decode或encode层的深度倍数。例如BERT 2x2模型代表在原始的BERT上，每个全连接层的宽度扩展为原来的两倍，网络层数也加深为原来的两倍） 开销对比开销对比时模拟器的参数设定： 开销对比的结果： 以上训练均在10天之内完成，使用TPU v3系列的系统在存储上的开销比Behemoth系统要贵3.65倍左右。 训练吞吐量对比训练吞吐对比时模拟器的参数设定： 其都采用Behemoth加速器作为计算核心，只有Flash管理上的不同。Baseline采用4个SSD并行达到64个Channel。 对比结果：（下图中的理想情况是指存储访问不花费任何开销） 平台专用的Flash管理系统接近于理想情况； 优化后的吞吐量是传统的SSD其训练吞吐量的2.05倍； 可以看出，SSD中固件造成的性能瓶颈是导致传统SSD性能差的主要因素。 自己的思考Behemoth解决了超大规模DNN模型的高效数据并行训练问题。针对存储墙、带宽浪费等问题，创新性地利用Flash作为主要存储完成了单节点上的训练加速器设计。仿真结果显示系统在存储开销和训练吞吐量上都有不错的效果。","link":"","tags":[{"name":"论文阅读","slug":"论文阅读","permalink":"https://recoderchris.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"name":"2021","slug":"2021","permalink":"https://recoderchris.github.io/tags/2021/"},{"name":"加速器","slug":"加速器","permalink":"https://recoderchris.github.io/tags/%E5%8A%A0%E9%80%9F%E5%99%A8/"},{"name":"DNN","slug":"DNN","permalink":"https://recoderchris.github.io/tags/DNN/"},{"name":"近存储计算","slug":"近存储计算","permalink":"https://recoderchris.github.io/tags/%E8%BF%91%E5%AD%98%E5%82%A8%E8%AE%A1%E7%AE%97/"},{"name":"KAIST","slug":"KAIST","permalink":"https://recoderchris.github.io/tags/KAIST/"},{"name":"FAST","slug":"FAST","permalink":"https://recoderchris.github.io/tags/FAST/"}]},{"title":"(SC21)GPETPU:用EdgeTPU加速通用应用","date":"2021-09-15T13:44:39.000Z","path":"2021/09/15/GPETPU/","text":"GPTPU: Accelerating Applications using Edge Tensor Processing Units(SC 21′) 作者：Kuan-Chieh Hsu and Hung-Wei Tseng @University of California, Riverside 本文介绍了一种在Edge TPU上进行通用计算加速的开源框架。 论文为什么要做这个事情？ 理论可行性：NN加速器理论上是一种张量处理器，因此对于任何以张量为输入、输出的应用都有潜在的加速效果； NN加速器的局限性：现有的商用NN加速器只向用户开放了AI或机器学习专用的调用接口，对通用领域没有相关设计； 框架的必要性：NN加速器几乎没有开放其硬件设计的细节，对于程序员来说，很难仅仅通过调用接口来加速通用的算法。因此，设计一个计算框架成为了提升可编程型的必要工作。 多年之前，GPU只是用来着色和渲染的工具，有CUDA和OpenCL之后，GPU凭借其大规模并行能力，面向了矢量计算领域。本工作和TPU的关系与CUDA与GPU的关系异曲同工。 这个事情之前别人是怎么做的，存在什么问题？① 背景知识 TPU：通过创建对张量执行操作的脉动阵列，来加速机器学习领域中的神经网络任务的领域特定硬件。 Edge TPU：谷歌开发的谷歌云TPU的精简版。相比于谷歌云TPU，它具有公共可得性、且其开放了部分后端C++代码、有更好的每瓦特性能（2TOPS&#x2F;W vs. 0.36TOPS&#x2F;W），因而被本工作选用为加速器。除谷歌之外，在TPU领域仍有非常多的工作，但是有绝大部分的TPU并不生产，用来学术研究；或者其性能和功耗表现较差，没有被本工作选用。然而，这些TPU都以张量为输入和输出，且其接口大同小异，所以本文提出的框架应该具备可移植性。 ② 前人工作*的问题本工作和前人工作有本质上的不同。 以往的NPU只可以加速与之前训练到的神经网络模型相匹配的算法， 而GPETPU可以将张量运算映射到TPU上来加速任何用户定义的算法； NPU只能产生近似于矩阵运算的效果，但GPETPU可以使用Edge TPU实现确切的矩阵操作结果； NPU被神经网络模型大致的结果所限制了精确度，GPETPU可以通过对原始输入的不同部分不断迭代达到所需的精度。 有一些工作关注了ASICS或者稀疏矩阵压缩，在这里并不属于同一范围内的问题。 (*: Neural acceleration for general-purpose approximate programs,2012; Neural acceleration for GPU throughput processors,2015) 该论文解决了什么难点，难点存在的原因是什么，作者是如何发现这些难点的？设计一个在TPU上的通用计算框架有以下的几个难点： 领域专用性：NN加速器为神经网络任务而进行优化，直接将传统的算法映射到加速器上会产生sub-optimal问题； 容错性的考虑：NN加速器往往会牺牲一定的精度换来面积或者功耗，在设计通用计算时需要做一定的调整； 底层不可知性：现存的框架对于神经网络加速器的软硬件接口介绍很少，普通的应用因此而调整参数或者数据格式将造成严重的性能开销； 算法适用性：通用领域优化过的算法对于张量计算不再适用，需要重新设计基于张量的优化算法。 针对以上难点，作者各做了哪些优化，优化背后的思想是什么？分析Edge TPU的特点作者采用了以下的原型机对TPU的特点进行了探索： TPUs: built two quad-EdgeTPU PCIe expansion cards using QNAP QM2-4P-384A, containing 4× M.2 Edge TPUs with M.2 slots connected to a PCIe switch (totally 8x TPUs) HOST: AMD Ryzen 3700X, 4.4GHz Cache: 32MB LLC Main memory: 64GB DDR Storage: NVMe SSD 分析Edge TPU指令文章分析了Edge TPU的一些指令的性能表现。由于像TOPS(tera op per second)、IPS(inference per second)这些常用的衡量标准都是面向神经网络的，所以本文建立了一些新的衡量标准RPS(results per second)。 RPS计算了某条指令产生结果数量的效率，测量得到的结果是： 从上表我们可以得到以下结论： conv2D的RPS非常高，这是因为卷积操作的使用频率高，TPU进行了特别的优化； 不同的指令RPS相差很大； OPS和RPS不是强相关的 分析Edge TPU采用的数据和模型格式Edge TPU 指令的输入一般是两种数据： 待推断的张量数据集， TFLite编译生成的模型数据。 对于通用计算的输入来说，必须要把其中一个张量数据转换为TFLite的模型格式，底层的TPU才能够接收。 而由于直接采用TFLite进行编译的延迟过长，对于除机器学习之外的应用不可忍受。 因此，通过对TFLite改变不同的数据输入、维度以及值，该工作解码了TFlite编译生成的文件。主要包括四个部分： 头部(Header)：每个模型头部都是有120byte的用于识别的模型头，最后4byte描述了数据段的大小。 数据段(Data Section)：这里用行优先的方式存放了二进制编码的8比特整数，超出8比特将使用一个缩放因子进行缩放。 额外数据段(Metadata Section)：存放了数据段的维度和缩放系数。 TFLite编译生成的模型采用小端存储的方式进行数据存放。 GPETPU整体架构 整个GPETPU可以分为如下几个部分： 前端开发了OpenCtpu方便编程，以及与之配套的编译器。 后端为API库和运行时系统，他们的作用是连接用户程序与EdgeTPU接口。 OpenCtpu: 编程接口（前端）作用描述TPU任务，协调异构计算与数据交换。 特点 将应用程序和设备使用的控制放在主机端； 需要程序员显示指定TPU运算的输入输出缓冲区； 提供了让程序员描述计算任务的函数 用法 编写描述计算的核函数：注意，在描述计算时，用invoke函数可以显式地表示这里需要采用TPU进行加速。除了这些显式的操作之外，在和函数中进行矩阵加减乘除等操作的时候也会默认使用TPU进行加速； 准备TPU运算核函数的输入输出缓冲区； 以任务的形式使核函数进队列 ； 适时进行同步：这里在调度时系统的设计中，任务之间是可以并行的，所以需要在恰当的时候进行同步操作来获得结果。 GPETPU的运行时系统 任务调度GPETPU的运行时任务调度策略是一种基于数据流的算法，由前端任务队列OPQ和后端指令队列IQ组成。OPQ的每一项包含任务ID，任务操作符，输入、输出、量化参数等必要的信息。IQ的每一项包含任务ID、TPU操作指令码、输入输出位置等。 OPQ 主机端调用enqueue API时，运行时系统新建一个taskID,为即将进入队列的kernel函数做准备，并且开始执行该kernel函数。 在kernel函数中，若触发了TPU操作，则阻塞当前的kernel函数，用当前taskID创建一个OPQ任务，并在OPQ的参数准备好后进入OPQ队列。 OPQ任务可以并行在Tensorizer上 。 IQTensorizer将OPQ任务转换为TPU指令，将数据转换为TPU可以接受的形式，并将其送至IQ上。若有一些IQ项有相同的输入、量化标志、taskID，但输出位置不同，IQ便将他们调度到同一个TPU上操作，以减少数据转移的开销以及数据格式的转换。其他采用FCFS方式。 Tensorizer作用作用主要是把用户要求的操作转换成对应的TPU指令，在这个过程中需要对数据也进行裁剪和缩放以适应TPU的特征，并且要仿照TFLite对数据进行包装。 将操作转换为TPU指令 将任务数据块划分为TPU指令可以达到最优性能的尺寸大小； Pair-wise操作(add): TPU指令在每个子块上操作后，整合运算结果得到输出； Matrix-wise操作(max): TPU指令在每个子块上操作后，生成CPU代码来集成最终结果。相较于迭代法，减少了数据的转移； 算术操作(conv2D&#x2F;FC): 类似于矩阵乘的分块算法，将运算分为PxQ大小的块，先用TPU指令再生成CPU代码来集成出最后的结果。 将数据转换为适用于TPU的格式Tensorizer将指令的输入数据缩放到定点数范围之内，并将这些数字包装成TPU可接受的模型或矩阵上。 缩放系数的计算： S与kernel函数上的操作顺序、操作总数、input的范围相关。 Tensorizer的开销Tensorizer还需要借助前文解码的TFLite生成的模型格式，仿照TFLite将一个张量包装成模型的形式。 实验测得：基于C的Tensorizer包装的速度比基于python的TFLite快1500倍。此时延可以与对EdgeTPU之间的数据转移相抵消。 为GPETPU优化应用Tensorizer完成了Task级别的优化，但是要完成一个应用，采用什么样的操作符进行计算仍然需要探究。我们以矩阵乘法为例： 用全连接操作符完成矩阵乘法非常简单，因为全连接本质上是一种矩阵-向量乘法，所以很容易实现，但是根据本工作前文对TPU指令的量化，全连接操作的RPS非常低。 除此之外我们还可以使用卷积操作实现矩阵乘法，这时候需要数据进行一定程度的变形。但是他的RPS非常好。 上图是使用全连接和卷积两种操作的效果，对比对象是CPU上使用OpenBLAS库进行计算。在只使用全连接算子时，其速度还不如CPU；但用卷积操作则可以加速超过两倍。 这个实验告诉我们，采用GPETPU进行编程时仍然需要注意算子的选择，可以依据之前测量的RPS进行选择。 为了进一步方便程序员的编程，GPETPU将一些领域内最常用的算法通过优化封装在标准库里，可以直接调用。领域涵盖了图计算、线性代数、物理仿真、模式识别以及金融领域。用户在前端像调用TPU基本操作一样调用这些算子就可以实现这些基本操作。 实验结果实验平台同上。 单核性能比较：TPU vs. CPU Baseline是用单核CPU进行应用实现的方案。可以看到TPU平均可以实现2.46倍的加速；且能耗要比CPU低45%。 GPETPU的扩展性 其次，使用多个TPU进行并行计算也有很好的扩展性。左边这个图的baseline仍然是使用单核CPU的计算速度。可以看到采用8核CPU的平均速度提升大概只在2.7倍左右，但采用8核TPU则可以提升13倍左右。 随着TPU数量的增多，大部分应用的速度几乎呈线性提升。 与GPU的比较 最后这张图展示了GPU与TPU的性能表现对比以及相对能耗。仍然是对比了单核的CPU。 虽然TPU的性能不如GPU，但是能耗非常低，相对于CPU可以平均节省40%的能量。 综合考虑能耗和性能，8x Edge TPU可以超出baseline 46%，强于GPU。 自己的思考借助GPETPU框架，Edge TPU在边缘计算以及嵌入式设备中有较好的应用前景。类似的设计方法在各种加速器设计上都可以参考。","link":"","tags":[{"name":"论文阅读","slug":"论文阅读","permalink":"https://recoderchris.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"name":"2021","slug":"2021","permalink":"https://recoderchris.github.io/tags/2021/"},{"name":"TPU","slug":"TPU","permalink":"https://recoderchris.github.io/tags/TPU/"},{"name":"通用计算","slug":"通用计算","permalink":"https://recoderchris.github.io/tags/%E9%80%9A%E7%94%A8%E8%AE%A1%E7%AE%97/"},{"name":"加速器","slug":"加速器","permalink":"https://recoderchris.github.io/tags/%E5%8A%A0%E9%80%9F%E5%99%A8/"},{"name":"UC(Riverside)","slug":"UC-Riverside","permalink":"https://recoderchris.github.io/tags/UC-Riverside/"},{"name":"SC","slug":"SC","permalink":"https://recoderchris.github.io/tags/SC/"}]},{"title":"图数据管理-课程笔记2","date":"2021-02-04T12:16:19.000Z","path":"2021/02/04/图数据管理笔记2/","text":"Network Properties and Random Graph ModelI. Key Network Properties Problem: How to measure a network? Answer: Using Network Properties. 1. Degree Distribution P(k)P(k) is the probability that a random chosen node has degree k. is number of nodes with degree k. Thus. 2. Path p, Average Distance and DiameterA path p is a sequence of nodes in which each node is linked to the next one. Shortest Path Distance: the number of edges along the shortest path connecting two nodes. Diameter: the maximum distance between any pair of nodes in a graph. That is, Average path length for a connected graph or a SCC graph is defined as: . In practice, we compute the only over the connected pairs of nodes. 3. Cluster Coefficient cA vertex’scluster coefficient c measures how a vertex’s neighbors are connected to each other. Assume as the number of edges between neighbors of vertex , and as the vertex degree. The cluster coefficient can be calculated as Globally, the Average clustering coefficient is . 4. ConnectivityTo show the connectivity of graph, one can calculate the size of the largest connected component in graph, BTW, largest component is also known as giant component. II. Measure Real-world Networks In this part, we use MSN network as an exaple to show some properties of real-world networks. 1. Degree Distribution Power law distribution: , where is a parameter whose value is typically between 2 and 3. The graph degree distribution is heavily skewed. 2. Clustering Coefficient Average Clustering Coefficient of Real Graph can be really big(0.1140) compared to the random graph. 3. Connected Components Nearly all of the vertices are in one largest(giant) connected component. 4. Diameter Average path length is 6.6. Besides, 90% of the nodes can be reached in &lt;8 hops. 5. Small World Effect(Six Degrees of Separation), 1967 A small-world network is a type of mathematical graph where most nodes are not neighbors of one another, but most nodes can be reached from every other by a small number of hops or steps. In mathematical format, assuming L is the distance between two randomly chosen nodes, and N is the number of nodes in a network, then we have . III. Graph Generation ModelThere are four kinds of Graph Generation Model. 1. Random Graph Model(Erdos-Renyi Graph)(1) Generation: : undirected graph on n nodes where each edge (u,v) appears i.i.d with probability p. : undirected graph with n nodes, and m edges picked uniformly at random. (2) Degree Distribution P(k) Binomial Distribution: . . (3) Clustering Coefficient Thus, And , so decreases with the graph size n. Note: the if . (4) Path LengthRandomly pick a node , and it will have: points whose distance is 1 points whose distance is 2 points whose distance is 3 points whose distance is At the same time, the number of vertices is . It means that: . So . In E-R Random Graph, dmax increase slowly with N. (5) Giant Component When p(n-1) &#x3D; 1, the Giant Connected Component emerges. When k&#x3D;ln N, the fully connected graph emerges. (6) Problems Degree distribution differs from that of real-world graph Giant component in most real networks does NOT emerge through a phase transition No local structure – Clustering Coefficient is too low. Conclusion: Real-world network is not random! 2. Small-world Model[Watts-Strogatz ’98] Problem: E-R random graph’s clustering is low! Need: High cluster and low diameter. Start with a low-dimensional regular lattice Has high clustering coefficient Rewire: Introduce randomness Add &#x2F; remove edges to create shortcuts to join remote parts of the lattice. For each edge, with probability p move the other endpoint to a random node. The more probability of rewiring p, the smaller clustering coefficient will be. 3. Barabasi-Albert(BA) Model Problem: How to model the power-law distribution of node degree? Solution: Introduce Growth and rich-get-richer. (1) Assumptions Growth: the graphs grows continuously Preferential attachment(i.e., rich-get-richer): nodes with larger connectivity tend to receive new edges (2) Model Definition Start with a small graph of vertices generated randomly. At each step, add a new vertex with edges connecting to distinct vertices already present in the graph. For each connection, the selection of the existing vertex is governed by the following equation.. (3)Properties The degree distribution of a BA graph follows power law distribution of γ&#x3D;3. The diameter of BA model is . 4. Kronecker Graph Model(1) Recursive Graph Generation Problem: How can we think of network structure recursively? Self-similarity Object is similar to a part of itself: the whole has the same shape as one or more of the parts. (2) Kronecker Graph Generation (3) Stochastic Kronecker Graphs Conclusion: Kronecker is similar to real-world graph.","link":"","tags":[{"name":"图计算","slug":"图计算","permalink":"https://recoderchris.github.io/tags/%E5%9B%BE%E8%AE%A1%E7%AE%97/"},{"name":"基础知识","slug":"基础知识","permalink":"https://recoderchris.github.io/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"},{"name":"课程笔记","slug":"课程笔记","permalink":"https://recoderchris.github.io/tags/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"},{"name":"PKU","slug":"PKU","permalink":"https://recoderchris.github.io/tags/PKU/"}]},{"title":"图数据管理-课程笔记1","date":"2021-02-04T12:08:11.000Z","path":"2021/02/04/图数据管理笔记1/","text":"Introduction to Graph DataI. Concept of Graph1. Origin of Graph problem [Seven Bridges of Knigsberg, 1736] The problem is to devise a walk across each of the seven bridges once and only once to touch every part of the town, or this walk does not exist. Solution: Seven Bridge Problem is to find the Euler Circuit in a graph. It is easier than finding a Hamilton Circuit. 2. Concept of Graph A graph is a set of nodes and edges that connect them. (1) Difference between Network and Graph① Network: Topology structure in a real system. Examples: Web, Social Network and Metabolic Network. Terminology: Network, Node, Link. ② Graph: Mathematical representation of network. Examples: Web Graph, Social Graphs, Knowledge Graph. Terminology: Graph, Vertex, Edge. (2) Application Field Social Network Citation Network Road Network Protein Network Knowledge Graph Internet … (3) Why Graph So Important? Graph is a general data structure to model relationships between different entities Graph provides a universal language to describe complex data Problem: Data availability and computational challenges Graph bridges Big Data and Artificial Intelligence II. Terminology in Graph1. Directed&#x2F;Undirected Graph and Vertex Degree Difference: Whether the edge has its direction. (1) Undirected Graph Properties of Edge: Symmetrical and reciprocal, i.e. (u,v)≡ (v,u) Examples: Wechat graph, Collaboration Graph Degree(v): the number of edges adjacent to vertex v. – – (2)Directed Graph Properties of Edge: Directed Examples: Weibo Following graph, Phone Call Graph Degree(v): Divide into in-degree and out-degree. And , and Source Vertex is the vertex with , and Sink Vertex is the vertex with . 2. Special Graphs(1) Clique(Complete Graph) Clique, a.k.a Complete Graph, is the undirected graph whose vertices are all connected. Number of Edges: In clique, assuming vertices, the total edge number is: . It is also the maximum of number of edge in a graph with vertices. (2) Bipartite Graph Bipartite Graph is a graph whose vertices can be divided into two disjoint sets U and V, and every edge connect between U and V. Examples: Authors-to-Papers, Buyers-to-Products 3. Representing Graphs(1) Adjacency Matrix ① Undirected Graph: Symmetric: The matrix is symmetric, that is, If Non-cyclic: for non-cyclic graph. Degree of vertex i: Total Edge of Graph: ② Directed Graph: Non-symmetric: The matrix is not symmetric, that is, . If Non-cyclic: for non-cyclic graph Degree of vertex *i*: Total Edge of Graph: ③ Disadvantages: Space Complexity: . However, the adjacency matrix is a sparse matrix. It is too much space costing. In graph processing system, adjacency matrix is totally abandoned for its expensive cost for storing. (2)Adjacency List Source Vertex Dst. 1 (Null) 2 3,4 3 2,4 4 5 5 1,2 Advantages: Easier to work when graph is large and sparse. Disadvantages: The time complexity of query a vertex is , which is time-expensive. Besides, point chasing happens when we want to find a path. (3) Compressed Sparse Representation Vertex Offset Edges 1 0 3 2 0 4 3 2 2 4 4 4 5 5 5 1 2 Offset of corresponding Vertex points to the position of vertex’s adjacency edge in Edges. Advantages: It is the most compact form of graph representation. Disadvantages: Leaving no space for dynamic graph processing. (4) Sparsity of Graph Most of graphs in real world are sparse. Conclusion: It is impossible to use adjacency matrix in graph database or processing system. In real graph computing system, we usually use compressed format to store the large-scale graph. However, it brings new challenge of building dynamic graph computing system for programmers and developers. Thus, *dynamic graph processing system* is one of the future direction for architecture researchers. Choice of the proper network representation of a given domain&#x2F;problem determines our ability to use network successfully. In some cases, there is a unique, unambiguous representation In some cases, the representation is by no means of unique The way you assign links will determine the nature of the question you study. 4. Topology Feature of Graph(1) Self-loop and Multigraph Note: In adjacency matrix, if there are 1 elements in diagonal, it is a self-loop graph. If there are , then the graph is a multigraph. (2) Connectivity① Connected Undirected Graph ② Conncted Directed Graph ③ Strong&#x2F;Weak Connected Components Weak Connected Components(WCC): Despite of direction, if the subgraph is a connected graph, then the subgraph can be called as a WCC of original Graph. 5. Some Real Graph Categories","link":"","tags":[{"name":"图计算","slug":"图计算","permalink":"https://recoderchris.github.io/tags/%E5%9B%BE%E8%AE%A1%E7%AE%97/"},{"name":"基础知识","slug":"基础知识","permalink":"https://recoderchris.github.io/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"},{"name":"课程笔记","slug":"课程笔记","permalink":"https://recoderchris.github.io/tags/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"},{"name":"PKU","slug":"PKU","permalink":"https://recoderchris.github.io/tags/PKU/"}]}]