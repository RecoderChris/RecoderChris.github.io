[{"title":"Meta关于SDC的项目基金和已有的研究成果调研","date":"2023-02-06T02:07:35.000Z","path":"2023/02/06/Meta关于SDC的项目基金和已有的研究成果调研/","text":"outline：软错误背景-SDC问题-meta在解决什么样的问题-meta推荐的研究方向-软件方向和硅方向-锁定在软件方向并且详细介绍(ABFT ai MM NN介绍)-AI方法怎么apply到这个上面(想做什么) 背景Meta的服务(facebook, instagram, whatsapp, messengers)都依赖于数据中心的服务。然而，静默数据错误(SDC)无法被大型系统所发现，在硬件层面无法被捕获，有可能会随着软硬件栈一直传播，最终形成应用层面的数据损失。这些损失可能需要数个月来修复，是大型基础计算设施中潜在的问题。 潜在研究方向针对以上前沿研究问题，Meta的系统和基础架构实验室已经开展了大量的工作并运用于云服务场景中以保证服务交付的准确性。经过多年的研究，Meta认为解决SDC问题有以下四个方向： 从**计算架构(Architecture)**层面处理SDC问题 架构层面上处理和转移SDC的方法，例如enhanced compute block ECC机制 自测试(self-test)的架构模块和模式，例如lockstep computing, checkpointing和redundant computing，这些方法都需要评估计算成本和性能代价 新颖的处理计算和内存错误的架构解决方案，包括但不仅限于增强传统的RAS架构。 从**分布式计算(Distributed Computing)**层面处理SDC的传播 遏制SDC传播的分布式计算弹性模型和解决方案 跨越多个子系统的错误检测能力 分布式规模的错误控制和测试机制 自测试SDC的分布式系统架构和恢复方案 从**软件(Software)**层面提供弹性服务 软件层面实现对SDC的弹性，例如冗余机制、概率和算法的容错机制 实现抗SDC的通用计算和数据搬运库 针对SDC的实时检测和遏制软件方法(需要评估计算成本和性能开销) 从过去的SDC中归纳SDC解决算法 从**硅设计(Silicon Design)**层面提供更可靠的硬件工艺 面向SDC问题，优化硅设计和制造策略 用于硅制造过程的进阶模拟、仿真和测试策略 硅测试的覆盖率评估、硅模块内发生故障的概率评估模型 用于SDC检测的生产过程中的测试例程开发 硅模块使用过程中逐渐衰退的模型和评估机制 Facebook在大规模集群中对SDC问题的理解以下内容来自论文《Silent Data Corruptions at Scale》(Arxiv) @Facebook。 补充背景以往的工作往往研究由于辐射或者合成错误注入产生的软错误问题，Meta的研究则注意到，由于设备的特性，SDC可能会发生并且大规模重复，这种SDC是可复现、非瞬态的。此外，在之前的错误注入模型的研究中，CPU的SDC概率被定为到百万分之一的几率级别，而在真实场景下，由于CPU功能模块中为了性能往往采用最小纠错机制，CPU SDC的概率要比预计高几个数量级。 产生缺陷的原因设备缺陷(Device Errors)在生产和设计阶段，设备本身有可能会有潜在的缺陷： 有些设计有可能有corner case。例如，在某个特定的功耗状态下管理cache控制器的模块，其功能会受到限制，有可能导致设备运行被卡住或者产生功能上发生错误； 在CPU布局布线期间，不能确定信号的到达时间，有可能导致错误的位翻转。例如timing path error 制造过程中，有可能所有的晶体管的蚀刻不够可靠，使得晶体管没有相同的峰值工作电压或功率阈值，各个设备的模块特点不同，导致制造上的错误。 早期故障(Early Life Failures)有些故障在生产过程中就被发现了，有些故障要等到硬件真正运行服务的过程中才会出现。根据晶体管内部缺陷的类型，故障可能会在运行前几周、几个月或预期设备寿命结束前的任何时间出现。 这些故障全部被归类为早期故障(Early Life Failures)。 设备老化(Degradation)随着频繁的使用，设备会逐渐老化。频繁使用的计算部分比 CPU 的其他部分老化的更快。与早期故障相比，这些由于设备老化而产生的故障并不常见，但仍有相关的例子，例如针对 DDR4 内存的RowHammer攻击。 芯片内部使用纠错机制(ECC)，可以防止设备内部性能下降。 晚期磨损(End-of-Life Wear-Out)当设备在现场服务工作负载一段时间后，超出其额定寿命，整个硅开始出现磨损，这在大多数组件中都可以观察到。 Case Study：应用层看SDC造成的影响Facebook中有大量不同类型的应用。我们以查询设施为例，最基本的查询基础设施是用来获取并执行SQL查询的设施，例如Spark, Presto, Hive等。下面以spark为例来描述SDC会给应用带来什么样的影响。 Spark 在Spark架构中，数据集被分为多个部分，这种数据集叫做弹性分布式数据集(Resilient Distributed Dataset, RDD)，每个数据集是单独并行执行的。执行过程是每个worker node上会分布有几个RDD块，每个worker首先做map，然后在shuffle reduce阶段将结果进行综合，最后再对用户提供请求的结果。 FaceBook解压缩应用FaceBook使用Spark进行压缩。这里，我们主要讨论解压缩的部分。当解压缩请求运行时，多组压缩文件会被输入到解压缩流水线中。在解压缩之前，系统会检查压缩文件的大小看是否大于0。如果结果大于0，才会执行解压缩。 在Spark中，解压缩流水线会提供文件的尺寸作为解压缩算法的输入，算法将执行Scala库的幂函数。有趣的是，FB在日常维护中发现了这样一个case study：幂函数会对一个已知具有非零大小的压缩文件返回0值。由于文件大小为0，因此这个文件不会被解压，这意味着随机丢失了一些文件。接下来，为压缩文件保留着k-v对的应用会发现这个错误，并且注意到此错误已经不可还原了，使得应用程序出错。 对以上问题做debug所有工程团队用日志记录下每一个worker的每一步的结果，并复现以上的错误→从日志中锁定出错机器→单机上复现错误→锁定错误只在某一核心上被某一组特定值触发→scala无法使用GDB调试，但可以用JVM兼容JAVA字节码 工具为了找出根本原因，Meta团队希望在不改变复现实验的含义的情况下，将编程语言从Scala转换为Java，再转换为JAVA字节码，最终到达指令级别来再通过GDB调试，以锁定错误出现的范围。但是java是即时编译的，并不能深入到指令级别，所以还需要一个提前的编译器编译出这些指令；或者需要一个探测器，可以在执行JAVA字节码时提供已经执行的二进制指令。 语言转换：Scala Compiler(scalac)，转换后可以生成交叉编译的java类型文件，可以当做java字节码 提前(ahead-of-time)编译器：GCJ，可以将java字节码转换成对象文件和二进制码，二进制码可以用GDB来debug。然而这个工具已经很久没有维护了，其他也没有更好的编译器工具。 **探测器(probe)**：JAVA提供+printassembly选项，用HotSpot追踪可以打印出已经运行出的汇编代码。 通过以上三个工具实现了GDB对代码进行debug。但是出来的都是汇编代码，且数量非常大(430K)，为了定位到错误的汇编代码，本文通过先将相关的函数筛选出来，再进行反向工程，实现对错误的定位。 很好的一些调试经验(略过了，太工程，大部分谷歌翻译了) 绝对地址引用：将绝对地址留在代码中跳转会导致分段错误。如果发现汇编的该部分对再现性没有依赖性，则最好消除绝对地址引用。 意外的分支：如果意外的分支和跳转调用没有被映射，代码会因分段错误而崩溃。最好限制复现器的可变性。 外部库引用：建议最好不要依赖外部库。 编译器优化：高性能代码具有多次编译器优化功能，观察数学方程式的优化有助于理解复制器所需的关键组件。 在逐步执行汇编指令时，优化可能不直观。 冗余指令：最好消除冗余指令，例如stub指令 输入&#x2F;输出寄存器：我们需要为关键指令识别数据输入和结果寄存器。识别后，必须添加额外的指令来提供用户输入、获得结果。 这实现了稳定的reproducer代码，并能够识别SDC的数据依赖性。 管理堆栈帧：独立的Reproducer需要适当地管理堆栈帧。 管理堆栈帧中的事务以防止缓冲区溢出或下溢对于稳定性至关重要。 没有堆栈框架，复制器代码无法管理基于堆栈的请求或函数调用。 内存偏移量引用：寄存器通常在指令中使用内存偏移量。 必须适当地初始化偏移量。 如果未计算和初始化偏移量，我们将遇到由于未初始化数据而导致的分段错误或复制器损坏。 特殊功能单元：需要监控特殊功能单元（如 ALU、DSP、FPU、AVX 等）的事务，它们会带来近似值。 此外，特殊功能单元利用不同的位宽、特殊功能寄存器和堆栈结构。 主框架：如果没有合适的主框架和功能框架，一个独立的reproducer是不完整的。 这使得代码可执行。 通过逆向工程，可以获得一个更简单的reproducer代码，之后通过GDB可以获得导致错误的指令，最终锁定在60行的汇编代码中。 重新审视应用的错误此后，本文注意到具有不同精度的不正确值，结果不尽相同。因此，应用程序可能解压缩了大小不正确的文件，并且将文件在没有在EoF终止符的时候下被错误截断。 这会导致文件节点悬空、数据丢失等无法跟踪的错误。 因为复杂的数据依赖以及数据的输入，如果无法没有复现代码，这种数据损坏几乎不可能被检测并溯源，尤其是在集群拥有数十万台机器、每秒执行几百万次计算的情况下。因此，以上的方法可以更快地溯源集群内SDC的根本原因。 对抗SDC的硬件方法我们观察到，在大规模基础设施中，SDC并不是局限于百万分之一概率的罕见事件。 这些错误是系统性的，不像其他故障模式那样容易理解。以下几种硬件方法可以降低处理器内的软错误率，对SDC也有效： 保护数据路径：使用ECC增强设备内的块，保护数据路径，提高设备的弹性。 专用的筛选模式：在制造流程中，设计专用的筛选和测试模式 了解大规模下SDC的行为：与大规模使用设备的客户密切合作，了解和评估SDC的影响。 研究发生率、生产故障时间、对频率、电压和环境条件的依赖性，有助于深入了解 SDC 的表现形式。 架构优先级：将来架构选择中，会优先考虑防止SDC的架构。 探测SDC的方法(不增加其他额外开销)为了探测到SDC，我们需要额外的机器来进行计算，之后与参考值比较结果。以下三种方法可以实现对SDC的探测： 找机会找机会利用处于维护状态的机器，并使用随机数据输入，执行指令级准确性验证。 这里的挑战在于其覆盖范围主要取决于机器有机会处于维护状态的频率。 在大型的集群中，我们并不希望有很大比例的机器处于这些状态。 周期性实施一个调度程序，定期监视机器的SDC覆盖率，然后根据定期计时器安排机器进行测试。这种方案的的开销很高。 对生产友好的方案当测试可以优化为最小的大小和运行时间时，可以使测试指令与机器上的工作负载同时执行。 结果被发送到收集器以通知机器的通过或失败状态。 此方法需要与工作负载密切配合，以免对生产工作负载产生任何不利影响。 软件容错机制为了处理静默错误，我们需要重新考虑基础架构软件设计理念和软件抽象的鲁棒性。 冗余机制防止应用程序级故障的更好方法是实施软件级冗余，并定期验证正在计算的数据在多个检查点是否准确。在将这些方法应用于大规模数据中心时，要考虑精确计算的成本。 冗余的代价对资源有着直接的影响：架构越冗余，重复资源池的需求就越大。 但是冗余为应用程序提供了容错的概率。 容错库将容错机制添加到 PyTorch 等知名开源库中将极大地帮助应用程序防止暴露于SDC。 构建容错的算法会增加应用程序的额外开销。因此，如果在性能下降可忽略不计的情况下，这一点可以被实现。 这项工作需要SDC研究社区和软件库社区之间的密切合作。 Facebook检测SDC问题的方法以下内容来自论文《Detecting silent data corruptions in the wild》(Arxiv) @Facebook。 硅测试流程在投入使用之前，基于硅的电子设备要经历不同的开发阶段。因此，我们要了解不同开发阶段所使用的测试策略，以理解集群范围内的与测试有关的开销，由此了解为什么测试是一件很难的事情。测试流程看重三点：测试量、测试时间、在该阶段出错产生的影响。 设计和验证(Design and Verification)采用模拟和仿真(Simulation and Emulation)的手段来进行方案的测试。测试的时间会很长，但是由于方案不断改变、新的器件会加入，所以测试周期一般会很短。在该阶段出错的代价相对来说是很低的 硅上验证(Post Silicon Validation)此阶段产生少量的样本进行验证。与之前一个阶段相比，设计加入了生产过程的变量。从开销上来说，这一阶段的验证会引入生产的开销。如果出错，需要对原始方案推倒重新设计，此外，在真实电路上开展测试的测试开销也很大。通过测试的方案被视为可以进行大规模生产。 生产厂商测试(Manufacturer testing)大规模生产之后，每个设备将用更加高级的测试设备进行自动测试。测试时间对生产通量有明显的影响，测试总量也从之前的几百个上升到上百万的量级。测试的开销随着数量也线性扩展。在这个阶段如果出错，开销会更大，往往导致重新设计或者重新生产该芯片。 集成测试这一阶段设备被运送到终端消费者手中，消费者用此设备集成到自己的开发环境中，集成阶段往往由集成者来协调。在这一阶段，需要跟其他不同的设备相配合，测试的复杂度也升高了；此外，测试的开销也从单设备到多种设备、多种配置的综合测试。此阶段出错，可能会导致不同的机架重新组装或者重新安装。 基础设施引入测试这一阶段主要是将机器接入网络，进行应用层面的测试。往往要持续几个小时到一天的时间。由于故障的来源多，所以锁定错误要更难一些。 基础设施集群测试正常的测试流程到上面就结束了。但是因为有SDC，如果不运行某个特定的测试程序是找不到SDC问题，以保护基础架构上运行的应用的。因此，按期进行检查是很有必要的。这种测试所需要花费的开销会更大，因为需要在保证程序正常运行的情况下对程序进行复杂的编排和调度。 此外，由于硬件的复杂度和配置的复杂度，分类和溯源错误是很昂贵的。因此，需要花费比较昂贵的开销，采用更多高级的方法来探测SDC。 SDC为什么是一个很难的问题？SDC可能的来源： 数据相关：SDC有可能天然与数据相关，也就是说这个CPU坏了，有可能大部分的计算还是正确的，但是就是一小部分是错误的。这使得测试的空间非常大。 电气因素：改变的电压、频率和电流会引起更多的错误，虽然在某一组电气配置下结果对了，但是其他情况不保证。这也使得测试空间非常大。 环境因素：地理位置的差异也会加速SDC问题的出现，这与温度、湿度都有很强的相关性。在大型集群中，也有可能因为workload不均匀一些机器温度高，使得某些计算中心结果和其他不一样。 寿命因素：随着时间，硅片的性能和可靠性也会改变。此外，SDC问题可能跟现有模型的预测有出入。因此今天计算正确，明天就不一定正确了。 从以上四点，本文总结保护集群以对抗SDC的唯一方法是通过不断改进的测试例程和高级测试模式生成来反复测试基础设施。 架构测试效果对比","link":"","tags":[{"name":"论文阅读","slug":"论文阅读","permalink":"https://recoderchris.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"name":"容错","slug":"容错","permalink":"https://recoderchris.github.io/tags/%E5%AE%B9%E9%94%99/"},{"name":"SDC","slug":"SDC","permalink":"https://recoderchris.github.io/tags/SDC/"},{"name":"软错误","slug":"软错误","permalink":"https://recoderchris.github.io/tags/%E8%BD%AF%E9%94%99%E8%AF%AF/"}]},{"title":"(FAST21)Behemoth:面向超大规模DNN、以闪存为中心存储的训练加速器","date":"2023-02-04T14:22:33.000Z","path":"2023/02/04/FAST21-Behemoth/","text":"Behemoth: A Flash-centric Training Accelerator for Extreme-scale DNNs 作者：Shine Kim, Yunho Jin, Gina Sohn, Jonghyun Bae, Tae Jun Ham and Jae W. Lee @ Seoul National University, Samsung Electronics 一句话描述该论文解决了什么事情？本文针对以自然语言处理领域模型为代表的超大规模深度神经网络(DNN)模型的训练问题，构建了一种以闪存为中心存储的训练加速器。 论文为什么要做这个事情？DNN在自然语言处理、计算机视觉、推荐算法等应用领域有广泛的应用。而增加模型参数的数量对于提升模型的准确度起到至关重要的作用。自然处理领域中各种基于Transformer的模型，其网络中的参数数量达到了超大规模(Extreme-scale)的级别。例如，GPT-3模型的参数数量达到了1,750亿以上。不仅如此，DNN模型的参数规模仍然在继续扩展，如如图1所示： 模型参数数量的扩展对存储系统提出了以下的两个挑战： 内存容量墙(Memory Capacity Wall) 以GPT-3为例，其模型参数大小为700GB。而一个NVIDIA A100的内存容量大概在40GB左右。由此我们可以看出以下几点： DNN模型的大小远远超过了一个GPU计算设备的内存容量； 用户将不得不采用模型并行的方式，将模型分布在多个GPU上完成训练。但是完成模型并行，需要细致地设计负载均衡机制。 GPU&#x2F;TPU上配备有高带宽存储系统(HBM)，其价格相当昂贵。 存储带宽的浪费(Memory B&#x2F;W Underutilization) 随着DNN模型的尺寸扩大，Transformer中各种全连接层矩阵的重用也会变得更加频繁； 因此，随着训练计算总量的不断扩张，实际存储的带宽需求并未大幅度提高；而GPU上的高带宽存储系统也随着计算量而不断增加，造成了明显的带宽浪费问题。 本文希望能够将超大规模DNN训练的中心存储由低容量、高花费的HBM替换为高容量、低开销的闪存块，并辅助加速器设计完成单SSD节点上的训练任务。 这个事情之前别人是怎么做的，存在什么问题？对于超大规模DNN模型训练而带来的内存容量墙问题，有以下两种解决方案： 针对张量管理的异构存储系统 由于用多个设备进行分布式训练对训练设备的内存带宽造成了极大的浪费，因此有一些方法考虑建立一种在异构内存系统中进行张量数据迁移的高效存储管理系统*。但是，超大规模的模型甚至在主机的存储器中也放不下，因此并不是解决超大规模模型的训练问题的长久之计。 参考文献: Hotness- and lifetime-aware data placement and migration for high-performance deep learning on heterogeneous memory systems, [TC 2020] vDNN: Virtualized deep neural networks for scalable, memory-efficient neural network design,[MICRO 2016] *SuperNeurons: Dynamic GPU memory management for training deep neural networks.[PPoPP 2018] 以二级存储为中心的机器学习系统 BLAS-on-flash：构建一个库，用于针对大型数据集的机器学习算法（例如 ISLE、XML）实现高效的闪存速度。 Cognitive SSD： 构建一个引擎，用于检索存储在闪存上的 DNN 模型推理所用的非结构化数据。 本工作的主要着眼点在于用闪存实现数千亿参数的超大规模神经网络语言模型的训练。 *: BLAS-on-flash: An efficient alternative for large-scale ML training and inference? [NSDI, 2019] Cognitive SSD: A deep learning engine for in-storage data retrieval**[USENIX ATC, 2019]** 该论文解决了什么难点，难点存在的原因是什么，作者是如何发现这些难点的？本论文主要解决了以下难点： SSD平台上模型的训练问题：需要设计一个加速器来完成模型的数据并行计算、控制模块间的通信以及充分利用带宽； Flash的低带宽问题：SSD有极低的带宽，特别是非顺序访问。除此之外，由于SSD底层有垃圾回收(Garbage Collection)、磨损平衡(Wear Leveling)等机制，其持续写带宽也明显比峰值带宽低很多。 Flash的耐久性(endurance)问题：SSD只能进行一定数量的P&#x2F;E操作，这被定义为SSD的耐久性或寿命。用SSD作为DNN训练的存储器明显将缩短其寿命，特别是在随机写时，有写放大(Write Amplification)的现象。 针对以上难点，作者各做了哪些优化，优化背后的思想是什么？Behemoth计算平台硬件模块Behemoth系统设计的整体目的是在单节点上完全容纳超大规模的DNN模型，实现数据并行训练。Behemoth计算平台的整体结构图如下所示： 由于执行过程的底层逻辑是一致的，所以计算加速器的硬件部分可以分为三个部分：计算核心、张量缓存以及平台专用的Flash存储系统。 计算核心(Compute Core)：计算张量；控制数据转移；指令翻译。 张量缓存(Tensor Buffer): 计算核心与NAND Flash之间的一块DDR DRAM区域，将张量暂时保存。 平台专用的Flash存储系统(Flash Memory System)：代替HBM成为训练过程中的主要存储，用来存储张量信息。相关优化在后面会提及。 整体上来说，Behemoth系统采用了DDR DRAM+NAND Flash的双层存储结构，这一点是与以往训练方案中采用HBM作为单层存储结构本质上不同的。 面向DNN训练的硬件模块组装利用基本硬件模块，本加速器组装出多个激活计算节点和单个权值计算节点，以实现DNN模型训练的数据并行。如图4所示： 权值计算节点(Weight Node)：更新并存储权值； 激活计算节点(Activation Node): 计算并存储激活值，激活值还将用于反向传播；多个激活计算节点实现了DNN模型训练的数据并行。 主机端(Host)：负责将训练命令序列(包含计算以及DMA命令)发送到加速器上，在训练结束之后，加速器将训练结果返回给主机端。 训练步骤DNN模型的训练是一个重复的正向&#x2F;反向传播的过程。图5(a)展示了单层训练的步骤，其正向传播包含以下几步： 从权重节点闪存中读取权重值，到权重节点的张量缓存中； 将权重节点张量缓存中的权值通过广播发送到激活节点的张量缓存中； 权重张量被加载到片上的SRAM中，由计算核心准备开始计算； 激活节点的计算核心完成计算，生成激活张量； 激活张量从片上的SRAM中被拷贝到激活节点的张量缓存中，准备写入激活节点的闪存块上； 张量缓存中的激活张量写入闪存块中，供反向传播使用；权值张量从张量缓存中被释放。 反向传播与正向传播的步骤类似。当一个迭代结束后，图5(b)展示了权值更新的过程： 最终权值的梯度下降张量从激活节点的张量缓存被送至权重节点的张量缓存； 收到所有权值的下降梯度后，权值节点将权值下降梯度加载至片上的SRAM； 由计算核心来更新权值； 更新后的权值张量被写入张量缓存； 将更新后的权值由张量缓存写入NAND Flash中以供下一轮迭代使用。 平台专用Flash管理系统——带宽问题相比于高带宽存储，闪存存在带宽低的问题。根据之前的分析，GPT-3模型训练要求的带宽大概在50GB&#x2F;s左右，而NAND Flash大概只能提供几个GB&#x2F;s的带宽，无法满足训练的需求。 事实上，NAND设备的带宽峰值并没有得到很好的发挥。为了优化NAND设备的带宽值，可能的方向有以下两个： 尽可能的顺序化对NAND设备的写操作：随机写会造成大量的带宽浪费以及写放大现象； 防止NAND设备固件成为性能的瓶颈：固件中的FTL一般都有如垃圾回收、磨损均衡、备份数据管理等机制，这些机制都会影响NAND设备发挥其峰值带宽。 数据分类 DNN训练的计算过程中涉及的数据读写类型非常固定，可以进行一定程度的分类。 在活跃计算节点上，主要有训练的输入和训练过程中生成的激活值两种张量数据。训练的输入其生命周期长，持续整个训练过程，这样的数据被称为NV数据流(非易失性数据流)。 相比来说，训练过程中生成的激活值是一种中间结果，其生命周期很短，往往只持续几分钟，这样的数据被称为V数据流(易失性数据流)。同样地，在权值节点上也有这两种数据流。上表分析了活跃计算节点和权值节点上数据的不同类型，我们可以发现： 在DNN训练中所有数据的写都可以用添加式的顺序写完成。 在活跃节点和权重节点上，都各有一个NV数据流和V数据流，两种数据流的数据保留时间相差很大。 由以上分析，NV数据流和V数据流在保存时间、读写权限上有很大的不同，存储在同一段地址空间中并不合适。在存储上，Flash管理系统设计了一种按照流来分层的存储方式，下图展示了激活计算节点的Flash分层管理结构： 这样设计的效果是： 用物理块地址明确的分离了每种数据，每个数据流在单独的存储空间上运行； 每个流有自己的逻辑地址空间、访问权限和基于数据保存时长的P&#x2F;E周期数。 轻量级FTL 通过对数据的分析可知，对Flash存储的写保证是添加式的顺序写。因此，在之前Flash翻译层(FTL)中复杂的垃圾回收和磨损均衡机制将不再必要。在本方案中移去了垃圾回收机制，并用最简单的轮盘赌(Round-robin)算法来实现充分的磨损均衡，极大程度简化了FTL。如图8中，假如共有4个物理块，一轮迭代需要写入3个逻辑块，采用轮盘赌算法在第一轮迭代写入0&#x2F;1&#x2F;2后，第二轮迭代将写入3&#x2F;0&#x2F;1块。 写路径的硬件自动化一般来说，通用的SSD平台实现了读路径的硬件自动化，这样能够最大程度的利用NAND设备的带宽。但是，由于写路径需要去考虑垃圾回收、磨损均衡、数据一致性、冗余数据管理等处理机制，其硬件自动化很难实现。在本平台专用的Flash管理系统中，基于轻量级的FTL，实现写路径的硬件自动化不再是问题。 硬件自动的写路径主要由两条流水线组成： 写命令流水线：将数据从计算节点的张量缓存转移到Flash存储的SRAM中，其带宽可以达到56GB&#x2F;s； NAND设备写入流水线：从Flash存储的SRAM转移到NAND设备中，其带宽可以达到64GB&#x2F;s。 注意到，以上设计没有讨论数据恢复的问题。这是因为在DNN训练中的数据多为临时数据； 并且，一旦数据丢失，可以从检查点重新开始执行。 平台专用Flash管理系统——耐久性问题一般来说，P&#x2F;E操作会对Flash颗粒造成永久性损伤，Flash颗粒所能承受的最大P&#x2F;E操作数量被称为Flash的寿命或耐久性。然而，这种损伤的实际体现为颗粒对于数据的保存时间变短，而当数据保持的时间低于预设值的时候，此颗粒就会被认定为已损坏。 因此，当把数据保存时间的预设时间限制降低时，从某种意义上来说，Flash颗粒仍能够发挥数据保存的作用。许多研究已经证明SSD的耐久性会随着对数据保留时间要求的放松而提高。 平台专用的Flash管理系统将NV数据流和V数据流分离开，在V数据流中，如下图所示，数据保留时间的限制仅仅为分钟级别，假设为5分钟。 图10 张量的寿命 据相关研究表明，当数据保留时间限制为5年时，一般的SSD设备能够承受5万个P&#x2F;E操作； 当把数据保留时间的限制从5年降低到3天的时候，Flash颗粒所能承受的P&#x2F;E次数大概能够提升40倍。 而因此保守估计，数据保留时间降低为3天的情况下，V数据流的颗粒承受P&#x2F;E次数大概为200万次左右，而V数据流的数据量为1.85TB。假定数据带宽为17.6GB&#x2F;s下，Flash管理系统中V数据流颗粒有5年以上的寿命。[这里写放大系数(WAF)为1，如图11所示，此Flash只支持添加写的顺序写操作，而不涉及垃圾回收等机制。]由此论证了存储系统的耐久性问题。 实验结果实验设定验证方法： 将Behemoth在存储上的费用开销与传统的基于TPU的DNN训练系统相对比；(验证Flash替换HBM作为主要存储的作用) 将Behemoth专用Flash管理系统训练吞吐量与传统Flash管理系统相对比。（验证平台专用的Flash管理系统的作用） 验证平台： 采用MAESTRO作为NPU模拟器平台，采用MQ-Sim作为SSD的模拟器平台。 训练模型： 图12 评测使用的训练模型 （PxQ：P代表全连接层扩展的宽度倍数，Q代表网络decode或encode层的深度倍数。例如BERT 2x2模型代表在原始的BERT上，每个全连接层的宽度扩展为原来的两倍，网络层数也加深为原来的两倍） 开销对比开销对比时模拟器的参数设定： 开销对比的结果： 以上训练均在10天之内完成，使用TPU v3系列的系统在存储上的开销比Behemoth系统要贵3.65倍左右。 训练吞吐量对比训练吞吐对比时模拟器的参数设定： 其都采用Behemoth加速器作为计算核心，只有Flash管理上的不同。Baseline采用4个SSD并行达到64个Channel。 对比结果：（下图中的理想情况是指存储访问不花费任何开销） 平台专用的Flash管理系统接近于理想情况； 优化后的吞吐量是传统的SSD其训练吞吐量的2.05倍； 可以看出，SSD中固件造成的性能瓶颈是导致传统SSD性能差的主要因素。 自己的思考Behemoth解决了超大规模DNN模型的高效数据并行训练问题。针对存储墙、带宽浪费等问题，创新性地利用Flash作为主要存储完成了单节点上的训练加速器设计。仿真结果显示系统在存储开销和训练吞吐量上都有不错的效果。","link":"","tags":[{"name":"DNN","slug":"DNN","permalink":"https://recoderchris.github.io/tags/DNN/"},{"name":"近存储计算","slug":"近存储计算","permalink":"https://recoderchris.github.io/tags/%E8%BF%91%E5%AD%98%E5%82%A8%E8%AE%A1%E7%AE%97/"},{"name":"加速器","slug":"加速器","permalink":"https://recoderchris.github.io/tags/%E5%8A%A0%E9%80%9F%E5%99%A8/"},{"name":"论文阅读","slug":"论文阅读","permalink":"https://recoderchris.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"name":"KAIST","slug":"KAIST","permalink":"https://recoderchris.github.io/tags/KAIST/"}]},{"title":"(SC21)GPETPU:用EdgeTPU加速通用应用","date":"2023-02-04T13:44:39.000Z","path":"2023/02/04/GPETPU/","text":"GPTPU: Accelerating Applications using Edge Tensor Processing Units(SC 21′) 作者：Kuan-Chieh Hsu and Hung-Wei Tseng @University of California, Riverside 1. 一句话描述该论文解决了什么事情？本文介绍了一种在Edge TPU上进行通用计算加速的开源框架。 2. 论文为什么要做这个事情？ 理论可行性：NN加速器理论上是一种张量处理器，因此对于任何以张量为输入、输出的应用都有潜在的加速效果； NN加速器的局限性：现有的商用NN加速器只向用户开放了AI或机器学习专用的调用接口，对通用领域没有相关设计； 框架的必要性：NN加速器几乎没有开放其硬件设计的细节，对于程序员来说，很难仅仅通过调用接口来加速通用的算法。因此，设计一个计算框架成为了提升可编程型的必要工作。 多年之前，GPU只是用来着色和渲染的工具，有CUDA和OpenCL之后，GPU凭借其大规模并行能力，面向了矢量计算领域。本工作和TPU的关系与CUDA与GPU的关系异曲同工。 3. 这个事情之前别人是怎么做的，存在什么问题？① 背景知识 TPU：通过创建对张量执行操作的脉动阵列，来加速机器学习领域中的神经网络任务的领域特定硬件。 Edge TPU：谷歌开发的谷歌云TPU的精简版。相比于谷歌云TPU，它具有公共可得性、且其开放了部分后端C++代码、有更好的每瓦特性能（2TOPS&#x2F;W vs. 0.36TOPS&#x2F;W），因而被本工作选用为加速器。除谷歌之外，在TPU领域仍有非常多的工作，但是有绝大部分的TPU并不生产，用来学术研究；或者其性能和功耗表现较差，没有被本工作选用。然而，这些TPU都以张量为输入和输出，且其接口大同小异，所以本文提出的框架应该具备可移植性。 ② 前人工作*的问题本工作和前人工作有本质上的不同。 以往的NPU只可以加速与之前训练到的神经网络模型相匹配的算法， 而GPETPU可以将张量运算映射到TPU上来加速任何用户定义的算法； NPU只能产生近似于矩阵运算的效果，但GPETPU可以使用Edge TPU实现确切的矩阵操作结果； NPU被神经网络模型大致的结果所限制了精确度，GPETPU可以通过对原始输入的不同部分不断迭代达到所需的精度。 有一些工作关注了ASICS或者稀疏矩阵压缩，在这里并不属于同一范围内的问题。 (*: Neural acceleration for general-purpose approximate programs,2012; Neural acceleration for GPU throughput processors,2015) 4. 该论文解决了什么难点，难点存在的原因是什么，作者是如何发现这些难点的？设计一个在TPU上的通用计算框架有以下的几个难点： 领域专用性：NN加速器为神经网络任务而进行优化，直接将传统的算法映射到加速器上会产生sub-optimal问题； 容错性的考虑：NN加速器往往会牺牲一定的精度换来面积或者功耗，在设计通用计算时需要做一定的调整； 底层不可知性：现存的框架对于神经网络加速器的软硬件接口介绍很少，普通的应用因此而调整参数或者数据格式将造成严重的性能开销； 算法适用性：通用领域优化过的算法对于张量计算不再适用，需要重新设计基于张量的优化算法。 5. 针对以上难点，作者各做了哪些优化，优化背后的思想是什么？分析Edge TPU的特点作者采用了以下的原型机对TPU的特点进行了探索： TPUs: built two quad-EdgeTPU PCIe expansion cards using QNAP QM2-4P-384A, containing 4× M.2 Edge TPUs with M.2 slots connected to a PCIe switch (totally 8x TPUs) HOST: AMD Ryzen 3700X, 4.4GHz Cache: 32MB LLC Main memory: 64GB DDR Storage: NVMe SSD 分析Edge TPU指令文章分析了Edge TPU的一些指令的性能表现。由于像TOPS(tera op per second)、IPS(inference per second)这些常用的衡量标准都是面向神经网络的，所以本文建立了一些新的衡量标准RPS(results per second)。 RPS计算了某条指令产生结果数量的效率，测量得到的结果是： 从上表我们可以得到以下结论： conv2D的RPS非常高，这是因为卷积操作的使用频率高，TPU进行了特别的优化； 不同的指令RPS相差很大； OPS和RPS不是强相关的 分析Edge TPU采用的数据和模型格式Edge TPU 指令的输入一般是两种数据： 待推断的张量数据集， TFLite编译生成的模型数据。 对于通用计算的输入来说，必须要把其中一个张量数据转换为TFLite的模型格式，底层的TPU才能够接收。 而由于直接采用TFLite进行编译的延迟过长，对于除机器学习之外的应用不可忍受。 因此，通过对TFLite改变不同的数据输入、维度以及值，该工作解码了TFlite编译生成的文件。主要包括四个部分： 头部(Header)：每个模型头部都是有120byte的用于识别的模型头，最后4byte描述了数据段的大小。 数据段(Data Section)：这里用行优先的方式存放了二进制编码的8比特整数，超出8比特将使用一个缩放因子进行缩放。 额外数据段(Metadata Section)：存放了数据段的维度和缩放系数。 TFLite编译生成的模型采用小端存储的方式进行数据存放。 GPETPU整体架构 整个GPETPU可以分为如下几个部分： 前端开发了OpenCtpu方便编程，以及与之配套的编译器。 后端为API库和运行时系统，他们的作用是连接用户程序与EdgeTPU接口。 OpenCtpu: 编程接口（前端）作用描述TPU任务，协调异构计算与数据交换。 特点 将应用程序和设备使用的控制放在主机端； 需要程序员显示指定TPU运算的输入输出缓冲区； 提供了让程序员描述计算任务的函数 用法 编写描述计算的核函数：注意，在描述计算时，用invoke函数可以显式地表示这里需要采用TPU进行加速。除了这些显式的操作之外，在和函数中进行矩阵加减乘除等操作的时候也会默认使用TPU进行加速； 准备TPU运算核函数的输入输出缓冲区； 以任务的形式使核函数进队列 ； 适时进行同步：这里在调度时系统的设计中，任务之间是可以并行的，所以需要在恰当的时候进行同步操作来获得结果。 GPETPU的运行时系统 任务调度GPETPU的运行时任务调度策略是一种基于数据流的算法，由前端任务队列OPQ和后端指令队列IQ组成。OPQ的每一项包含任务ID，任务操作符，输入、输出、量化参数等必要的信息。IQ的每一项包含任务ID、TPU操作指令码、输入输出位置等。 OPQ 主机端调用enqueue API时，运行时系统新建一个taskID,为即将进入队列的kernel函数做准备，并且开始执行该kernel函数。 在kernel函数中，若触发了TPU操作，则阻塞当前的kernel函数，用当前taskID创建一个OPQ任务，并在OPQ的参数准备好后进入OPQ队列。 OPQ任务可以并行在Tensorizer上 。 IQTensorizer将OPQ任务转换为TPU指令，将数据转换为TPU可以接受的形式，并将其送至IQ上。若有一些IQ项有相同的输入、量化标志、taskID，但输出位置不同，IQ便将他们调度到同一个TPU上操作，以减少数据转移的开销以及数据格式的转换。其他采用FCFS方式。 Tensorizer作用作用主要是把用户要求的操作转换成对应的TPU指令，在这个过程中需要对数据也进行裁剪和缩放以适应TPU的特征，并且要仿照TFLite对数据进行包装。 将操作转换为TPU指令 将任务数据块划分为TPU指令可以达到最优性能的尺寸大小； Pair-wise操作(add): TPU指令在每个子块上操作后，整合运算结果得到输出； Matrix-wise操作(max): TPU指令在每个子块上操作后，生成CPU代码来集成最终结果。相较于迭代法，减少了数据的转移； 算术操作(conv2D&#x2F;FC): 类似于矩阵乘的分块算法，将运算分为PxQ大小的块，先用TPU指令再生成CPU代码来集成出最后的结果。 将数据转换为适用于TPU的格式Tensorizer将指令的输入数据缩放到定点数范围之内，并将这些数字包装成TPU可接受的模型或矩阵上。 缩放系数的计算： S与kernel函数上的操作顺序、操作总数、input的范围相关。 Tensorizer的开销Tensorizer还需要借助前文解码的TFLite生成的模型格式，仿照TFLite将一个张量包装成模型的形式。 实验测得：基于C的Tensorizer包装的速度比基于python的TFLite快1500倍。此时延可以与对EdgeTPU之间的数据转移相抵消。 为GPETPU优化应用Tensorizer完成了Task级别的优化，但是要完成一个应用，采用什么样的操作符进行计算仍然需要探究。我们以矩阵乘法为例： 用全连接操作符完成矩阵乘法非常简单，因为全连接本质上是一种矩阵-向量乘法，所以很容易实现，但是根据本工作前文对TPU指令的量化，全连接操作的RPS非常低。 除此之外我们还可以使用卷积操作实现矩阵乘法，这时候需要数据进行一定程度的变形。但是他的RPS非常好。 上图是使用全连接和卷积两种操作的效果，对比对象是CPU上使用OpenBLAS库进行计算。在只使用全连接算子时，其速度还不如CPU；但用卷积操作则可以加速超过两倍。 这个实验告诉我们，采用GPETPU进行编程时仍然需要注意算子的选择，可以依据之前测量的RPS进行选择。 为了进一步方便程序员的编程，GPETPU将一些领域内最常用的算法通过优化封装在标准库里，可以直接调用。领域涵盖了图计算、线性代数、物理仿真、模式识别以及金融领域。用户在前端像调用TPU基本操作一样调用这些算子就可以实现这些基本操作。 实验结果实验平台同上。 单核性能比较：TPU vs. CPU Baseline是用单核CPU进行应用实现的方案。可以看到TPU平均可以实现2.46倍的加速；且能耗要比CPU低45%。 GPETPU的扩展性 其次，使用多个TPU进行并行计算也有很好的扩展性。左边这个图的baseline仍然是使用单核CPU的计算速度。可以看到采用8核CPU的平均速度提升大概只在2.7倍左右，但采用8核TPU则可以提升13倍左右。 随着TPU数量的增多，大部分应用的速度几乎呈线性提升。 与GPU的比较 最后这张图展示了GPU与TPU的性能表现对比以及相对能耗。仍然是对比了单核的CPU。 虽然TPU的性能不如GPU，但是能耗非常低，相对于CPU可以平均节省40%的能量。 综合考虑能耗和性能，8x Edge TPU可以超出baseline 46%，强于GPU。 6. 自己的思考借助GPETPU框架，Edge TPU在边缘计算以及嵌入式设备中有较好的应用前景。类似的设计方法在各种加速器设计上都可以参考。","link":"","tags":[{"name":"加速器","slug":"加速器","permalink":"https://recoderchris.github.io/tags/%E5%8A%A0%E9%80%9F%E5%99%A8/"},{"name":"论文阅读","slug":"论文阅读","permalink":"https://recoderchris.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"name":"TPU","slug":"TPU","permalink":"https://recoderchris.github.io/tags/TPU/"},{"name":"通用计算","slug":"通用计算","permalink":"https://recoderchris.github.io/tags/%E9%80%9A%E7%94%A8%E8%AE%A1%E7%AE%97/"},{"name":"UC(Riverside)","slug":"UC-Riverside","permalink":"https://recoderchris.github.io/tags/UC-Riverside/"}]},{"title":"(HPCA06)从体系结构角度看软错误问题","date":"2023-02-04T13:34:34.000Z","path":"2023/02/04/软错误问题/","text":"以下内容来自HPCA’06的论文《The Soft Error Problem: An Architectural Perspective》。 背景知识什么是软错误高能粒子(e.g. 宇宙射线中的中子和封装材料中的α粒子)通过半导体设备(电路)时，会在其上产生电子空穴对(electron-hole pairs)，这会可能会导致瞬态错误(Transient Fault)。具体来说，电子空穴对会对晶体管的源(source)和扩散节点充电，如果充电量到达一定的阈值，会使得设备上某个组件(e.g. SRAM、锁存器、门电路)的逻辑状态发生改变，最终使得整个电路的运行结果产生逻辑错误，这种因为瞬态错误继而产生的运行错误被称为**软错误(soft error)**。 1PS: 与此相对应，硬错误(hard error)是指电路原件发生破坏，错误无法在短时间内修正的错误。个人理解是：软错误相对于硬错误来说是一种逻辑错误，而不是真实硬件发生了错误。软错误在接下来也许很难复现。 一般来说，每个电子元器件器件的纯软错误率(raw error rate)是一个常数。随着工艺水平的提升，单位面积的电路中将会集成更多的电子元器件，因而整个电路的纯软错误率线性增长。为了降低纯软错误率，一些DRAM厂商通过减缓单位电容的提升、降低供电电压等措施来降低每比特的软错误率，以应对工艺迭代会造成的软错误率提升。 软错误：从工业界到学术界工业界中亟待解决的因为软错误而产生的问题： 在芯片设计中，厂商希望能够了解软错误会对其设计产生多大的影响 希望能够从现有的技术中选择一种合适的策略来降低软错误产生的影响，以达到通过最小代价换取其可靠性指标的目的。 而学术界要解决以下的具体问题： 开发更好的设计框架、分析技术和软工具，以对软错误造成的系统影响产生更直观的理解和更量化的测量 探索和刻画软错误避免、探测和恢复的技术，在合理的性能、功耗、面积和复杂度要求下，满足对不同可靠性指标的要求 单比特软错误的分类SDC &amp; DUE **SDC(Silent Data Corruption，静默数据错误)**：没有被探测到、没有被修复，且对最终结果产生了影响的软错误。 **DUE(Detected Unrecoverable Error，被探测到且无法修复错误)**：被探测到了，但是无法被修复的错误。 目前，业界认为软错误率为SDC和DUE之和。DUE错误因为无法修正，所以它并不会影响整体的错误率。对抗DUE错误的最基本的对抗办法是fail-stop(重新运行)，以保证最后结果是正确的，但是它实际上并没有降低错误率。 DUE有两种分类： 第一种分类：会对最终结果产生影响的被称为true DUE，不会产生影响的被成为false DUE。保守的系统会对所有的DUE错误进行处理，一些系统则会分辨false DUE，例如错误选择路径的指令，来分门别类进行处理。 另一种分类：根据重新运行方式被分为process-kill和system-kill的DUE。所谓的process-kill是指一些DUE(e.g. 奇偶位校验错误)产生后，操作系统会隔离开这个错误，锁定一组出错的进程将它们杀死，保留其他进程正常运行，这种DUE被称为process-kill DUE；而另一种DUE是只能将整个系统关闭并重启才可以修复的错误，被称为system-kill DUE。 衡量软错误的单位：FIT和MTTF **FIT(failure in time)**：是指在109小时内产生的错误次数。SDC和DUE都可以用它来量化，FIT具有可加性，例如芯片的FIT可以用它上面所有组件的FIT求和得到。SDC和DUE的FIT数值和是整个芯片或系统的SER(soft error rate，软错误率)。 **MTTF(mean time to failure)**：是FIT的倒数，一种更加直观的展示。含义是经过多久系统会发生一次软错误。例如：1FIT&#x3D;114 years 一般来说，厂商都会在推出产品前规定自己产品的软错误率预算。例如IBM的power4宣称其SDC MTTF&#x3D;1000yrs，process-kill DUE MTTF&#x3D;10yrs，system-kill DUE MTTF&#x3D;25yrs。有更好的错误检测机制的系统，其SDC FIT会越低，但是DUE会更高。只有同时具备良好的检测(detection)和恢复(recovery)机制的系统，SDC和DUE FIT都会更低，使得SER会更低。 计算或估计SDC和DUE FIT芯片或系统设计团队要面临的一大问题是：设计出来的产品能否满足其软错误率预算的要求。当然，最准确的测量设计的软错误率的方法应该是这样的：使用加速中子或者放射性阿尔法粒子对真实电路进行实验。但是这种方法的实验代价太大，且需要将产品研制出来之后再测试。这种方法显然既耗时间，又不经济，不符合在芯片设计阶段的要求。 要计算或者估计设计的软错误率，目前普遍采用的是建模和计算的思想。首先，软错误率为SDC FIT和DUE FIT之和，之后再将整个系统或芯片的FIT拆解成每个组件的FIT。而每个组件的FIT又可以分为以下两个部分： 原始设备错误率(raw circuit error, 指瞬态错误能够导致组件状态转变的可能性) 体系结构脆弱因子(AVF, 指这个组件的状态变化，能够产生架构层面上可见的SDC&#x2F;DUE的可能性) 由此可见，计算设计的软错误率需要计算原始设备错误率和体系结构脆弱因子两项。 原始设备错误率每个组件的原始设备错误率由两个因素来决定：环境中的粒子流量和由工艺和实现决定的底层电路错误率。 粒子流量(particle flux)错误率基本和环境中的粒子流量成线性关系。环境中的粒子可以通过施加适当的屏蔽机制来实现一定程度的屏蔽，但是一般来说效果并不好。(需要很厚的混凝土隔离，不好实现)此外，随着海拔高度升高，粒子流量也会上升。 底层电路错误率(circuit error rate)电路错误率又由两个因素来决定：原始电路错误率和时间脆弱性因子(TVF, time vulnerability factor)。 原始电路错误率：指的是特定的电路单元会发生比特反转的可能性。这个跟比特反转的充电量Qcrit有关系，而这个数值由电路的电容和电压决定。一般来说，为了估算原始电路错误率，可以通过在不同时间点模拟电路单元(cell)的所有节点上的所有尺度的电流脉冲来计算原始错误率。但是这个花费的时间复杂度太高。因此，对于全芯片模拟，通常我们使用近似模型或蒙特卡洛模拟技术。 TVF：它是指在一个周期内一个比特反转会被捕获到的概率。例如，对于DRAM来说，因为它每个周期都会刷新一次，所以其被捕捉到的概率是100%；对于锁存器，只有在其内部保存数据的时候电路翻转才会被捕捉到，故概率大概是50%；对于一些静态逻辑电路，例如NAND电路，它只有在其前向电路(例如：前向电路中有锁存器)捕捉到这一变化时才有效。 123例子：计算原始设备利用率一般假定FIT/bit在0.001~0.01之间(原始电路错误率)，我们假定最小值0.001。TVF=50%，目标系统有4个CPU，SDC FIT预算为114FIT，我们可以算出每个处理器中最多有57,000(=114FIT/(0.001FIT/bit*0.5TVF*4CPU))个不受保护的锁存器。 SDC AVF SDC AVF：一个比特反转真正能够导致系统实质性执行错误的可能性，因为这个反转既没有被处理，也没有被探测到。 比特反转并不一定造成实质性执行错误：例如，如果分支预测的寄存器发生比特反转，不会影响程序的执行结果(SDC AVF&#x3D;0%)；而如果程序计数器的寄存器发生比特反转，程序大概率会被影响(SDC AVF&#x3D;100%)；指令队列中的寄存器比特反转就不一定了，如果里面存储的是错误路径的指令，那么SDC AVF很低；而如果是关键路径，那么SDC AVF就很高了。 ACE(architecturally correct execution)：ACE代表任何一种能够向用户产生正确系统执行结果的执行。 ACE bit：是指其中包含的信息一旦改变，将会影响程序的最终结果。un-ACE bit反之。 一个存储单元的SDC AVF是其中ACE bit的占比。如果一个程序执行了1000万个周期，其中的一个存储单元包含100万个周期的ACE位，则该单元的SDC AVF为10%。一个设计的SDC AVF是所有单元SDC AVF的总和。 DUE AVF DUE AVF是指比特反转会导致DUE的概率。它是true DUE和false DUE的总和。 True DUE AVF实际上是SDC AVF中被探测到的那部分，它在数值上等于老的SDC AVF 计算SDC和DUE AVF有三种方法：统计性错误注入、分析模型和性能模型(模拟器)。 统计性错误注入(Statistical Fault Injection, SFI) SFI是一种历经时间检验的测量脆弱性参数的方法 方法：在被研究的RTL设计结构中引入比特反转(时间空间随机)，之后运行设计，比较没有错误的模型和当前模型的体系结构状态。在运行几个模拟cycle之后，如果对比没有差异，说明错误潜伏在处理器中，或者已经被掩盖了。后者可以通过继续比较微结构状态来说明。此架构的AVF就是结果差异数量和比特反转数量的比值。（Architectural state includes main memory, architectural registers, and the program counter. Architectural state is defined by the instruction set architecture and can be manipulated by the programmer using instructions.） 优点：有力、不需要对处理器内部设计了解。 缺点：只能在详细的模型(RTL级，需要建模所有的比特)生效，对于每一个注入错误比特，需要上万cycle的测试，相当耗费时间；有错误和无错误的模型之间的不匹配不一定意味着存在错误，因为体系结构状态实际上可能包含非ACE位，例如动态死寄存器值。 分析模型 场景：比特流未经修改且没有重复地流经电路 Little’s Law: N &#x3D; B x L N – 结构中的平均比特数量 B – 每个周期进入结构的平均比特带宽 L – 每个比特通过结构的平均延迟 分析模型：$$\\frac{B_{ace}\\times L_{ace}}{number,of,bits,in,structure}$$ 举例：对于指令队列，Bace是 IPC（每周期指令数）乘以每条指令的 ACE 位数。 Lace 是指令在指令队列中的驻留周期。 当性能模型和 RTL 都不可用时，此方法在设计的早期阶段很有用。 性能模型 基本思想：识别流经机器的比特哪些是 ACE状态，哪些不是 ACE状态。 根据定义，每个比特位包含 ACE 状态的时间占比是位的 AVF，此过程被称为寿命分析。 主要挑战：确定每个位寿命的un-ACE 部分。导致un-ACE 状态的例子(在指令中)有dynamically dead&#x2F;wrong-path&#x2F;falsely predicted instruction。因而生命周期分析需要深入了解架构和微架构。 优势：与 SFI 不同，性能模型中的 ACE 分析要快得多，因为可以在一个实验中计算大量处理器结构的 AVF。 此外，性能模型可以实际运行数千万个周期，此它可以提供比 SFI 更高的准确性。 减小软错误率的方法以上只是介绍了如何估计一个设计的软错误率。而真正要减少软错误率，可以从以下三个方面入手：制造工艺、电路层面和架构层面。 处理工艺层面 方案：绝缘体上硅 (silicon-on-insulator, SOI)。 原理：因为其硅层薄的多，SOI 器件从 alpha 或中子粒子撞击中收集的电荷较少。 效果或优势：SOI 工艺可以使 SRAM 设备的 SER 降低 5 倍。(IBM) 缺点：尚不清楚是否会从 SOI 锁存器和逻辑器件中获得类似的 SER 降低；SOI 芯片的批量生产仍然是一个挑战。 电路层面 方案：调整设备参数、创建抗辐射（或抗辐射）单元。 原理：增加设备的电容和&#x2F;或电源电压，这两者都会提高 Qcrit，来降低软错误率； 抗辐射单元(cell)可能包含冗余状态，可以用来从软错误中恢复。 缺点：抗辐射单元会带来显着的面积和功耗开销。 架构层面架构解决方案可能比电路级解决方案更有效： 错误的【定义】通常存在于体系结构中（例如，对分支预测器的攻击不会导致微处理器中出现错误）。 典型的架构层面解决方案（例如奇偶校验或 ECC）的开销通常可以分摊到大量的位上。例如，ECC 的开销为每 64 位数据额外多出8位（即 13%），而抗辐射单元可能有 30-100% 的面积损失 微观层面(micro solution) 奇偶校验(parity)：奇偶校验可以检测任何单比特错误，但只能检测，不能修正。 受奇偶校验保护的位，通常具有SDC AVF&#x3D;0，但DUE AVF不等于0。 但是，奇偶校验保护结构的 DUE AVF 可以通过架构知识减少到零。 例如，受保护的直写缓存可以使奇偶校验错误的块无效，并从较低级别的缓存中重新获取正确的块。 SECDED ECC(single error correct, double error detected)：通常用于处理器高速缓存，可以纠正所有单比特错误并检测所有双比特错误，为单比特错误提供零SDC 和 DUE AVF。 ECC 可以inline或out-of-band实现：inline ECC 需要在读取返回数据之前计算并验证 ECC 代码是否正确，这通常会在处理器流水线中产生一个或多个额外的周期；out-of-band ECC 检查允许处理器继续读取数据，如果 ECC 检查检测到错误，读取不正确数据的指令重新提交，缓存数据被更正。 π 比特：π 比特是一种错误传播机制，可减少false DUE。 检测到错误后，不会立即发出错误信号，而是将错误发布在π位中并传播，直到有更多有价值的信息出现。 例如，在奇偶校验中，不会保护的寄存器文件中直接引发错误，而是可以通过在读取特定寄存器的指令时设置π位来发布错误。如果确定有问题的指令在错误的路径上，则忽略π位，避免false DUE 事件的发生。 宏观层面(macro solution)微观层面的方法可能需要大量的面积和设计工作负担。 因此，在某些情况下，使用 CPU 或线程进行故障检测可能会更简单。两种广泛的故障检测解决方案包括流水线的逐周期锁步(lockstepping) 和**冗余多线程 (redundant multithreading, RMT)**。 逐周期锁步： 在逐周期锁步中，同一的程序在相同的流水线上运行，每个周期都会检查二者的输出是否一致。 RMT：在指令的提交点检查选定的指令的输出是否匹配。 与锁步不同，RMT 并不需要两个线程每个周期进行同步。 以上方法降低了 SDC 率，但增加了 DUE 率。 降低处理器的 DUE 率需要通过硬件或软件进行错误恢复实现。从检测到的错误中恢复需要识别有问题的处理器并保持可以启动恢复的正确状态。 当外部检查器检测到错误时，可以从内部错误信号中识别有问题的处理器；或者可以定期检查处理器的状态，并在错误回滚时从检查点重启流水线或线程。 未来方向 计算不同架构的 SDC 和 DUE AVF：需要详细的生命周期分析和潜在的新技术，以识别处理器结构中的 ACE 和非 ACE 组件。 适用于不同处理器结构的AVF缩减技术 保护数据通过的微架构状态：现代处理器芯片既包含处理器内核，也包含系统组件，例如内存控制器和路由器。 保护流经“非核心”部分的数据可能并不难，我们可以通过在流水线中创建数据的位置，生成错误保护位提供端到端的错误保护，并让其保持不变地流动，直到数据被使用。 然而，保护数据通过的微架构的状态可能需要进一步研究。 RMT 的软件版本：锁步基本上是一个硬件概念，而 RMT 可以在硬件或软件中实现。与之前的软件故障检测实施相比，RMT 模型允许设计人员减少必要的软件检查次数。 此外，软件无法完全了解硬件，因此硬件可能必须有选择地去保护RMT无法涵盖的一些结构，这涉及软硬件协同。 了解和描述软错误与功率的权衡关系： 当电源电压降低时，软错误率会急剧上升。 二者如何权衡是下一步要解决的一个问题。 来自其他问题的软错误：例如电源噪声、耦合等。这需要详细了解不同的故障模型，但关于软错误的许多定义可以被转移到这些类型的故障中。","link":"","tags":[{"name":"论文阅读","slug":"论文阅读","permalink":"https://recoderchris.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"name":"容错","slug":"容错","permalink":"https://recoderchris.github.io/tags/%E5%AE%B9%E9%94%99/"},{"name":"SDC","slug":"SDC","permalink":"https://recoderchris.github.io/tags/SDC/"},{"name":"软错误","slug":"软错误","permalink":"https://recoderchris.github.io/tags/%E8%BD%AF%E9%94%99%E8%AF%AF/"},{"name":"Umich","slug":"Umich","permalink":"https://recoderchris.github.io/tags/Umich/"}]},{"title":"图数据管理-课程笔记2","date":"2023-02-04T12:16:19.000Z","path":"2023/02/04/图数据管理笔记2/","text":"Network Properties and Random Graph ModelI. Key Network Properties Problem: How to measure a network? Answer: Using Network Properties. 1. Degree Distribution P(k)P(k) is the probability that a random chosen node has degree k. is number of nodes with degree k. Thus. 2. Path p, Average Distance and DiameterA path p is a sequence of nodes in which each node is linked to the next one. Shortest Path Distance: the number of edges along the shortest path connecting two nodes. Diameter: the maximum distance between any pair of nodes in a graph. That is, Average path length for a connected graph or a SCC graph is defined as: . In practice, we compute the only over the connected pairs of nodes. 3. Cluster Coefficient cA vertex’scluster coefficient c measures how a vertex’s neighbors are connected to each other. Assume as the number of edges between neighbors of vertex , and as the vertex degree. The cluster coefficient can be calculated as Globally, the Average clustering coefficient is . 4. ConnectivityTo show the connectivity of graph, one can calculate the size of the largest connected component in graph, BTW, largest component is also known as giant component. II. Measure Real-world Networks In this part, we use MSN network as an exaple to show some properties of real-world networks. 1. Degree Distribution Power law distribution: , where is a parameter whose value is typically between 2 and 3. The graph degree distribution is heavily skewed. 2. Clustering Coefficient Average Clustering Coefficient of Real Graph can be really big(0.1140) compared to the random graph. 3. Connected Components Nearly all of the vertices are in one largest(giant) connected component. 4. Diameter Average path length is 6.6. Besides, 90% of the nodes can be reached in &lt;8 hops. 5. Small World Effect(Six Degrees of Separation), 1967 A small-world network is a type of mathematical graph where most nodes are not neighbors of one another, but most nodes can be reached from every other by a small number of hops or steps. In mathematical format, assuming L is the distance between two randomly chosen nodes, and N is the number of nodes in a network, then we have . III. Graph Generation ModelThere are four kinds of Graph Generation Model. 1. Random Graph Model(Erdos-Renyi Graph)(1) Generation: : undirected graph on n nodes where each edge (u,v) appears i.i.d with probability p. : undirected graph with n nodes, and m edges picked uniformly at random. (2) Degree Distribution P(k) Binomial Distribution: . . (3) Clustering Coefficient Thus, And , so decreases with the graph size n. Note: the if . (4) Path LengthRandomly pick a node , and it will have: points whose distance is 1 points whose distance is 2 points whose distance is 3 points whose distance is At the same time, the number of vertices is . It means that: . So . In E-R Random Graph, dmax increase slowly with N. (5) Giant Component When p(n-1) &#x3D; 1, the Giant Connected Component emerges. When k&#x3D;ln N, the fully connected graph emerges. (6) Problems Degree distribution differs from that of real-world graph Giant component in most real networks does NOT emerge through a phase transition No local structure – Clustering Coefficient is too low. Conclusion: Real-world network is not random! 2. Small-world Model[Watts-Strogatz ’98] Problem: E-R random graph’s clustering is low! Need: High cluster and low diameter. Start with a low-dimensional regular lattice Has high clustering coefficient Rewire: Introduce randomness Add &#x2F; remove edges to create shortcuts to join remote parts of the lattice. For each edge, with probability p move the other endpoint to a random node. The more probability of rewiring p, the smaller clustering coefficient will be. 3. Barabasi-Albert(BA) Model Problem: How to model the power-law distribution of node degree? Solution: Introduce Growth and rich-get-richer. (1) Assumptions Growth: the graphs grows continuously Preferential attachment(i.e., rich-get-richer): nodes with larger connectivity tend to receive new edges (2) Model Definition Start with a small graph of vertices generated randomly. At each step, add a new vertex with edges connecting to distinct vertices already present in the graph. For each connection, the selection of the existing vertex is governed by the following equation.. (3)Properties The degree distribution of a BA graph follows power law distribution of γ&#x3D;3. The diameter of BA model is . 4. Kronecker Graph Model(1) Recursive Graph Generation Problem: How can we think of network structure recursively? Self-similarity Object is similar to a part of itself: the whole has the same shape as one or more of the parts. (2) Kronecker Graph Generation (3) Stochastic Kronecker Graphs Conclusion: Kronecker is similar to real-world graph.","link":"","tags":[{"name":"图计算","slug":"图计算","permalink":"https://recoderchris.github.io/tags/%E5%9B%BE%E8%AE%A1%E7%AE%97/"},{"name":"基础知识","slug":"基础知识","permalink":"https://recoderchris.github.io/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"},{"name":"课程笔记","slug":"课程笔记","permalink":"https://recoderchris.github.io/tags/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"},{"name":"PKU","slug":"PKU","permalink":"https://recoderchris.github.io/tags/PKU/"}]},{"title":"图数据管理-课程笔记1","date":"2023-02-04T12:08:11.000Z","path":"2023/02/04/图数据管理笔记1/","text":"Introduction to Graph DataI. Concept of Graph1. Origin of Graph problem [Seven Bridges of Knigsberg, 1736] The problem is to devise a walk across each of the seven bridges once and only once to touch every part of the town, or this walk does not exist. Solution: Seven Bridge Problem is to find the Euler Circuit in a graph. It is easier than finding a Hamilton Circuit. 2. Concept of Graph A graph is a set of nodes and edges that connect them. (1) Difference between Network and Graph① Network: Topology structure in a real system. Examples: Web, Social Network and Metabolic Network. Terminology: Network, Node, Link. ② Graph: Mathematical representation of network. Examples: Web Graph, Social Graphs, Knowledge Graph. Terminology: Graph, Vertex, Edge. (2) Application Field Social Network Citation Network Road Network Protein Network Knowledge Graph Internet … (3) Why Graph So Important? Graph is a general data structure to model relationships between different entities Graph provides a universal language to describe complex data Problem: Data availability and computational challenges Graph bridges Big Data and Artificial Intelligence II. Terminology in Graph1. Directed&#x2F;Undirected Graph and Vertex Degree Difference: Whether the edge has its direction. (1) Undirected Graph Properties of Edge: Symmetrical and reciprocal, i.e. (u,v)≡ (v,u) Examples: Wechat graph, Collaboration Graph Degree(v): the number of edges adjacent to vertex v. – – (2)Directed Graph Properties of Edge: Directed Examples: Weibo Following graph, Phone Call Graph Degree(v): Divide into in-degree and out-degree. And , and Source Vertex is the vertex with , and Sink Vertex is the vertex with . 2. Special Graphs(1) Clique(Complete Graph) Clique, a.k.a Complete Graph, is the undirected graph whose vertices are all connected. Number of Edges: In clique, assuming vertices, the total edge number is: . It is also the maximum of number of edge in a graph with vertices. (2) Bipartite Graph Bipartite Graph is a graph whose vertices can be divided into two disjoint sets U and V, and every edge connect between U and V. Examples: Authors-to-Papers, Buyers-to-Products 3. Representing Graphs(1) Adjacency Matrix ① Undirected Graph: Symmetric: The matrix is symmetric, that is, If Non-cyclic: for non-cyclic graph. Degree of vertex i: Total Edge of Graph: ② Directed Graph: Non-symmetric: The matrix is not symmetric, that is, . If Non-cyclic: for non-cyclic graph Degree of vertex *i*: Total Edge of Graph: ③ Disadvantages: Space Complexity: . However, the adjacency matrix is a sparse matrix. It is too much space costing. In graph processing system, adjacency matrix is totally abandoned for its expensive cost for storing. (2)Adjacency List Source Vertex Dst. 1 (Null) 2 3,4 3 2,4 4 5 5 1,2 Advantages: Easier to work when graph is large and sparse. Disadvantages: The time complexity of query a vertex is , which is time-expensive. Besides, point chasing happens when we want to find a path. (3) Compressed Sparse Representation Vertex Offset Edges 1 0 3 2 0 4 3 2 2 4 4 4 5 5 5 1 2 Offset of corresponding Vertex points to the position of vertex’s adjacency edge in Edges. Advantages: It is the most compact form of graph representation. Disadvantages: Leaving no space for dynamic graph processing. (4) Sparsity of Graph Most of graphs in real world are sparse. Conclusion: It is impossible to use adjacency matrix in graph database or processing system. In real graph computing system, we usually use compressed format to store the large-scale graph. However, it brings new challenge of building dynamic graph computing system for programmers and developers. Thus, *dynamic graph processing system* is one of the future direction for architecture researchers. Choice of the proper network representation of a given domain&#x2F;problem determines our ability to use network successfully. In some cases, there is a unique, unambiguous representation In some cases, the representation is by no means of unique The way you assign links will determine the nature of the question you study. 4. Topology Feature of Graph(1) Self-loop and Multigraph Note: In adjacency matrix, if there are 1 elements in diagonal, it is a self-loop graph. If there are , then the graph is a multigraph. (2) Connectivity① Connected Undirected Graph ② Conncted Directed Graph ③ Strong&#x2F;Weak Connected Components Weak Connected Components(WCC): Despite of direction, if the subgraph is a connected graph, then the subgraph can be called as a WCC of original Graph. 5. Some Real Graph Categories","link":"","tags":[{"name":"图计算","slug":"图计算","permalink":"https://recoderchris.github.io/tags/%E5%9B%BE%E8%AE%A1%E7%AE%97/"},{"name":"基础知识","slug":"基础知识","permalink":"https://recoderchris.github.io/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"},{"name":"课程笔记","slug":"课程笔记","permalink":"https://recoderchris.github.io/tags/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"},{"name":"PKU","slug":"PKU","permalink":"https://recoderchris.github.io/tags/PKU/"}]}]